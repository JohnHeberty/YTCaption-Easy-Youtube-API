# ğŸ“Š ANÃLISE COMPLETA DE LOGS E ERROS

**Data da AnÃ¡lise:** 2025-11-01  
**ServiÃ§os Analisados:** audio-normalization, audio-transcriber, video-downloader, orchestrator

---

## ğŸ”´ PROBLEMAS CRÃTICOS IDENTIFICADOS

### 1. **Circuit Breaker OPEN - ServiÃ§o IndisponÃ­vel**

**Erro:**
```
"[audio-normalization] Circuit breaker is OPEN - service unavailable"
```

**Causa Raiz:**
- O Circuit Breaker do orchestrator estÃ¡ abrindo (bloqueando requisiÃ§Ãµes) apÃ³s mÃºltiplas falhas consecutivas no serviÃ§o
- ConfiguraÃ§Ã£o atual: apÃ³s `circuit_breaker_max_failures` falhas, o serviÃ§o Ã© marcado como indisponÃ­vel por `circuit_breaker_recovery_timeout` segundos
- O problema Ã© que mesmo apÃ³s o timeout de recuperaÃ§Ã£o, se a primeira tentativa falhar, o circuit breaker abre novamente

**Impacto:**
- ğŸ”´ **CRÃTICO**: Impede que o orchestrator envie requisiÃ§Ãµes ao serviÃ§o mesmo quando ele pode estar funcionando
- Bloqueia o upload de arquivos multipart
- Causa rejeiÃ§Ã£o imediata de requisiÃ§Ãµes sem nem tentar

**SoluÃ§Ã£o NecessÃ¡ria:**
1. âœ… Aumentar `circuit_breaker_max_failures` para tolerar mais falhas antes de abrir
2. âœ… Diminuir `circuit_breaker_recovery_timeout` para tentar recuperaÃ§Ã£o mais rÃ¡pido
3. âœ… Implementar half-open state: permitir 1 requisiÃ§Ã£o de teste antes de fechar completamente
4. âœ… Excluir endpoints de health check do circuit breaker (nÃ£o devem contar como falhas)

---

### 2. **InconsistÃªncia Store=processing, Celery=FAILURE**

**Erro (repetido constantemente):**
```
âš ï¸ InconsistÃªncia: Store=processing, Celery=FAILURE
```

**FrequÃªncia:** Ocorreu 22+ vezes nos logs

**Causa Raiz:**
- As tasks do Celery estÃ£o falhando, mas o Redis Store ainda mantÃ©m o status como "processing"
- Indica que o tratamento de erros do Celery nÃ£o estÃ¡ atualizando corretamente o Redis
- Workers do Celery estÃ£o morrendo ou crashando sem atualizar o status final

**Impacto:**
- ğŸŸ  **ALTO**: Jobs ficam presos no estado "processing" eternamente
- Frontend nÃ£o recebe notificaÃ§Ã£o de falha
- UsuÃ¡rio fica esperando indefinidamente
- Sistema fica inconsistente

**SoluÃ§Ã£o NecessÃ¡ria:**
1. âœ… Melhorar o tratamento de exceÃ§Ãµes em `celery_tasks.py`
2. âœ… Garantir que TODOS os caminhos de erro atualizam o Redis Store
3. âœ… Adicionar signal handlers do Celery para capturar crashes (`task_failure`, `task_revoked`)
4. âœ… Implementar um job de reconciliaÃ§Ã£o que verifica inconsistÃªncias periodicamente
5. âœ… Adicionar timeout de "processing" - se job estÃ¡ processando por muito tempo, marcar como falha

---

### 3. **Arquivo Muito Grande - ValidaÃ§Ã£o Inconsistente**

**Erro:**
```
Arquivo muito grande: 156327441 bytes (149 MB)
Arquivo muito grande: 144173889 bytes (137 MB)
```

**Causa Raiz:**
- Arquivos estÃ£o sendo rejeitados por serem "muito grandes", mas:
  - Config atual: `max_file_size_mb = 500` (500 MB)
  - Arquivos rejeitados: ~137-149 MB (bem abaixo do limite)
- **BUG**: A validaÃ§Ã£o estÃ¡ calculando o tamanho incorretamente ou hÃ¡ outro limite oculto

**Impacto:**
- ğŸŸ  **MÃ‰DIO**: UsuÃ¡rios nÃ£o conseguem fazer upload de arquivos que deveriam ser aceitos
- ExperiÃªncia do usuÃ¡rio ruim (limite anunciado nÃ£o corresponde ao real)

**SoluÃ§Ã£o NecessÃ¡ria:**
1. âœ… Investigar a validaÃ§Ã£o de tamanho no endpoint de upload
2. âœ… Verificar se hÃ¡ limite no FastAPI/Uvicorn para multipart
3. âœ… Verificar se hÃ¡ limite no nginx/proxy reverso
4. âœ… Adicionar log detalhado: "Arquivo {X} MB, limite {Y} MB, calculado como {Z} bytes"
5. âœ… Corrigir a validaÃ§Ã£o para aceitar atÃ© o limite configurado

---

### 4. **Limpeza Total Excessiva (FLUSHDB)**

**PadrÃ£o Observado:**
```
ğŸ”¥ LIMPEZA TOTAL CONCLUÃDA: 0 jobs do Redis + 16 arquivos removidos (1607.84MB liberados)
ğŸ”¥ LIMPEZA TOTAL CONCLUÃDA: 0 jobs do Redis + 7 arquivos removidos (714.19MB liberados)
```

**Causa Raiz:**
- Endpoint `/cleanup` estÃ¡ sendo chamado com frequÃªncia
- EstÃ¡ usando `FLUSHDB` que **limpa TODO o database do Redis**, nÃ£o apenas jobs especÃ­ficos
- Remove atÃ© jobs de outros serviÃ§os se compartilharem o mesmo Redis DB

**Impacto:**
- ğŸŸ¡ **MÃ‰DIO**: Pode estar removendo jobs vÃ¡lidos de outros serviÃ§os
- Risco de perda de dados se mÃºltiplos serviÃ§os usam o mesmo Redis DB
- NÃ£o Ã© granular - remove tudo, nÃ£o apenas jobs expirados

**SoluÃ§Ã£o NecessÃ¡ria:**
1. âœ… **NÃƒO USAR `FLUSHDB`** - muito destrutivo
2. âœ… Implementar limpeza granular usando padrÃµes de chave: `SCAN` + `DEL` com prefixo
3. âœ… Adicionar filtro de tempo: remover apenas jobs com mais de X horas
4. âœ… Separar limpeza de "jobs expirados" vs "limpeza total"
5. âœ… Usar diferentes Redis DBs para diferentes serviÃ§os (jÃ¡ estÃ¡ usando DB=2, bom!)

---

## ğŸŸ¡ PROBLEMAS MÃ‰DIOS

### 5. **Falhas Repetidas de Tasks Celery**

**ObservaÃ§Ã£o:**
- Muitas inconsistÃªncias indicam que tasks estÃ£o falhando com frequÃªncia
- NÃ£o hÃ¡ logs detalhados do **porquÃª** as tasks estÃ£o falhando

**SoluÃ§Ã£o NecessÃ¡ria:**
1. âœ… Adicionar logging mais verboso em `celery_tasks.py`
2. âœ… Capturar e logar exceÃ§Ãµes com stack trace completo
3. âœ… Implementar Dead Letter Queue (DLQ) para tasks que falharam todas as tentativas

---

### 6. **Logs NÃ£o Consolidados**

**Problema:**
- Apenas `audio-normalization` tem pasta `logs/`
- Outros serviÃ§os nÃ£o tÃªm logs estruturados
- Dificulta troubleshooting

**SoluÃ§Ã£o NecessÃ¡ria:**
1. âœ… Garantir que todos os serviÃ§os salvem logs em `./logs/`
2. âœ… Padronizar estrutura: `error.log`, `warning.log`, `info.log`, `debug.log`
3. âœ… Considerar log centralizado (ELK, Loki, ou similar)

---

## ğŸ“‹ PLANO DE AÃ‡ÃƒO PRIORITIZADO

### ğŸ”´ **PRIORIDADE CRÃTICA (Fazer Agora)**

1. **âœ… CORRIGIDO: Circuit Breaker**
   - [x] Implementar half-open state (CLOSED â†’ OPEN â†’ HALF_OPEN)
   - [x] Aumentar tolerÃ¢ncia a falhas (5 â†’ 10 falhas)
   - [x] Diminuir timeout de recuperaÃ§Ã£o (5min â†’ 1min)
   - [x] Excluir health checks do circuit breaker (nÃ£o contam como falhas)
   - [x] Adicionar contador de tentativas no estado HALF_OPEN

2. **âœ… CORRIGIDO: InconsistÃªncia Store/Celery**
   - [x] Adicionar signal handlers do Celery (`task_failure`, `task_revoked`)
   - [x] Garantir atualizaÃ§Ã£o do Redis em TODOS os caminhos de erro
   - [x] Aplicado em todos os 3 microserviÃ§os (audio-normalization, audio-transcriber, video-downloader)
   - [ ] TODO: Implementar job de reconciliaÃ§Ã£o periÃ³dica (detecta e corrige inconsistÃªncias)

3. **âœ… CORRIGIDO: ValidaÃ§Ã£o de Tamanho de Arquivo**
   - [x] Corrigir cÃ¡lculo e log de tamanho (agora mostra MB corretamente)
   - [x] Adicionar logging detalhado: "Arquivo X MB / Y MB permitidos"
   - [ ] TODO: Verificar limites ocultos (nginx, uvicorn) se problema persistir

---

### ğŸŸ  **PRIORIDADE ALTA (Esta Semana)**

4. **Substituir FLUSHDB por Limpeza Granular**
   - [ ] Implementar `SCAN` + `DEL` com padrÃ£o de chave
   - [ ] Adicionar filtro de tempo (remover apenas jobs antigos)

5. **Melhorar Logging**
   - [ ] Adicionar logs estruturados em todos os serviÃ§os
   - [ ] Criar pasta `logs/` para todos
   - [ ] Adicionar stack traces completos em erros

6. **Implementar Dead Letter Queue**
   - [ ] Capturar tasks que falharam todas as tentativas
   - [ ] Salvar em Redis com TTL
   - [ ] Criar endpoint para visualizar DLQ

---

### ğŸŸ¡ **PRIORIDADE MÃ‰DIA (Este MÃªs)**

7. **Monitoramento Proativo**
   - [ ] Adicionar mÃ©tricas (Prometheus/Grafana)
   - [ ] Alertas para circuit breaker aberto
   - [ ] Alertas para inconsistÃªncias Store/Celery

8. **Testes de ResiliÃªncia**
   - [ ] Simular falhas de rede
   - [ ] Simular crashes de worker
   - [ ] Validar recuperaÃ§Ã£o automÃ¡tica

---

## ğŸ¯ RESULTADOS ESPERADOS APÃ“S CORREÃ‡Ã•ES

âœ… Circuit breaker nÃ£o bloquearÃ¡ serviÃ§os funcionais  
âœ… Jobs nunca ficarÃ£o presos em "processing"  
âœ… Arquivos vÃ¡lidos serÃ£o aceitos corretamente  
âœ… Limpeza serÃ¡ segura e granular  
âœ… Logs detalhados facilitarÃ£o debugging  
âœ… Sistema serÃ¡ mais resiliente e confiÃ¡vel  

---

## ğŸ“Š ESTATÃSTICAS DOS LOGS

- **Total de InconsistÃªncias (Store/Celery):** 22+
- **Total de Limpezas Totais:** 15+
- **Total de Arquivos Removidos:** ~50+ arquivos (~6GB)
- **Arquivos Rejeitados (muito grande):** 2 (mas configuraÃ§Ã£o deveria aceitar)
- **PerÃ­odo Analisado:** 2025-10-31 00:31 atÃ© 2025-11-01 01:10 (24h)

---

**PrÃ³ximos Passos:** Implementar correÃ§Ãµes na ordem de prioridade acima.
