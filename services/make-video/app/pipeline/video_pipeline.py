"""
Pipeline Service - Download, Transform, Validate, Approve

Gerencia o pipeline completo de v√≠deos:
1. Download ‚Üí data/raw/
2. Transform ‚Üí data/transform/ (convers√£o H264)
3. Validate ‚Üí data/validate/ (detec√ß√£o legendas)
4. Approve ‚Üí data/approved/ (v√≠deos finais)
5. Cleanup ‚Üí Remove das pastas anteriores
6. Blacklist ‚Üí V√≠deos reprovados n√£o s√£o reprocessados
"""

import asyncio
import logging
import subprocess
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import httpx
from datetime import datetime

from app.core.config import get_settings
from app.video_processing.subtitle_detector_v2 import SubtitleDetectorV2
from app.services.blacklist_factory import get_blacklist

logger = logging.getLogger(__name__)
settings = get_settings()


class VideoPipeline:
    """
    Pipeline completo para processar v√≠deos
    
    Fluxo:
    1. Download ‚Üí data/raw/shorts/
    2. Transform ‚Üí data/transform/videos/ (H264)
    3. Validate ‚Üí Detector de legendas (97.73% acur√°cia)
    4. Approve/Reject:
       - Aprovado: Move para data/approved/videos/
       - Reprovado: Adiciona ao blacklist
    5. Cleanup: Remove de pastas anteriores
    """
    
    def __init__(self):
        self.detector = SubtitleDetectorV2(show_log=True)
        self.blacklist = get_blacklist()  # SQLite blacklist (oficial)
        self.settings = settings
        
        # Criar diret√≥rios
        self._ensure_directories()
    
    def _ensure_directories(self):
        """Garantir que todos os diret√≥rios existem"""
        dirs = [
            'data/raw/shorts',
            'data/transform/videos',
            'data/transform/temp',
            'data/validate/in_progress',
            'data/approved/videos',
        ]
        for dir_path in dirs:
            Path(dir_path).mkdir(parents=True, exist_ok=True)
    
    async def download_shorts(self, query: str, max_count: int = 50) -> List[Dict]:
        """
        1. DOWNLOAD: Buscar e baixar shorts via youtube-search + video-downloader
        
        Args:
            query: Query de busca
            max_count: M√°ximo de shorts para baixar
        
        Returns:
            Lista de shorts baixados com metadados
        """
        logger.info(f"üì• DOWNLOAD: Buscando shorts para '{query}' (max: {max_count})")
        
        downloaded = []
        
        try:
            # 1. Buscar shorts via youtube-search (ass√≠ncrono)
            youtube_search_url = self.settings.get('youtube_search_url')
            async with httpx.AsyncClient(timeout=120.0) as client:
                # 1.1. Criar job de busca
                response = await client.post(
                    f"{youtube_search_url}/search/shorts",
                    params={
                        "query": query,
                        "max_results": max_count
                    }
                )
                response.raise_for_status()
                job_data = response.json()
                job_id = job_data.get('id')
                
                logger.info(f"   üìã Job criado: {job_id} (aguardando...)")
                
                # 1.2. Aguardar job completar
                wait_response = await client.get(
                    f"{youtube_search_url}/jobs/{job_id}/wait",
                    timeout=90.0
                )
                wait_response.raise_for_status()
                completed_job = wait_response.json()
                
                # 1.3. Extrair resultados
                shorts = completed_job.get('result', {}).get('results', [])
            
            logger.info(f"   ‚úÖ {len(shorts)} shorts encontrados")

            # 1.4. Deduplicar por video_id para evitar contagem inflada e sobrescrita
            unique_shorts = []
            seen_video_ids = set()
            duplicated_count = 0

            for short in shorts:
                video_id = short.get('video_id')
                if not video_id:
                    continue

                if video_id in seen_video_ids:
                    duplicated_count += 1
                    continue

                seen_video_ids.add(video_id)
                unique_shorts.append(short)

            if duplicated_count > 0:
                logger.info(
                    f"   üîÅ Duplicados removidos: {duplicated_count} "
                    f"(√∫nicos: {len(unique_shorts)})"
                )
            
            # 2. Baixar cada short via video-downloader (ass√≠ncrono)
            video_downloader_url = self.settings.get('video_downloader_url')
            
            for i, short in enumerate(unique_shorts, 1):
                video_id = short.get('video_id')
                
                # Verificar blacklist ANTES de baixar
                if self.blacklist.is_blacklisted(video_id):  # Sync call
                    logger.info(f"   ‚ö´ [{i}/{len(unique_shorts)}] {video_id}: BLACKLISTED (skip)")
                    continue
                
                try:
                    async with httpx.AsyncClient(timeout=120.0) as client:
                        # 2.1. Criar job de download
                        response = await client.post(
                            f"{video_downloader_url}/jobs",
                            json={
                                "url": f"https://www.youtube.com/watch?v={video_id}",
                                "quality": "best"
                            }
                        )
                        response.raise_for_status()
                        job = response.json()
                        job_id = job.get('id')
                        
                        logger.info(f"   üì¶ [{i}/{len(unique_shorts)}] {video_id}: Job {job_id} criado")
                        
                        # 2.2. Aguardar job completar (polling)
                        max_retries = 30  # 30 tentativas √ó 2s = 60s timeout 
                        for retry in range(max_retries):
                            await asyncio.sleep(2)
                            status_response = await client.get(
                                f"{video_downloader_url}/jobs/{job_id}"
                            )
                            status_response.raise_for_status()
                            job_status = status_response.json()
                            
                            if job_status.get('status') == 'completed':
                                file_path = job_status.get('file_path')
                                logger.info(f"   ‚úÖ [{i}/{len(unique_shorts)}] {video_id}: Download conclu√≠do ({file_path})")
                                break
                            elif job_status.get('status') == 'failed':
                                error_msg = job_status.get('error_message', 'Unknown error')
                                raise Exception(f"Download failed: {error_msg}")
                        else:
                            raise Exception("Download timeout (60s)")
                        
                        # 2.3. Baixar arquivo via GET /jobs/{job_id}/download
                        download_response = await client.get(
                            f"{video_downloader_url}/jobs/{job_id}/download",
                            timeout=60.0
                        )
                        download_response.raise_for_status()
                    
                    # 2.4. Salvar em data/raw/shorts/ com extens√£o real
                    file_ext = ".mp4"
                    if file_path:
                        parsed_ext = Path(file_path).suffix
                        if parsed_ext:
                            file_ext = parsed_ext

                    video_path = Path(f"data/raw/shorts/{video_id}{file_ext}")
                    video_path.parent.mkdir(parents=True, exist_ok=True)
                    
                    with open(video_path, 'wb') as f:
                        f.write(download_response.content)
                    
                    logger.info(f"   üíæ [{i}/{len(unique_shorts)}] {video_id}: Salvo em {video_path}")
                    
                    downloaded.append({
                        'video_id': video_id,
                        'title': short.get('title'),
                        'raw_path': str(video_path),
                        'downloaded_at': datetime.utcnow().isoformat()
                    })
                    
                    logger.info(f"   ‚úÖ [{i}/{len(unique_shorts)}] {video_id}: Downloaded")
                    
                except Exception as e:
                    logger.error(f"   ‚ùå [{i}/{len(unique_shorts)}] {video_id}: Download failed - {e}")
                    continue

                logger.info(f"üì• DOWNLOAD COMPLETO: {len(downloaded)}/{len(unique_shorts)} baixados")
            return downloaded
            
        except Exception as e:
            logger.error(f"‚ùå Erro no download: {e}", exc_info=True)
            return []
    
    def transform_video(self, video_id: str, raw_path: str) -> Optional[str]:
        """
        2. TRANSFORM: Converter v√≠deo para H264 compat√≠vel
        
        Args:
            video_id: ID do v√≠deo
            raw_path: Caminho do v√≠deo bruto (data/raw/)
        
        Returns:
            Caminho do v√≠deo transformado (data/transform/) ou None se falhou
        """
        logger.info(f"üîÑ TRANSFORM: Convertendo {video_id} para H264")
        
        try:
            raw_video = Path(raw_path)
            if not raw_video.exists():
                logger.error(f"   ‚ùå Arquivo n√£o encontrado: {raw_path}")
                return None
            
            # Caminho de sa√≠da
            transform_path = Path(f"data/transform/videos/{video_id}.mp4")
            
            # Convers√£o FFmpeg para H264
            cmd = [
                'ffmpeg',
                '-i', str(raw_video),
                '-c:v', 'libx264',  # Codec H264
                '-preset', 'fast',
                '-crf', '23',
                '-c:a', 'aac',
                '-b:a', '128k',
                '-movflags', '+faststart',
                '-y',  # Sobrescrever
                str(transform_path)
            ]
            
            result = subprocess.run(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                timeout=120
            )
            
            if result.returncode == 0 and transform_path.exists():
                logger.info(f"   ‚úÖ Convertido: {transform_path}")
                return str(transform_path)
            else:
                logger.error(f"   ‚ùå Convers√£o falhou (code {result.returncode})")
                return None
                
        except Exception as e:
            logger.error(f"‚ùå Erro na convers√£o: {e}", exc_info=True)
            return None
    
    def validate_video(self, video_id: str, transform_path: str) -> Tuple[bool, Dict]:
        """
        3. VALIDATE: Detectar legendas/texto no v√≠deo
        
        Args:
            video_id: ID do v√≠deo  
            transform_path: Caminho do v√≠deo transformado
        
        Returns:
            (aprovado, metadados)
            - aprovado: True se SEM legendas, False se COM legendas
            - metadados: Detalhes da detec√ß√£o
        """
        logger.info(f"‚úÖ VALIDATE: Detectando legendas em {video_id}")
        
        try:
            # Detec√ß√£o com SubtitleDetectorV2 (97.73% acur√°cia)
            has_text, confidence, sample_text, metadata = self.detector.detect(transform_path)
            
            # Aprovado = SEM legendas
            aprovado = not has_text
            
            result_meta = {
                'video_id': video_id,
                'has_text': has_text,
                'confidence': confidence,
                'sample_text': sample_text,
                'frames_processed': metadata.get('frames_processed', 0),
                'frames_with_text': metadata.get('frames_with_text', 0),
                'detection_ratio': metadata.get('detection_ratio', 0.0),
                'validated_at': datetime.utcnow().isoformat()
            }
            
            if aprovado:
                logger.info(f"   ‚úÖ APROVADO: {video_id} (SEM legendas, conf: {confidence:.2f})")
            else:
                logger.info(f"   ‚ùå REPROVADO: {video_id} (COM legendas, conf: {confidence:.2f})")
                logger.info(f"      Texto detectado: '{sample_text[:100]}'")
            
            return aprovado, result_meta
            
        except Exception as e:
            logger.error(f"‚ùå Erro na valida√ß√£o: {e}", exc_info=True)
            # Em caso de erro, rejeitar por seguran√ßa
            return False, {'error': str(e), 'video_id': video_id}
    
    async def approve_video(self, video_id: str, transform_path: str, metadata: Dict):
        """
        4a. APPROVE: Mover v√≠deo aprovado para data/approved/
        
        Args:
            video_id: ID do v√≠deo
            transform_path: Caminho do v√≠deo transformado
            metadata: Metadados da valida√ß√£o
        """
        logger.info(f"‚úÖ APPROVE: Movendo {video_id} para approved/")
        
        try:
            transform_video = Path(transform_path)
            approved_path = Path(f"data/approved/videos/{video_id}.mp4")
            
            # Mover (n√£o copiar) para economizar espa√ßo
            if transform_video.exists():
                transform_video.rename(approved_path)
                logger.info(f"   ‚úÖ Movido: {approved_path}")
            
            # Limpar pastas anteriores
            await self._cleanup_previous_stages(video_id)
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao aprovar: {e}", exc_info=True)
    
    async def reject_video(self, video_id: str, metadata: Dict):
        """
        4b. REJECT: Adicionar ao blacklist e limpar
        
        Args:
            video_id: ID do v√≠deo
            metadata: Metadados da valida√ß√£o (motivo da rejei√ß√£o)
        """
        logger.info(f"‚ùå REJECT: Adicionando {video_id} ao blacklist")
        
        try:
            # Adicionar ao blacklist
            confidence = metadata.get('confidence', 0.0)
            reason = f"Legendas detectadas (conf: {confidence:.2f})"
            self.blacklist.add(  # Sync call
                video_id=video_id,
                reason=reason,
                confidence=confidence,
                metadata=metadata
            )
            
            logger.info(f"   ‚ö´ Blacklisted: {video_id}")
            
            # Limpar todas as pastas
            await self._cleanup_all_stages(video_id)
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao rejeitar: {e}", exc_info=True)
    
    async def _cleanup_previous_stages(self, video_id: str):
        """
        5. CLEANUP: Remover v√≠deo de pastas anteriores (aprovado)
        
        Remove de:
        - data/raw/shorts/
        - data/transform/videos/
        """
        logger.info(f"üßπ CLEANUP: Removendo {video_id} de pastas anteriores")
        
        raw_dir = Path("data/raw/shorts")
        transform_dir = Path("data/transform/videos")

        # Remove todas as variantes de extens√£o no raw (ex: .mp4, .webm, .mkv)
        for path in raw_dir.glob(f"{video_id}.*"):
            if path.is_file():
                path.unlink()
                logger.info(f"   üóëÔ∏è  Removido: {path}")

        # Remove transformado (normalmente .mp4)
        for path in transform_dir.glob(f"{video_id}.*"):
            if path.is_file():
                path.unlink()
                logger.info(f"   üóëÔ∏è  Removido: {path}")
    
    async def _cleanup_all_stages(self, video_id: str):
        """
        5. CLEANUP: Remover v√≠deo de TODAS as pastas (rejeitado)
        
        Remove de:
        - data/raw/shorts/
        - data/transform/videos/
        - data/validate/in_progress/
        - data/approved/videos/ (se existir)
        """
        logger.info(f"üßπ CLEANUP COMPLETO: Removendo {video_id} de todas as pastas")
        
        stage_dirs = [
            Path("data/raw/shorts"),
            Path("data/transform/videos"),
            Path("data/validate/in_progress"),
            Path("data/approved/videos"),
        ]

        for stage_dir in stage_dirs:
            for path in stage_dir.glob(f"{video_id}.*"):
                if path.is_file():
                    path.unlink()
                    logger.info(f"   üóëÔ∏è  Removido: {path}")
    
    async def process_pipeline(self, query: str, max_shorts: int = 50) -> Dict:
        """
        Pipeline completo: Download ‚Üí Transform ‚Üí Validate ‚Üí Approve/Reject
        
        Args:
            query: Query de busca
            max_shorts: M√°ximo de shorts para processar
        
        Returns:
            Estat√≠sticas do pipeline
        """
        logger.info(f"üöÄ PIPELINE INICIADO: '{query}' (max: {max_shorts})")
        
        stats = {
            'query': query,
            'downloaded': 0,
            'transformed': 0,
            'approved': 0,
            'rejected': 0,
            'errors': 0,
            'start_time': datetime.utcnow().isoformat()
        }
        
        # 1. DOWNLOAD
        shorts = await self.download_shorts(query, max_shorts)
        stats['downloaded'] = len(shorts)
        
        if not shorts:
            logger.warning("‚ö†Ô∏è  Nenhum short baixado. Pipeline finalizado.")
            stats['end_time'] = datetime.utcnow().isoformat()
            return stats
        
        # 2. TRANSFORM + 3. VALIDATE + 4. APPROVE/REJECT
        processed_video_ids = set()

        for short in shorts:
            video_id = short['video_id']
            raw_path = short['raw_path']

            if video_id in processed_video_ids:
                logger.info(f"   üîÅ DUPLICADO no pipeline final (skip): {video_id}")
                continue

            processed_video_ids.add(video_id)
            
            try:
                # 2. Transform
                transform_path = self.transform_video(video_id, raw_path)
                if not transform_path:
                    stats['errors'] += 1
                    await self._cleanup_all_stages(video_id)
                    continue
                stats['transformed'] += 1
                
                # 3. Validate
                aprovado, metadata = self.validate_video(video_id, transform_path)
                
                # 4. Approve ou Reject
                if aprovado:
                    await self.approve_video(video_id, transform_path, metadata)
                    stats['approved'] += 1
                else:
                    await self.reject_video(video_id, metadata)
                    stats['rejected'] += 1
                
            except Exception as e:
                logger.error(f"‚ùå Erro processando {video_id}: {e}", exc_info=True)
                stats['errors'] += 1
                await self._cleanup_all_stages(video_id)
                continue
        
        stats['end_time'] = datetime.utcnow().isoformat()
        
        logger.info(f"üéâ PIPELINE COMPLETO:")
        logger.info(f"   üì• Downloaded: {stats['downloaded']}")
        logger.info(f"   üîÑ Transformed: {stats['transformed']}")
        logger.info(f"   ‚úÖ Approved: {stats['approved']}")
        logger.info(f"   ‚ùå Rejected: {stats['rejected']}")
        logger.info(f"   ‚ö†Ô∏è  Errors: {stats['errors']}")
        
        return stats
