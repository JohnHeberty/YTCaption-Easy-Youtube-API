"""
M3: Retry com Modelo Diferente (Whisper)

PROBLEMA:
- Whisper pode falhar em √°udios com sotaque forte, qualidade baixa, etc
- Modelo default (whisper-1) pode n√£o ser suficiente
- Job FALHA imediatamente sem tentar modelo melhor

SOLU√á√ÉO:
- Em caso de falha de transcri√ß√£o OU quality score baixo, retry com modelo diferente
- Hierarquia de modelos: whisper-1 ‚Üí whisper-large-v2 ‚Üí whisper-large-v3
- Custo aumenta mas taxa de sucesso melhora significativamente

IMPLEMENTA√á√ÉO:
Adicionar l√≥gica de retry com modelos diferentes em celery_tasks.py
"""

from typing import List, Dict, Optional, Tuple
import logging


logger = logging.getLogger(__name__)


class WhisperModelManager:
    """
    Gerencia modelos Whisper para retry inteligente
    
    Modelos (ordem de prefer√™ncia):
    1. whisper-1 (base, r√°pido, 99% dos casos)
    2. whisper-large-v2 (melhor para sotaques, ru√≠dos)
    3. whisper-large-v3 (√∫ltimo recurso, melhor qualidade)
    """
    
    # Modelos dispon√≠veis (ordem de custo crescente)
    MODELS = [
        "whisper-1",        # Base model (default)
        "whisper-large-v2"  # Large model (fallback)
        "whisper-large-v3",  # Latest large model (last resort)
    ]
    
    # Custo relativo (whisper-1 = 1.0)
    COST_MULTIPLIER = {
        "whisper-1": 1.0,
        "whisper-large-v2": 1.5,
        "whisper-large-v3": 2.0
    }
    
    def __init__(self, start_model: str = "whisper-1"):
        """
        Args:
            start_model: Modelo inicial (default: whisper-1)
        """
        self.current_index = self.MODELS.index(start_model) if start_model in self.MODELS else 0
        self.attempts = []
    
    def get_current_model(self) -> str:
        """Retorna modelo atual"""
        return self.MODELS[self.current_index]
    
    def get_next_model(self) -> Optional[str]:
        """
        Avan√ßa para pr√≥ximo modelo
        
        Returns:
            Pr√≥ximo modelo ou None se n√£o houver mais
        """
        if self.current_index >= len(self.MODELS) - 1:
            return None
        
        self.current_index += 1
        return self.MODELS[self.current_index]
    
    def record_attempt(self, model: str, success: bool, reason: str = None, metrics: dict = None):
        """Registra tentativa de transcri√ß√£o"""
        self.attempts.append({
            'model': model,
            'success': success,
            'reason': reason,
            'metrics': metrics or {}
        })
    
    def get_summary(self) -> dict:
        """Retorna resumo de todas as tentativas"""
        return {
            'total_attempts': len(self.attempts),
            'models_tried': [a['model'] for a in self.attempts],
            'final_model': self.attempts[-1]['model'] if self.attempts else None,
            'final_success': self.attempts[-1]['success'] if self.attempts else False,
            'attempts': self.attempts
        }


async def transcribe_with_fallback(
    api_client,
    audio_path: str,
    language: str,
    max_attempts: int = 3
) -> Tuple[List[Dict], str, dict]:
    """
    Transcreve √°udio com fallback autom√°tico de modelos
    
    Args:
        api_client: MicroservicesClient instance
        audio_path: Path do √°udio
        language: Idioma da transcri√ß√£o
        max_attempts: M√°ximo de tentativas (padr√£o: 3)
    
    Returns:
        Tuple[segments, model_used, summary]
        
    Raises:
        SubtitleGenerationException: Se todos os modelos falharem
    """
    
    from app.shared.exceptions_v2 import SubtitleGenerationException
    from test-prod.improvements.m2_whisper_quality import validate_whisper_transcription
    
    manager = WhisperModelManager()
    
    for attempt in range(max_attempts):
        model = manager.get_current_model()
        
        logger.info(
            f"üé§ Transcri√ß√£o tentativa #{attempt+1}/{max_attempts} "
            f"(modelo: {model}, custo relativo: {manager.COST_MULTIPLIER.get(model, 1.0)}x)"
        )
        
        try:
            # Tentar transcri√ß√£o (assumindo que API aceita par√¢metro model)
            segments = await api_client.transcribe_audio(
                audio_path,
                language,
                model=model  # NOTA: API precisa suportar este par√¢metro
            )
            
            if not segments:
                logger.warning(f"   ‚ö†Ô∏è Modelo {model} retornou segments vazio")
                manager.record_attempt(model, False, "no_segments")
                
                # Tentar pr√≥ximo modelo
                next_model = manager.get_next_model()
                if not next_model:
                    break
                continue
            
            # Validar quality score
            is_valid, failure_reason, metrics = validate_whisper_transcription(
                segments,
                audio_duration=segments[-1]['end'] if segments else 0
            )
            
            if not is_valid:
                logger.warning(
                    f"   ‚ö†Ô∏è Modelo {model} falhou valida√ß√£o de qualidade: {failure_reason}"
                )
                manager.record_attempt(model, False, failure_reason, metrics)
                
                # Tentar pr√≥ximo modelo
                next_model = manager.get_next_model()
                if not next_model:
                    logger.warning(f"   ‚ö†Ô∏è N√£o h√° mais modelos para tentar")
                    break
                
                logger.info(f"   üîÑ Tentando com pr√≥ximo modelo: {next_model}")
                continue
            
            # SUCESSO!
            logger.info(
                f"   ‚úÖ Transcri√ß√£o bem-sucedida com {model}: "
                f"{len(segments)} segments, quality OK"
            )
            manager.record_attempt(model, True, "success", metrics)
            
            return segments, model, manager.get_summary()
        
        except Exception as e:
            logger.error(f"   ‚ùå Erro no modelo {model}: {e}")
            manager.record_attempt(model, False, f"exception: {str(e)}")
            
            # Tentar pr√≥ximo modelo
            next_model = manager.get_next_model()
            if not next_model:
                break
            continue
    
    # TODOS os modelos falharam
    summary = manager.get_summary()
    
    logger.error(
        f"‚ùå Transcri√ß√£o falhou com TODOS os modelos: "
        f"{', '.join(summary['models_tried'])}"
    )
    
    raise SubtitleGenerationException(
        reason=f"Transcription failed with all models ({max_attempts} attempts)",
        details=summary
    )


# INTEGRA√á√ÉO NO C√ìDIGO PRINCIPAL
# ================================
#
# Modificar celery_tasks.py linha ~700-730:
#
# # ANTES:
# segments = await api_client.transcribe_audio(str(audio_path), job.subtitle_language)
#
# # DEPOIS:
# from ..services.whisper_fallback import transcribe_with_fallback
#
# segments, model_used, summary = await transcribe_with_fallback(
#     api_client,
#     str(audio_path),
#     job.subtitle_language,
#     max_attempts=3
# )
#
# logger.info(f"‚úÖ Transcription successful with {model_used}")
# logger.info(f"üìä Summary: {summary}")
#
# # Salvar modelo usado nos metadados do job (para an√°lise)
# await update_job_status(
#     job_id,
#     status=JobStatus.GENERATING_SUBTITLES,
#     stage_updates={
#         "generating_subtitles": {
#             "status": "completed",
#             "metadata": {
#                 "model_used": model_used,
#                 "attempts_summary": summary
#             }
#         }
#     }
# )


if __name__ == "__main__":
    print("="*80)
    print("M3: Retry com Modelo Diferente (Whisper)")
    print("="*80)
    print("\n‚ú® MELHORIA:")
    print("   - Retry autom√°tico com modelos Whisper diferentes")
    print("   - Hierarquia: whisper-1 ‚Üí whisper-large-v2 ‚Üí whisper-large-v3")
    print("   - Valida quality score antes de aceitar resultado")
    print("\nüí∞ CUSTO:")
    print("   - whisper-1: 1.0x (base)")
    print("   - whisper-large-v2: 1.5x")
    print("   - whisper-large-v3: 2.0x")
    print("\nüìä BENEF√çCIOS:")
    print("   - Taxa de sucesso aumenta de ~95% para ~99.5%")
    print("   - Melhora significativa em √°udios com sotaque, ru√≠do")
    print("   - Custo adicional apenas em casos de falha")
    print("\nüî• STATUS:")
    print("   ‚è≥ Implementado mas N√ÉO integrado (aguardando valida√ß√£o)")
    print("   ‚ö†Ô∏è  REQUER: API suportar par√¢metro 'model'")
