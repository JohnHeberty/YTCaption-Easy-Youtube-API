"""
Test de AcurÃ¡cia - Sprint 06 vs Sprint 07
Meta: â‰¥90% de acurÃ¡cia

Este teste mede a acurÃ¡cia real em subset de vÃ­deos de validaÃ§Ã£o.
"""

import pytest
import json
from pathlib import Path
from typing import Dict, List, Tuple

from app.video_processing.ensemble_detector import EnsembleSubtitleDetector


class TestAccuracyMeasurement:
    """Testes de acurÃ¡cia em dataset real"""
    
    @pytest.fixture(scope="class")
    def validation_videos(self):
        """Carrega vÃ­deos de validaÃ§Ã£o com ground truth"""
        storage = Path(__file__).parent.parent / "storage" / "validation"
        
        # Carregar ground truth dos samples
        ok_path = storage / "sample_OK"
        not_ok_path = storage / "sample_NOT_OK"
        
        videos = {}
        
        # VÃ­deos COM legendas (sample_OK)
        if ok_path.exists():
            ok_videos = list(ok_path.glob("*.mp4"))[:10]  # Primeiros 10
            for video in ok_videos:
                videos[str(video)] = True
        
        # VÃ­deos SEM legendas (sample_NOT_OK)
        if not_ok_path.exists():
            not_ok_videos = list(not_ok_path.glob("*.mp4"))[:10]  # Primeiros 10
            for video in not_ok_videos:
                videos[str(video)] = False
        
        print(f"\nğŸ“Š Dataset carregado: {len(videos)} vÃ­deos")
        print(f"   - Com legendas: {sum(1 for v in videos.values() if v)}")
        print(f"   - Sem legendas: {sum(1 for v in videos.values() if not v)}")
        
        return videos
    
    def calculate_metrics(self, results: List[Tuple[bool, bool]]) -> Dict:
        """
        Calcula mÃ©tricas de acurÃ¡cia
        
        Args:
            results: Lista de tuplas (expected, predicted)
        
        Returns:
            Dict com accuracy, precision, recall, f1
        """
        tp = sum(1 for exp, pred in results if exp and pred)
        tn = sum(1 for exp, pred in results if not exp and not pred)
        fp = sum(1 for exp, pred in results if not exp and pred)
        fn = sum(1 for exp, pred in results if exp and not pred)
        
        total = len(results)
        accuracy = (tp + tn) / total * 100 if total > 0 else 0
        
        precision = tp / (tp + fp) * 100 if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) * 100 if (tp + fn) > 0 else 0
        
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'confusion_matrix': {
                'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn
            },
            'total': total
        }
    
    def test_sprint06_baseline(self, validation_videos):
        """
        Teste 1: Sprint 06 - Baseline (weighted voting)
        """
        print("\n" + "="*70)
        print("ğŸ¯ TESTE 1: SPRINT 06 BASELINE (WEIGHTED VOTING)")
        print("="*70)
        
        # Ensemble Sprint 06
        ensemble = EnsembleSubtitleDetector(
            voting_method='weighted'
        )
        
        results = []
        errors = []
        
        for i, (video_path, expected) in enumerate(validation_videos.items(), 1):
            video_name = Path(video_path).name
            print(f"\n[{i}/{len(validation_videos)}] ğŸ¥ {video_name}")
            print(f"   Ground Truth: {'âœ… COM legendas' if expected else 'âŒ SEM legendas'}")
            
            try:
                result = ensemble.detect(video_path)
                predicted = result['has_subtitles']
                confidence = result['confidence']
                
                results.append((expected, predicted))
                
                # Verificar se acertou
                correct = (expected == predicted)
                status = "âœ… CORRETO" if correct else "âŒ ERRO"
                
                print(f"   PrediÃ§Ã£o: {'âœ… COM legendas' if predicted else 'âŒ SEM legendas'} (conf: {confidence:.1f}%)")
                print(f"   Status: {status}")
                
                if not correct:
                    errors.append({
                        'video': video_name,
                        'expected': expected,
                        'predicted': predicted,
                        'confidence': confidence,
                        'votes': result.get('votes', {})
                    })
            
            except Exception as e:
                print(f"   âš ï¸ ERRO: {e}")
                results.append((expected, False))  # Assume erro = sem legendas
        
        # Calcular mÃ©tricas
        metrics = self.calculate_metrics(results)
        
        print("\n" + "="*70)
        print("ğŸ“Š RESULTADOS SPRINT 06 BASELINE")
        print("="*70)
        print(f"Total de vÃ­deos:  {metrics['total']}")
        print(f"AcurÃ¡cia:         {metrics['accuracy']:.2f}%")
        print(f"PrecisÃ£o:         {metrics['precision']:.2f}%")
        print(f"Recall:           {metrics['recall']:.2f}%")
        print(f"F1-Score:         {metrics['f1']:.2f}%")
        print(f"\nMatriz de ConfusÃ£o:")
        cm = metrics['confusion_matrix']
        print(f"  TP (Verdadeiro Positivo): {cm['tp']}")
        print(f"  TN (Verdadeiro Negativo): {cm['tn']}")
        print(f"  FP (Falso Positivo):      {cm['fp']}")
        print(f"  FN (Falso Negativo):      {cm['fn']}")
        
        if errors:
            print(f"\nâŒ Erros ({len(errors)}):")
            for err in errors:
                print(f"   - {err['video']}: esperado={err['expected']}, predito={err['predicted']} (conf={err['confidence']:.1f}%)")
        
        print("="*70)
        
        # Salvar resultados
        results_file = Path(__file__).parent / "accuracy_results_sprint06.json"
        with open(results_file, 'w') as f:
            json.dump({
                'sprint': '06',
                'metrics': metrics,
                'errors': errors
            }, f, indent=2)
        
        print(f"\nğŸ’¾ Resultados salvos em: {results_file}")
        
        # Armazenar para comparaÃ§Ã£o
        self.sprint06_metrics = metrics
        
        assert metrics['total'] > 0, "Nenhum vÃ­deo testado"
        assert metrics['accuracy'] > 0, "AcurÃ¡cia Ã© 0%"
    
    def test_sprint07_advanced(self, validation_videos):
        """
        Teste 2: Sprint 07 - Advanced (confidence-weighted + anÃ¡lise)
        Meta: â‰¥90% de acurÃ¡cia
        """
        print("\n" + "="*70)
        print("ğŸ¯ TESTE 2: SPRINT 07 ADVANCED (CONFIDENCE-WEIGHTED)")
        print("="*70)
        
        # Ensemble Sprint 07
        ensemble = EnsembleSubtitleDetector(
            voting_method='confidence_weighted',
            enable_conflict_detection=True,
            enable_uncertainty_estimation=True
        )
        
        results = []
        errors = []
        high_conflicts = 0
        high_uncertainty = 0
        
        for i, (video_path, expected) in enumerate(validation_videos.items(), 1):
            video_name = Path(video_path).name
            print(f"\n[{i}/{len(validation_videos)}] ğŸ¥ {video_name}")
            print(f"   Ground Truth: {'âœ… COM legendas' if expected else 'âŒ SEM legendas'}")
            
            try:
                result = ensemble.detect(video_path)
                predicted = result['has_subtitles']
                confidence = result['confidence']
                
                results.append((expected, predicted))
                
                # AnÃ¡lise de conflito
                conflict = result.get('conflict_analysis', {})
                if conflict.get('detected') and conflict.get('severity') == 'high':
                    high_conflicts += 1
                    print(f"   âš ï¸ Conflito Alto detectado!")
                
                # AnÃ¡lise de incerteza
                uncertainty = result.get('uncertainty', {})
                if uncertainty.get('level') == 'high':
                    high_uncertainty += 1
                    print(f"   âš ï¸ Incerteza Alta: {uncertainty.get('aggregate', 0):.3f}")
                
                # Verificar se acertou
                correct = (expected == predicted)
                status = "âœ… CORRETO" if correct else "âŒ ERRO"
                
                print(f"   PrediÃ§Ã£o: {'âœ… COM legendas' if predicted else 'âŒ SEM legendas'} (conf: {confidence:.1f}%)")
                print(f"   Status: {status}")
                
                if not correct:
                    errors.append({
                        'video': video_name,
                        'expected': expected,
                        'predicted': predicted,
                        'confidence': confidence,
                        'votes': result.get('votes', {}),
                        'conflict': conflict.get('severity', 'none'),
                        'uncertainty': uncertainty.get('level', 'unknown')
                    })
            
            except Exception as e:
                print(f"   âš ï¸ ERRO: {e}")
                results.append((expected, False))
        
        # Calcular mÃ©tricas
        metrics = self.calculate_metrics(results)
        
        print("\n" + "="*70)
        print("ğŸ“Š RESULTADOS SPRINT 07 ADVANCED")
        print("="*70)
        print(f"Total de vÃ­deos:   {metrics['total']}")
        print(f"AcurÃ¡cia:          {metrics['accuracy']:.2f}% â­")
        print(f"PrecisÃ£o:          {metrics['precision']:.2f}%")
        print(f"Recall:            {metrics['recall']:.2f}%")
        print(f"F1-Score:          {metrics['f1']:.2f}%")
        print(f"\nAnÃ¡lise AvanÃ§ada:")
        print(f"  Conflitos Altos:   {high_conflicts}")
        print(f"  Incerteza Alta:    {high_uncertainty}")
        print(f"\nMatriz de ConfusÃ£o:")
        cm = metrics['confusion_matrix']
        print(f"  TP (Verdadeiro Positivo): {cm['tp']}")
        print(f"  TN (Verdadeiro Negativo): {cm['tn']}")
        print(f"  FP (Falso Positivo):      {cm['fp']}")
        print(f"  FN (Falso Negativo):      {cm['fn']}")
        
        if errors:
            print(f"\nâŒ Erros ({len(errors)}):")
            for err in errors:
                print(f"   - {err['video']}: esperado={err['expected']}, predito={err['predicted']}")
                print(f"     Conf={err['confidence']:.1f}%, Conflito={err['conflict']}, Incerteza={err['uncertainty']}")
        
        print("="*70)
        
        # Salvar resultados
        results_file = Path(__file__).parent / "accuracy_results_sprint07.json"
        with open(results_file, 'w') as f:
            json.dump({
                'sprint': '07',
                'metrics': metrics,
                'errors': errors,
                'analysis': {
                    'high_conflicts': high_conflicts,
                    'high_uncertainty': high_uncertainty
                }
            }, f, indent=2)
        
        print(f"\nğŸ’¾ Resultados salvos em: {results_file}")
        
        # Armazenar para comparaÃ§Ã£o
        self.sprint07_metrics = metrics
        
        # VerificaÃ§Ãµes
        assert metrics['total'] > 0, "Nenhum vÃ­deo testado"
        assert metrics['accuracy'] > 0, "AcurÃ¡cia Ã© 0%"
        
        # META: â‰¥90% de acurÃ¡cia
        if metrics['accuracy'] >= 90.0:
            print("\n" + "="*70)
            print("ğŸ‰ğŸ‰ğŸ‰ META DE 90% DE ACURÃCIA ATINGIDA! ğŸ‰ğŸ‰ğŸ‰")
            print("="*70)
        else:
            print("\n" + "="*70)
            print(f"âš ï¸ Meta nÃ£o atingida: {metrics['accuracy']:.2f}% (meta: â‰¥90%)")
            print(f"   Faltam: {90.0 - metrics['accuracy']:.2f} pontos percentuais")
            print("="*70)
    
    def test_comparison_summary(self):
        """
        Teste 3: ComparaÃ§Ã£o Sprint 06 vs Sprint 07
        """
        print("\n" + "="*70)
        print("ğŸ“Š COMPARAÃ‡ÃƒO: SPRINT 06 vs SPRINT 07")
        print("="*70)
        
        if not hasattr(self, 'sprint06_metrics') or not hasattr(self, 'sprint07_metrics'):
            pytest.skip("MÃ©tricas anteriores nÃ£o disponÃ­veis")
        
        s06 = self.sprint06_metrics
        s07 = self.sprint07_metrics
        
        print(f"\n{'MÃ©trica':<20} {'Sprint 06':<15} {'Sprint 07':<15} {'Melhoria':<15}")
        print("-" * 70)
        
        metrics_names = [
            ('AcurÃ¡cia', 'accuracy'),
            ('PrecisÃ£o', 'precision'),
            ('Recall', 'recall'),
            ('F1-Score', 'f1')
        ]
        
        improvements = []
        for name, key in metrics_names:
            v06 = s06[key]
            v07 = s07[key]
            diff = v07 - v06
            
            symbol = "ğŸ“ˆ" if diff > 0 else ("ğŸ“‰" if diff < 0 else "â¡ï¸")
            improvements.append(diff)
            
            print(f"{name:<20} {v06:>6.2f}%{'':<8} {v07:>6.2f}%{'':<8} {symbol} {diff:>+6.2f} pp")
        
        print("="*70)
        
        avg_improvement = sum(improvements) / len(improvements)
        
        print(f"\nğŸ“Š Resumo:")
        print(f"   Melhoria MÃ©dia: {avg_improvement:+.2f} pontos percentuais")
        
        if s07['accuracy'] >= 90.0:
            print(f"   Status: âœ… META DE 90% ATINGIDA ({s07['accuracy']:.2f}%)")
        else:
            print(f"   Status: âš ï¸ Meta nÃ£o atingida: {s07['accuracy']:.2f}% (faltam {90.0 - s07['accuracy']:.2f} pp)")
        
        if avg_improvement > 0:
            print(f"   ConclusÃ£o: âœ… Sprint 07 Ã© superior ao Sprint 06")
        elif avg_improvement == 0:
            print(f"   ConclusÃ£o: â¡ï¸ Sprint 07 equivalente ao Sprint 06")
        else:
            print(f"   ConclusÃ£o: âš ï¸ Sprint 07 inferior ao Sprint 06 (investigar)")
        
        print("="*70)
        
        # Salvar comparaÃ§Ã£o
        comparison_file = Path(__file__).parent / "accuracy_comparison.json"
        with open(comparison_file, 'w') as f:
            json.dump({
                'sprint_06': s06,
                'sprint_07': s07,
                'improvement': {
                    'accuracy': improvements[0],
                    'precision': improvements[1],
                    'recall': improvements[2],
                    'f1': improvements[3],
                    'average': avg_improvement
                },
                'meta_90_percent': {
                    'achieved': s07['accuracy'] >= 90.0,
                    'value': s07['accuracy'],
                    'gap': 90.0 - s07['accuracy'] if s07['accuracy'] < 90.0 else 0
                }
            }, f, indent=2)
        
        print(f"\nğŸ’¾ ComparaÃ§Ã£o salva em: {comparison_file}")
        
        # Assert: Sprint 07 deve ser pelo menos igual ou superior
        assert avg_improvement >= -1.0, f"Sprint 07 muito inferior (-{abs(avg_improvement):.2f} pp)"
