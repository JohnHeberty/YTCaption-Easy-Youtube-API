# Sprint 07: ROC Calibration & Threshold Tuning

**Objetivo**: Calibrar probabilidades do classificador e otimizar threshold final via ROC curve  
**Impacto Esperado**: +1-3% (precision boost via threshold Ã³timo + calibraÃ§Ã£o)  
**Criticidade**: â­â­â­ IMPORTANTE (Finaliza tuning do sistema)  
**Data**: 2026-02-13  
**Status**: ğŸŸ¡ Aguardando Sprint 06  
**DependÃªncias**: Sprint 06 (classificador treinado), Dataset validaÃ§Ã£o (50+ vÃ­deos)

---

## 1ï¸âƒ£ Objetivo TÃ©cnico Claro

### Problema EspecÃ­fico

O classificador da Sprint 06 retorna **probabilidades nÃ£o-calibradas**:

```python
# OUTPUT Sprint 06 (LogisticRegression)
probability = 0.92  # Mas isso NÃƒO significa "92% de chance real"
```

**Problemas CrÃ­ticos:**

### 1) **Probabilidades nÃ£o-calibradas**

LogisticRegression **nÃ£o garante** que `predict_proba()` seja calibrado:

```
VÃ­deo A: proba=0.80 â†’ ground truth: True  (80% ok? âœ…)
VÃ­deo B: proba=0.80 â†’ ground truth: False (80% ok? âŒ)
VÃ­deo C: proba=0.80 â†’ ground truth: True  (80% ok? âœ…)
VÃ­deo D: proba=0.80 â†’ ground truth: False (80% ok? âŒ)

AnÃ¡lise de 100 vÃ­deos com proba â‰ˆ 0.80:
  - Positivos reais: 45 (esperado: 80, mas foi 45!)
  - CalibraÃ§Ã£o: 45% != 80% (descalibrado!)
```

**Problema**: Probabilidade `0.80` deveria significar "80% de chance", mas na prÃ¡tica Ã© **45%**.

**Impacto**:
- DecisÃµes baseadas em probabilidade ficam **erradas**
- Threshold selecionado pode ser **subÃ³timo**
- Dificulta interpretaÃ§Ã£o para usuÃ¡rios ("92% confiÃ¡vel" Ã© mentira)

---

### 2) **Threshold selecionado na Sprint 06 Ã© "good enough", nÃ£o "Ã³timo"**

Sprint 06 seleciona threshold via **max F1**:

```python
# Sprint 06: grid search simples
best_threshold = 0.5  # Inicial
for threshold in np.arange(0.30, 0.91, 0.05):  # Step 0.05 (grosso!)
    f1 = compute_f1(y_val, y_val_proba >= threshold)
    if f1 > best_f1:
        best_threshold = threshold
```

**Problemas**:
1. **Step 0.05 Ã© grosso**: pode perder threshold Ã³timo entre 0.75 e 0.80
2. **MÃ©trica fixa (F1)**: nÃ£o considera trade-off precision/recall
3. **Sem anÃ¡lise de custo**: FP e FN tÃªm custo diferente (negÃ³cio)

**Exemplo**:
```
Threshold 0.75: Precision=94%, Recall=96%, F1=95.0%
Threshold 0.78: Precision=95%, Recall=95%, F1=95.0%  â† Tie!
Threshold 0.80: Precision=96%, Recall=94%, F1=94.9%

Sprint 06 escolhe: 0.75 ou 0.78 (primeiro com max F1)
Sprint 07 escolhe: 0.80 (se custo de FP > custo de FN)
```

---

### 3) **Sem anÃ¡lise de trade-off Precision/Recall**

Sistema atual nÃ£o **visualiza** nem **documenta** o trade-off:

```
Threshold 0.50: Precision=88%, Recall=98%  (mais recall, aceita FP)
Threshold 0.75: Precision=95%, Recall=95%  (balanceado)
Threshold 0.90: Precision=98%, Recall=88%  (mais precision, perde recall)

Qual escolher? Depende do CUSTO DO ERRO (negÃ³cio), nÃ£o apenas F1!
```

**Custo do erro (exemplo real):**
- **FP (falso positivo)**: VÃ­deo sem legenda classificado como "tem legenda"
  - UsuÃ¡rio tenta extrair â†’ processo falha â†’ frustraÃ§Ã£o â†’ custo **BAIXO** (retry)
  
- **FN (falso negativo)**: VÃ­deo com legenda classificado como "sem legenda"
  - Sistema nÃ£o processa â†’ **PERDE LEGENDA** â†’ usuÃ¡rio nÃ£o sabe que havia legenda â†’ custo **ALTO** (informaÃ§Ã£o perdida)

**Se FN custa 3Ã— mais que FP**, threshold Ã³timo **nÃ£o Ã©** max F1, Ã© **max cost-weighted metric**.

---

### 4) **Sem monitoramento de drift**

ApÃ³s deploy, sistema **nÃ£o monitora** se probabilidades continuam calibradas:

```
Semana 1: proba=0.80 â†’ acurÃ¡cia 78% (ok)
Semana 4: proba=0.80 â†’ acurÃ¡cia 65% (descalibrado!)  â† Drift detectado!

Causa possÃ­vel: novos tipos de vÃ­deos (TikTok, shorts) nÃ£o vistos no treino
```

Sprint 07 prepara **monitoramento de calibraÃ§Ã£o** em produÃ§Ã£o.

---

### MÃ©trica Impactada

| MÃ©trica | After Sprint 06 | Alvo Sprint 07 | ValidaÃ§Ã£o |
|---------|----------------|----------------|-----------|
| **Precision** | ~97% | ~98% (+1%) | Via threshold Ã³timo calibrado |
| **Recall** | ~97% | ~97% (mantÃ©m) | Garante no drop |
| **FPR** | ~0.5% | ~0.3% (-0.2%) | Threshold mais conservador se custo FP alto |
| **F1 Score** | ~97% | ~97.5% (+0.5%) | BalanÃ§o precision/recall |
| **Brier Score** | ~0.08 | ~0.04 (-0.04) | Melhora calibraÃ§Ã£o (0=perfeito) |
| **ECE (Expected Calibration Error)** | ~0.12 | ~0.05 (-0.07) | Probabilidades mais confiÃ¡veis |

**Nota Importante:**

Sprint 07 Ã© **refinamento** (vs Sprint 05-06 que foram transformacionais).

Ganho esperado +1-3%:
- **CenÃ¡rio conservador**: +0.5-1% (calibraÃ§Ã£o melhora pouco)
- **CenÃ¡rio realista**: +1-2% (threshold Ã³timo + calibraÃ§Ã£o)
- **CenÃ¡rio otimista**: +2-3% (se descalibraÃ§Ã£o inicial for severa)

Impacto principal: **confiabilidade** (probabilidades corretas) + **interpretabilidade**.

---

## 2ï¸âƒ£ HipÃ³tese TÃ©cnica

### Por Que CalibraÃ§Ã£o Aumenta Performance?

**Problema Raiz**: LogisticRegression minimiza **log-loss**, nÃ£o **calibraÃ§Ã£o**.

**Fato EmpÃ­rico (ML Theory):**

LogReg pode ter **alta acurÃ¡cia** mas **probabilidades descalibradas**:

```
Modelo A: Accuracy=95%, Brier Score=0.10 (descalibrado)
Modelo B: Accuracy=95%, Brier Score=0.04 (calibrado)

Ambos tÃªm mesma accuracy, mas B tem probabilidades confiÃ¡veis!
```

**HipÃ³tese:**

Ao **calibrar probabilidades** via Platt scaling ou Isotonic regression:
1. Probabilidades refletem **chance real** (interpretÃ¡vel)
2. Threshold selecionado via ROC Ã© **mais robusto**
3. DecisÃµes baseadas em probabilidade ficam **corretas**

---

### Base Conceitual (Calibration Theory)

#### DefiniÃ§Ã£o: Probabilidade Calibrada

Um modelo estÃ¡ **calibrado** se:

$$
P(\text{Positive} \mid \hat{p} = p) = p
$$

Ou seja: entre todos os exemplos com probabilidade predita $\hat{p} = 0.80$, **80% devem ser positivos**.

**Teste de calibraÃ§Ã£o (Brier Score):**

$$
\text{Brier Score} = \frac{1}{N} \sum_{i=1}^{N} (\hat{p}_i - y_i)^2
$$

- 0 = perfeito (probabilidades exatas)
- 1 = pior (probabilidades completamente erradas)

**Teste de calibraÃ§Ã£o (Expected Calibration Error - ECE):**

$$
\text{ECE} = \sum_{b=1}^{B} \frac{n_b}{N} \left| \text{acc}(b) - \text{conf}(b) \right|
$$

onde:
- $B$: nÃºmero de bins (ex: 10 bins de 0-0.1, 0.1-0.2, ..., 0.9-1.0)
- $n_b$: nÃºmero de exemplos no bin $b$
- $\text{acc}(b)$: acurÃ¡cia no bin $b$
- $\text{conf}(b)$: confianÃ§a mÃ©dia no bin $b$

**ECE = 0** â†’ perfeitamente calibrado.

---

#### MÃ©todo 1: Platt Scaling

**Ideia**: Treinar regressÃ£o logÃ­stica **sobre as probabilidades** (meta-modelo):

$$
\hat{p}_{\text{calibrated}} = \sigma(a \cdot \log \frac{\hat{p}}{1 - \hat{p}} + b)
$$

onde $\sigma$ Ã© sigmoid, $a$ e $b$ sÃ£o aprendidos em **validation set separado**.

**Vantagens**:
- âœ… RÃ¡pido (2 parÃ¢metros apenas)
- âœ… Funciona bem para LogisticRegression
- âœ… Preserva ordem (ranking)

**Desvantagens**:
- âŒ Assume forma paramÃ©trica (logÃ­stica)
- âŒ Pode nÃ£o corrigir descalibraÃ§Ã£o nÃ£o-linear

---

#### MÃ©todo 2: Isotonic Regression

**Ideia**: Mapear probabilidades via **funÃ§Ã£o monotÃ´nica nÃ£o-paramÃ©trica**:

$$
\hat{p}_{\text{calibrated}} = f(\hat{p})
$$

onde $f$ Ã© piecewise constant monotonic function aprendida via isotonic regression.

**Vantagens**:
- âœ… Mais flexÃ­vel (captura nÃ£o-linearidades)
- âœ… Funciona bem para tree-based models (XGBoost)
- âœ… Sem suposiÃ§Ãµes paramÃ©tricas

**Desvantagens**:
- âŒ Precisa de mais dados (>100 samples validation)
- âŒ Pode overfit com poucos dados

---

#### MÃ©todo 3: Beta Calibration (Estado da Arte)

**Ideia**: Usa distribuiÃ§Ã£o Beta (mais flexÃ­vel que Platt):

$$
\hat{p}_{\text{calibrated}} = \text{Beta}(\hat{p}; a, b, c)
$$

**Sprint 07**: Usaremos **Platt Scaling** (LogReg) ou **Isotonic** (se >100 validation).

---

### Threshold Tuning via ROC Curve

**Problema**: Threshold fixo (0.5 ou max F1) **nÃ£o considera custo do erro**.

**SoluÃ§Ã£o**: ROC curve + anÃ¡lise de custo.

#### ROC Curve (Receiver Operating Characteristic)

Plot de **TPR vs FPR** variando threshold:

```
TPR = True Positive Rate = TP / (TP + FN)  # Recall
FPR = False Positive Rate = FP / (FP + TN)

Threshold 0.0: TPR=1.00, FPR=1.00 (classifica tudo como positivo)
Threshold 0.5: TPR=0.95, FPR=0.05 (balanceado)
Threshold 1.0: TPR=0.00, FPR=0.00 (classifica tudo como negativo)
```

**AUC (Area Under Curve)**: mÃ©trica de performance global (0.5=random, 1.0=perfeito).

---

#### SeleÃ§Ã£o de Threshold via Custo

**FÃ³rmula de custo total:**

$$
\text{Cost} = C_{\text{FP}} \cdot \text{FP} + C_{\text{FN}} \cdot \text{FN}
$$

onde:
- $C_{\text{FP}}$: custo de falso positivo (ex: 1.0)
- $C_{\text{FN}}$: custo de falso negativo (ex: 3.0)

**Threshold Ã³timo**: minimiza custo total no validation set.

**Exemplo:**
```python
# Assumindo C_FN = 3.0, C_FP = 1.0

best_cost = float('inf')
best_threshold = 0.5

for threshold in np.arange(0.0, 1.0, 0.01):  # Step 0.01 (fino!)
    y_pred = (y_proba >= threshold).astype(int)
    
    fp = ((y_pred == 1) & (y_true == 0)).sum()
    fn = ((y_pred == 0) & (y_true == 1)).sum()
    
    cost = 1.0 * fp + 3.0 * fn
    
    if cost < best_cost:
        best_cost = cost
        best_threshold = threshold

# Resultado: threshold â‰ˆ 0.65 (mais conservador que 0.50)
```

---

### MatemÃ¡tica do Impacto

**Assumindo:**
- Modelo Sprint 06: Brier Score = 0.08, ECE = 0.12
- ApÃ³s Platt Scaling: Brier Score = 0.04, ECE = 0.05
- Threshold mudou de 0.75 (max F1) para 0.68 (min custo)

**Precision Boost (via threshold Ã³timo):**

```
Threshold 0.75: Precision=95%, FP=5%
Threshold 0.68: Precision=96%, FP=4%

Î”Precision â‰ˆ +1% âœ…
```

**Confiabilidade Boost (via calibraÃ§Ã£o):**

```
CalibraÃ§Ã£o antes: 80% dos vÃ­deos com proba=0.80 sÃ£o positivos â†’ 60% acerto
CalibraÃ§Ã£o depois: 80% dos vÃ­deos com proba_calibrated=0.80 sÃ£o positivos â†’ 78% acerto

Melhoria: +18 pontos percentuais em confiabilidade âœ…
```

---

## 3ï¸âƒ£ AlteraÃ§Ãµes Arquiteturais

### MudanÃ§as em Pipeline

**Antes** (Sprint 06):
```
Classifier â†’ predict_proba() â†’ threshold fixo â†’ Decision
```

**Depois** (Sprint 07):
```
Classifier â†’ predict_proba() â†’ Calibrator (Platt/Isotonic) â†’ proba_calibrated â†’ threshold Ã³timo (ROC) â†’ Decision
```

**Novas FunÃ§Ãµes:**
- `calibrate_probabilities()`: Aplica Platt ou Isotonic calibration
- `plot_calibration_curve()`: Visualiza calibraÃ§Ã£o (reliability diagram)
- `plot_roc_curve()`: Visualiza ROC + threshold Ã³timo
- `select_optimal_threshold()`: Seleciona via custo ou mÃ©trica customizada
- `compute_calibration_metrics()`: Calcula Brier Score, ECE

---

### MudanÃ§as em Estrutura

**ExtensÃ£o: `SubtitleClassifier` (app/ml/subtitle_classifier.py)**

```python
class SubtitleClassifier:
    """
    ... (cÃ³digo Sprint 06) ...
    """
    
    def __init__(self, ...):
        ...
        self.calibrator = None  # Platt ou Isotonic
        self.calibration_method = None  # 'platt' ou 'isotonic'
    
    def calibrate(
        self,
        X_cal: np.ndarray,
        y_cal: np.ndarray,
        method: str = 'platt'
    ):
        """
        Calibra probabilidades em calibration set SEPARADO.
        
        Args:
            X_cal: Features de calibraÃ§Ã£o (nÃ£o usado em treino!)
            y_cal: Labels de calibraÃ§Ã£o
            method: 'platt' ou 'isotonic'
        """
        ...
    
    def predict_proba_calibrated(
        self,
        features: np.ndarray
    ) -> float:
        """
        Prediz probabilidade CALIBRADA.
        
        Returns:
            Probability calibrada [0, 1]
        """
        ...
    
    def select_optimal_threshold(
        self,
        X_val: np.ndarray,
        y_val: np.ndarray,
        cost_fp: float = 1.0,
        cost_fn: float = 1.0,
        metric: str = 'cost'  # 'cost', 'f1', 'balanced_accuracy'
    ) -> float:
        """
        Seleciona threshold Ã³timo via ROC curve.
        
        Args:
            X_val: Features de validaÃ§Ã£o
            y_val: Labels de validaÃ§Ã£o
            cost_fp: Custo de falso positivo
            cost_fn: Custo de falso negativo
            metric: MÃ©trica para otimizar
        
        Returns:
            Threshold Ã³timo
        """
        ...
```

---

### MudanÃ§as em ParÃ¢metros

| ParÃ¢metro | Sprint 06 | Sprint 07 | Justificativa |
|-----------|----------|----------|---------------|
| `calibration_method` | N/A | 'platt' ou 'isotonic' | MÃ©todo de calibraÃ§Ã£o |
| `threshold_selection` | 'max_f1' (grid 0.05) | 'min_cost' (grid 0.01) | Custo customizado |
| `cost_fp` | N/A | 1.0 (default) | Custo de FP |
| `cost_fn` | N/A | 3.0 (default - FN pior) | Custo de FN |

---

## 4ï¸âƒ£ MudanÃ§as de CÃ³digo (Pseudo + Real)

### PseudocÃ³digo: CalibraÃ§Ã£o + Threshold Tuning

```python
# CRITICAL: Pipeline com split ÃšNICO e DISJUNTO (sem vazamento)
# Ordem: train â†’ cal â†’ val â†’ test (sem reutilizaÃ§Ã£o)

# FASE 0: Split Ãºnico em 4 conjuntos disjuntos
from sklearn.model_selection import train_test_split

# Split 1: test set (20%)
X_trainvalcal, X_test, y_trainvalcal, y_test = train_test_split(
    X_all, y_all,
    test_size=0.20,
    stratify=y_all,
    random_state=42
)

# Split 2: cal set (15% do trainvalcal = 12% do total)
X_trainval, X_cal, y_trainval, y_cal = train_test_split(
    X_trainvalcal, y_trainvalcal,
    test_size=0.15,
    stratify=y_trainvalcal,
    random_state=42
)

# Split 3: val set (20% do trainval = 13.6% do total)
X_train, X_val, y_train, y_val = train_test_split(
    X_trainval, y_trainval,
    test_size=0.20,
    stratify=y_trainval,
    random_state=42
)

print(f"Split disjunto:")
print(f"  Train: {len(X_train)} samples (54.4% - treino do modelo)")
print(f"  Cal:   {len(X_cal)} samples (12.0% - calibraÃ§Ã£o)")
print(f"  Val:   {len(X_val)} samples (13.6% - threshold tuning)")
print(f"  Test:  {len(X_test)} samples (20.0% - avaliaÃ§Ã£o final)")

# FASE 1: Treinar modelo DO ZERO (dentro da Sprint 07)
# CRITICAL: NÃƒO carregar modelo pronto (evita vazamento)
clf = SubtitleClassifier(model_type='logistic')
clf.train(X_train, y_train)  # Treina SEM validaÃ§Ã£o interna (serÃ¡ calibrado depois)

print(f"\nModelo treinado em {len(X_train)} samples")

# FASE 2: CalibraÃ§Ã£o (cal set - NUNCA visto no treino)
# âš ï¸ **CORREÃ‡ÃƒO P1 (FIX_OCR.md + scikit-learn best practices)**
# Isotonic regression NÃƒO Ã© recomendada com amostras pequenas (<< 1000)
# Fonte: https://scikit-learn.org/stable/modules/calibration.html
# "Isotonic calibration is generally more powerful than parametric methods such as Platt scaling.
#  However, it tends to overfit the calibration set which is significantly smaller than the train set."
#  
# Recommendation sklearn docs: N_cal >> 1000 para isotonic ser seguro
# Com N_cal < 500, preferir Platt (sigmoid) por robustez

N_cal = len(X_cal)
if N_cal < 500:  # Threshold conservador (era 100, agora 500)
    calibration_method = 'platt'  # Mais robusto com poucos dados
    print(f"N_cal={N_cal} < 500 â†’ usando Platt Scaling (robusto, recomendado por sklearn)")
    print(f"  RazÃ£o: Isotonic regression tende a overfit com N < 500")
else:
    calibration_method = 'platt'  # Default: SEMPRE Platt (mais seguro)
    print(f"N_cal={N_cal} â‰¥ 500 â†’ usando Platt Scaling (default seguro)")
    print(f"  Nota: Isotonic poderia ser usado, mas Platt Ã© mais robusto")
    print(f"  Se desejar isotonic explicitamente, mudar para method='isotonic' manual")

# Alternative: Force Platt always (safest)
# calibration_method = 'platt'  # Sempre Platt (mais conservador)

clf.calibrate(X_cal, y_cal, method=calibration_method, verbose=True)

# FASE 3: Threshold tuning (val set - NUNCA visto no treino/cal)
optimal_threshold = clf.select_optimal_threshold(
    X_val,
    y_val,
    cost_fp=1.0,
    cost_fn=3.0,  # FN custa 3Ã— mais (exemplo)
    metric='cost',
    verbose=True
)

print(f"\nOptimal threshold: {optimal_threshold:.3f}")
clf.threshold = optimal_threshold

# FASE 4: AvaliaÃ§Ã£o final no test set (NUNCA visto antes)
y_test_proba_uncal = clf.predict_proba(X_test)  # Uncalibrated
y_test_proba_cal = clf.predict_proba_calibrated(X_test)  # Calibrated
y_test_pred = (y_test_proba_cal >= clf.threshold).astype(int)

# MÃ©tricas de calibraÃ§Ã£o
brier_uncal = compute_brier_score(y_test, y_test_proba_uncal)
brier_cal = compute_brier_score(y_test, y_test_proba_cal)
ece_uncal = compute_expected_calibration_error(y_test, y_test_proba_uncal)
ece_cal = compute_expected_calibration_error(y_test, y_test_proba_cal)

# AUC antes/depois (reportar, nÃ£o assumir que mantÃ©m)
from sklearn.metrics import roc_auc_score, average_precision_score
auc_uncal = roc_auc_score(y_test, y_test_proba_uncal)
auc_cal = roc_auc_score(y_test, y_test_proba_cal)
pr_auc_uncal = average_precision_score(y_test, y_test_proba_uncal)
pr_auc_cal = average_precision_score(y_test, y_test_proba_cal)

print(f"\n{'='*60}")
print(f"CALIBRATION IMPACT")
print(f"{'='*60}")
print(f"Brier Score: {brier_uncal:.4f} â†’ {brier_cal:.4f} (Î”={brier_uncal - brier_cal:.4f})")
print(f"ECE:         {ece_uncal:.4f} â†’ {ece_cal:.4f} (Î”={ece_uncal - ece_cal:.4f})")
print(f"ROC-AUC:     {auc_uncal:.4f} â†’ {auc_cal:.4f} (Î”={auc_cal - auc_uncal:.4f})")
print(f"PR-AUC:      {pr_auc_uncal:.4f} â†’ {pr_auc_cal:.4f} (Î”={pr_auc_cal - pr_auc_uncal:.4f})")

# Plot
plot_calibration_curve(y_test, y_test_proba_uncal, y_test_proba_cal)
plot_roc_curve(y_test, y_test_proba_cal, threshold=optimal_threshold)

# FASE 5: Save model com calibrador + metadata
metadata = {
    'calibration_method': calibration_method,
    'n_calibration_samples': N_cal,
    'optimal_threshold': optimal_threshold,
    'brier_uncalibrated': float(brier_uncal),
    'brier_calibrated': float(brier_cal),
    'ece_uncalibrated': float(ece_uncal),
    'ece_calibrated': float(ece_cal),
    'roc_auc_uncalibrated': float(auc_uncal),
    'roc_auc_calibrated': float(auc_cal),
    'pr_auc_uncalibrated': float(pr_auc_uncal),
    'pr_auc_calibrated': float(pr_auc_cal),
}

clf.save("models/subtitle_classifier_calibrated_v1.pkl", metadata=metadata)
print(f"\nâœ… Modelo salvo com calibraÃ§Ã£o {calibration_method} e threshold {optimal_threshold:.3f}")
```

---

### MudanÃ§as Reais (CÃ³digo Completo)

#### Arquivo 1: `app/ml/subtitle_classifier.py` (ESTENDER)

**Novas FunÃ§Ãµes: CalibraÃ§Ã£o**

```python
# Adicionar imports
from sklearn.calibration import CalibratedClassifierCV
from sklearn.isotonic import IsotonicRegression

class SubtitleClassifier:
    """... (cÃ³digo Sprint 06) ..."""
    
    def __init__(self, ...):
        # ... (cÃ³digo Sprint 06) ...
        self.calibrator = None  # CalibratedClassifierCV ou IsotonicRegression
        self.calibration_method = None
        self.is_calibrated = False
    
    def calibrate(
        self,
        X_cal: np.ndarray,
        y_cal: np.ndarray,
        method: Literal['platt', 'isotonic', 'auto'] = 'auto',
        verbose: bool = True
    ):
        """
        Calibra probabilidades em calibration set SEPARADO.
        
        Args:
            X_cal: Features de calibraÃ§Ã£o (N, 56)
            y_cal: Labels de calibraÃ§Ã£o (N,)
            method: 'platt' (sigmoid), 'isotonic' (non-parametric), 'auto' (escolhe baseado em N)
            verbose: Print mÃ©tricas
        
        Note:
            CRUCIAL: X_cal NÃƒO pode ter sido usado no treinamento do modelo base!
            Usar calibration set independente previne overfitting.
            
            Auto-selection rule:
              - N < 100: Platt Scaling (mais robusto com poucos dados)
              - N >= 100: Isotonic Regression (mais flexÃ­vel com muitos dados)
        """
        if self.model is None:
            raise ValueError("Model not trained. Train first, then calibrate.")
        
        # Validate
        self.validate_feature_vector(X_cal)
        
        N_cal = len(X_cal)
        
        # Auto-select method based on calibration set size
        if method == 'auto':
            if N_cal < 100:
                method = 'platt'
                if verbose:
                    print(f"Auto-select: N_cal={N_cal} < 100 â†’ using Platt Scaling (robust)")
            else:
                method = 'isotonic'
                if verbose:
                    print(f"Auto-select: N_cal={N_cal} >= 100 â†’ using Isotonic Regression (flexible)")
        
        # Warning for small calibration sets
        if N_cal < 50:
            print(f"âš ï¸  WARNING: Calibration set very small (N={N_cal}). Results may be unreliable.")
            print(f"    Recommendation: Use at least 50-100 samples for calibration.")
        
        # Get uncalibrated probabilities
        if self.scaler is not None:
            X_cal_scaled = self.scaler.transform(X_cal)
        else:
            X_cal_scaled = X_cal
        
        y_proba_uncalibrated = self.model.predict_proba(X_cal_scaled)[:, 1]
        
        # Compute uncalibrated Brier Score
        brier_before = np.mean((y_proba_uncalibrated - y_cal) ** 2)
        
        # Calibrate
        if method == 'platt':
            # Platt scaling: fit logistic regression on predictions
            # Use robust solver and regularization for small samples
            from sklearn.linear_model import LogisticRegression
            
            self.calibrator = LogisticRegression(
                solver='lbfgs',
                max_iter=1000,
                C=1.0,  # L2 regularization (default)
                random_state=42
            )
            self.calibrator.fit(y_proba_uncalibrated.reshape(-1, 1), y_cal)
        
        elif method == 'isotonic':
            # Isotonic regression: fit monotonic function
            # Only use if N >= 100 (more data needed)
            if N_cal < 100:
                print(f"âš ï¸  WARNING: Isotonic with N={N_cal} < 100 may overfit. Consider 'platt'.")
            
            self.calibrator = IsotonicRegression(
                out_of_bounds='clip',
                increasing=True  # Ensure monotonicity
            )
            self.calibrator.fit(y_proba_uncalibrated, y_cal)
        
        else:
            raise ValueError(f"Unknown calibration method: {method}")
        
        self.calibration_method = method
        self.is_calibrated = True
        self.n_calibration_samples = N_cal
        
        # Get calibrated probabilities
        y_proba_calibrated = self._calibrate_proba(y_proba_uncalibrated)
        
        # Compute calibrated Brier Score
        brier_after = np.mean((y_proba_calibrated - y_cal) ** 2)
        
        if verbose:
            print(f"\nCalibration ({method}):")
            print(f"  N samples: {N_cal}")
            print(f"  Brier Score Before: {brier_before:.4f}")
            print(f"  Brier Score After:  {brier_after:.4f}")
            print(f"  Improvement: {brier_before - brier_after:.4f}")
    
    def _calibrate_proba(self, proba_uncalibrated: np.ndarray) -> np.ndarray:
        """
        Aplica calibrador nas probabilidades.
        
        Args:
            proba_uncalibrated: Probabilidades nÃ£o-calibradas
        
        Returns:
            Probabilidades calibradas
        """
        if not self.is_calibrated:
            return proba_uncalibrated
        
        if self.calibration_method == 'platt':
            # Platt: predict via logistic regression
            proba_calibrated = self.calibrator.predict_proba(
                proba_uncalibrated.reshape(-1, 1)
            )[:, 1]
        elif self.calibration_method == 'isotonic':
            # Isotonic: transform via monotonic function
            proba_calibrated = self.calibrator.predict(proba_uncalibrated)
        else:
            proba_calibrated = proba_uncalibrated
        
        return np.clip(proba_calibrated, 0.0, 1.0)
    
    def predict_proba_calibrated(self, features: np.ndarray) -> float:
        """
        Prediz probabilidade CALIBRADA.
        
        Args:
            features: Feature vector (56,) ou (N, 56)
        
        Returns:
            Probability calibrada [0, 1]
        """
        # Get uncalibrated probability
        proba_uncalibrated = self.predict_proba(features)
        
        # Calibrate
        if self.is_calibrated:
            if isinstance(proba_uncalibrated, float):
                proba_uncalibrated = np.array([proba_uncalibrated])
            
            proba_calibrated = self._calibrate_proba(proba_uncalibrated)
            
            if proba_calibrated.shape[0] == 1:
                return float(proba_calibrated[0])
            return proba_calibrated
        else:
            return proba_uncalibrated
    
    def select_optimal_threshold(
        self,
        X_val: np.ndarray,
        y_val: np.ndarray,
        cost_fp: float = 1.0,
        cost_fn: float = 1.0,
        metric: Literal['cost', 'f1', 'balanced_accuracy', 'youden'] = 'cost',
        step: float = 0.01,
        verbose: bool = True
    ) -> float:
        """
        Seleciona threshold Ã³timo via ROC curve.
        
        Args:
            X_val: Features de validaÃ§Ã£o (N, 56)
            y_val: Labels de validaÃ§Ã£o (N,)
            cost_fp: Custo de falso positivo (default=1.0)
            cost_fn: Custo de falso negativo (default=1.0)
            metric: MÃ©trica para otimizar
              - 'cost': minimiza custo total (cost_fp Ã— FP + cost_fn Ã— FN)
              - 'f1': maximiza F1 score
              - 'balanced_accuracy': maximiza (TPR + TNR) / 2
              - 'youden': maximiza Youden's J statistic (TPR - FPR)
            step: Step do grid search (default=0.01)
            verbose: Print resultados
        
        Returns:
            Threshold Ã³timo
        
        Note:
            - Usa probabilidades CALIBRADAS se calibrator estÃ¡ setado
            - Threshold Ã© selecionado no validation set, NÃƒO no trainset
            - Se cost_fn > cost_fp: threshold tende para MENOR (mais recall)
            - Se cost_fp > cost_fn: threshold tende para MAIOR (mais precision)
        """
        # Get probabilities (calibrated if available)
        if self.is_calibrated:
            y_proba = self.predict_proba_calibrated(X_val)
        else:
            y_proba = self.predict_proba(X_val)
        
        best_score = float('-inf') if metric != 'cost' else float('inf')
        best_threshold = 0.5
        
        results = []
        
        for threshold in np.arange(0.0, 1.0 + step, step):
            y_pred = (y_proba >= threshold).astype(int)
            
            # Confusion matrix
            tp = ((y_pred == 1) & (y_val == 1)).sum()
            tn = ((y_pred == 0) & (y_val == 0)).sum()
            fp = ((y_pred == 1) & (y_val == 0)).sum()
            fn = ((y_pred == 0) & (y_val == 1)).sum()
            
            # Compute metric
            if metric == 'cost':
                score = cost_fp * fp + cost_fn * fn
                is_better = score < best_score  # Minimize cost
            
            elif metric == 'f1':
                precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
                recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
                score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
                is_better = score > best_score  # Maximize F1
            
            elif metric == 'balanced_accuracy':
                tpr = tp / (tp + fn) if (tp + fn) > 0 else 0.0
                tnr = tn / (tn + fp) if (tn + fp) > 0 else 0.0
                score = (tpr + tnr) / 2.0
                is_better = score > best_score  # Maximize balanced accuracy
            
            elif metric == 'youden':
                tpr = tp / (tp + fn) if (tp + fn) > 0 else 0.0
                fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0
                score = tpr - fpr  # Youden's J statistic
                is_better = score > best_score  # Maximize J
            
            else:
                raise ValueError(f"Unknown metric: {metric}")
            
            results.append({
                'threshold': threshold,
                'score': score,
                'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn,
            })
            
            if is_better:
                best_score = score
                best_threshold = threshold
        
        if verbose:
            best_result = [r for r in results if r['threshold'] == best_threshold][0]
            
            tp, tn, fp, fn = best_result['tp'], best_result['tn'], best_result['fp'], best_result['fn']
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
            
            print(f"\\nOptimal Threshold Selection (metric={metric}):")
            print(f"  Threshold: {best_threshold:.3f}")
            print(f"  Metric Score: {best_score:.4f}")
            print(f"  Precision: {precision:.4f}")
            print(f"  Recall: {recall:.4f}")
            print(f"  F1: {f1:.4f}")
            print(f"  FP: {fp}, FN: {fn}")
            
            if metric == 'cost':
                print(f"  Total Cost: {best_score:.2f} (FP cost={cost_fp}, FN cost={cost_fn})")
        
        return best_threshold
```

---

#### Arquivo 2: `app/ml/calibration_utils.py` (NOVO)

**Utilidades para AnÃ¡lise de CalibraÃ§Ã£o**

```python
"""
Calibration utilities (Sprint 07).

FunÃ§Ãµes para anÃ¡lise e visualizaÃ§Ã£o de calibraÃ§Ã£o de probabilidades.
"""
import numpy as np
import matplotlib.pyplot as plt
from sklearn.calibration import calibration_curve
from sklearn.metrics import roc_curve, auc, brier_score_loss


def compute_expected_calibration_error(
    y_true: np.ndarray,
    y_proba: np.ndarray,
    n_bins: int = 10,
    strategy: Literal['uniform', 'quantile'] = 'uniform'
) -> float:
    """
    Calcula Expected Calibration Error (ECE).
    
    Args:
        y_true: Labels verdadeiros (0 ou 1)
        y_proba: Probabilidades preditas [0, 1]
        n_bins: NÃºmero de bins para agrupar probabilidades
        strategy: 'uniform' (bins de largura igual) ou 'quantile' (adaptive bins)
    
    Returns:
        ECE (0 = perfeitamente calibrado)
    
    Note:
        ECE mede distÃ¢ncia mÃ©dia entre confianÃ§a predita e acurÃ¡cia real.
        
        Strategies:
          - 'uniform': Bins [0, 0.1), [0.1, 0.2), ..., [0.9, 1.0]
            â†’ Bom quando probabilidades distribuÃ­das uniformemente
          - 'quantile': Bins com mesmo nÃºmero de amostras
            â†’ Melhor quando probabilidades concentradas (ex: 90% em [0.8, 1.0])
        
        Exemplo:
          Bin [0.7, 0.8]: 100 exemplos
            - Confidence mÃ©dia: 0.75
            - AcurÃ¡cia real: 0.68
            - ContribuiÃ§Ã£o: (100/N) Ã— |0.75 - 0.68| = (100/N) Ã— 0.07
    """
    # Create bins
    if strategy == 'uniform':
        bins = np.linspace(0, 1, n_bins + 1)
    elif strategy == 'quantile':
        # Adaptive bins: equal number of samples per bin
        bins = np.percentile(y_proba, np.linspace(0, 100, n_bins + 1))
        bins[0] = 0.0  # Ensure 0
        bins[-1] = 1.0  # Ensure 1
    else:
        raise ValueError(f"Unknown strategy: {strategy}")
    
    bin_indices = np.digitize(y_proba, bins) - 1
    bin_indices = np.clip(bin_indices, 0, n_bins - 1)
    
    ece = 0.0
    
    for bin_idx in range(n_bins):
        mask = (bin_indices == bin_idx)
        
        if mask.sum() == 0:
            continue
        
        bin_confidence = y_proba[mask].mean()
        bin_accuracy = y_true[mask].mean()
        bin_weight = mask.sum() / len(y_true)
        
        ece += bin_weight * abs(bin_confidence - bin_accuracy)
    
    return ece


def plot_calibration_curve(
    y_true: np.ndarray,
    y_proba_uncalibrated: np.ndarray,
    y_proba_calibrated: Optional[np.ndarray] = None,
    n_bins: int = 10,
    save_path: Optional[str] = None
):
    """
    Plota reliability diagram (calibration curve).
    
    Args:
        y_true: Labels verdadeiros
        y_proba_uncalibrated: Probabilidades nÃ£o-calibradas
        y_proba_calibrated: Probabilidades calibradas (opcional)
        n_bins: NÃºmero de bins
        save_path: Se fornecido, salva figura
    
    Note:
        Reliability diagram: plota confianÃ§a predita vs acurÃ¡cia real.
        Linha diagonal = perfeitamente calibrado.
        Abaixo da diagonal = overconfident.
        Acima da diagonal = underconfident.
    """
    fig, ax = plt.subplots(figsize=(8, 6))
    
    # Uncalibrated curve
    fraction_of_positives, mean_predicted_value = calibration_curve(
        y_true, y_proba_uncalibrated, n_bins=n_bins, strategy='uniform'
    )
    
    ax.plot(mean_predicted_value, fraction_of_positives, 's-',
            label='Uncalibrated', color='red', alpha=0.7)
    
    # Calibrated curve (if provided)
    if y_proba_calibrated is not None:
        fraction_of_positives_cal, mean_predicted_value_cal = calibration_curve(
            y_true, y_proba_calibrated, n_bins=n_bins, strategy='uniform'
        )
        
        ax.plot(mean_predicted_value_cal, fraction_of_positives_cal, 'o-',
                label='Calibrated', color='green', alpha=0.7)
    
    # Perfect calibration line
    ax.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')
    
    ax.set_xlabel('Mean Predicted Probability')
    ax.set_ylabel('Fraction of Positives (Accuracy)')
    ax.set_title('Calibration Curve (Reliability Diagram)')
    ax.legend()
    ax.grid(alpha=0.3)
    
    # Compute metrics
    brier_uncal = brier_score_loss(y_true, y_proba_uncalibrated)
    ece_uncal = compute_expected_calibration_error(y_true, y_proba_uncalibrated, n_bins)
    
    text = f"Uncalibrated:\\n  Brier: {brier_uncal:.4f}\\n  ECE: {ece_uncal:.4f}"
    
    if y_proba_calibrated is not None:
        brier_cal = brier_score_loss(y_true, y_proba_calibrated)
        ece_cal = compute_expected_calibration_error(y_true, y_proba_calibrated, n_bins)
        
        text += f"\\n\\nCalibrated:\\n  Brier: {brier_cal:.4f}\\n  ECE: {ece_cal:.4f}"
    
    ax.text(0.05, 0.95, text, transform=ax.transAxes,
            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150)
        print(f"Calibration curve saved to {save_path}")
    else:
        plt.show()


def plot_roc_curve_with_threshold(
    y_true: np.ndarray,
    y_proba: np.ndarray,
    threshold: float = 0.5,
    save_path: Optional[str] = None
):
    """
    Plota ROC curve com threshold marcado.
    
    Args:
        y_true: Labels verdadeiros
        y_proba: Probabilidades preditas
        threshold: Threshold selecionado (marca no plot)
        save_path: Se fornecido, salva figura
    """
    # Compute ROC curve
    fpr, tpr, thresholds = roc_curve(y_true, y_proba)
    roc_auc = auc(fpr, tpr)
    
    # Find threshold point
    threshold_idx = np.argmin(np.abs(thresholds - threshold))
    threshold_fpr = fpr[threshold_idx]
    threshold_tpr = tpr[threshold_idx]
    
    # Plot
    fig, ax = plt.subplots(figsize=(8, 6))
    
    ax.plot(fpr, tpr, color='darkblue', lw=2,
            label=f'ROC Curve (AUC = {roc_auc:.3f})')
    
    # Mark selected threshold
    ax.plot(threshold_fpr, threshold_tpr, 'ro', markersize=10,
            label=f'Threshold = {threshold:.3f}\\n(TPR={threshold_tpr:.3f}, FPR={threshold_fpr:.3f})')
    
    # Random classifier line
    ax.plot([0, 1], [0, 1], 'k--', lw=1, label='Random Classifier')
    
    ax.set_xlabel('False Positive Rate (FPR)')
    ax.set_ylabel('True Positive Rate (TPR / Recall)')
    ax.set_title('ROC Curve')
    ax.legend(loc='lower right')
    ax.grid(alpha=0.3)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150)
        print(f"ROC curve saved to {save_path}")
    else:
        plt.show()
```

---

#### Arquivo 3: `scripts/calibrate_and_tune_threshold.py` (NOVO)

**Script de CalibraÃ§Ã£o + Threshold Tuning**

```python
"""
Script para calibrar classificador e tunar threshold (Sprint 07).

Usage:
  python scripts/calibrate_and_tune_threshold.py \\
    --model models/subtitle_classifier.pkl \\
    --dataset data/features.csv \\
    --output models/subtitle_classifier_calibrated.pkl \\
    --calibration-method platt \\
    --cost-fn 3.0
"""
import argparse
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

from app.ml.subtitle_classifier import SubtitleClassifier
from app.ml.calibration_utils import (
    plot_calibration_curve,
    plot_roc_curve_with_threshold,
    compute_expected_calibration_error,
)
from sklearn.metrics import brier_score_loss


def main():
    parser = argparse.ArgumentParser(description='Calibrate classifier and tune threshold')
    parser.add_argument('--dataset', required=True, help='Path to dataset CSV')
    parser.add_argument('--output', required=True, help='Output calibrated model path')
    parser.add_argument('--calibration-method', default='auto', choices=['platt', 'isotonic', 'auto'],
                        help='Calibration method (auto=Platt if N<100, Isotonic if N>=100)')
    parser.add_argument('--cost-fp', type=float, default=1.0, help='Cost of false positive')
    parser.add_argument('--cost-fn', type=float, default=3.0, help='Cost of false negative')
    parser.add_argument('--test-size', type=float, default=0.20, help='Hold-out test size')
    parser.add_argument('--cal-size', type=float, default=0.15, help='Calibration set size (from trainval)')
    args = parser.parse_args()
    
    # Load dataset
    df = pd.read_csv(args.dataset)
    y = df['has_subtitles'].values
    X = df.drop(columns=['video_path', 'has_subtitles']).values
    
    print(f"Dataset: {X.shape[0]} samples, {X.shape[1]} features")
    
    # Split: test / trainval
    X_trainval, X_test, y_trainval, y_test = train_test_split(
        X, y, test_size=args.test_size, stratify=y, random_state=42
    )
    
    # Split trainval: calibration / train-threshold
    X_train_threshold, X_cal, y_train_threshold, y_cal = train_test_split(
        X_trainval, y_trainval, test_size=args.cal_size, stratify=y_trainval, random_state=42
    )
    
    # Split train_threshold: train / val (for threshold selection)
    X_train, X_val, y_train, y_val = train_test_split(
        X_train_threshold, y_train_threshold, test_size=0.20, stratify=y_train_threshold, random_state=42
    )
    
    print(f"\\nSplit:")
    print(f"  Train:       {len(X_train)} (model training)")
    print(f"  Val:         {len(X_val)} (threshold selection)")
    print(f"  Calibration: {len(X_cal)} (calibration)")
    print(f"  Test:        {len(X_test)} (final evaluation)")
    
    # Load model
    print(f"\\n{'='*60}")
    print(f"Loading model from {args.model}")
    print(f"{'='*60}")
    
    clf = SubtitleClassifier()
    clf.load(args.model)
    
    # Evaluate uncalibrated
    print(f"\\n{'='*60}")
    print(f"Uncalibrated Model Performance")
    print(f"{'='*60}")
    
    y_test_proba_uncal = clf.predict_proba(X_test)
    brier_uncal = brier_score_loss(y_test, y_test_proba_uncal)
    ece_uncal = compute_expected_calibration_error(y_test, y_test_proba_uncal)
    
    print(f"  Brier Score: {brier_uncal:.4f}")
    print(f"  ECE:         {ece_uncal:.4f}")
    
    # Calibrate
    print(f"\\n{'='*60}")
    print(f"Calibrating with method={args.calibration_method}")
    print(f"{'='*60}")
    
    clf.calibrate(X_cal, y_cal, method=args.calibration_method, verbose=True)
    
    # Evaluate calibrated
    y_test_proba_cal = clf.predict_proba_calibrated(X_test)
    brier_cal = brier_score_loss(y_test, y_test_proba_cal)
    ece_cal = compute_expected_calibration_error(y_test, y_test_proba_cal)
    
    print(f"\\nCalibrated Performance:")
    print(f"  Brier Score: {brier_cal:.4f} (Î”={brier_uncal - brier_cal:.4f})")
    print(f"  ECE:         {ece_cal:.4f} (Î”={ece_uncal - ece_cal:.4f})")
    
    # Tune threshold
    print(f"\\n{'='*60}")
    print(f"Threshold Tuning (cost_fp={args.cost_fp}, cost_fn={args.cost_fn})")
    print(f"{'='*60}")
    
    optimal_threshold = clf.select_optimal_threshold(
        X_val, y_val,
        cost_fp=args.cost_fp,
        cost_fn=args.cost_fn,
        metric='cost',
        verbose=True
    )
    
    clf.threshold = optimal_threshold
    
    # Final evaluation on test
    print(f"\\n{'='*60}")
    print(f"Final Test Set Evaluation (calibrated + optimal threshold)")
    print(f"{'='*60}")
    
    y_test_pred = clf.predict(X_test)
    
    from sklearn.metrics import classification_report
    print(classification_report(y_test, y_test_pred, target_names=['No Subtitle', 'Has Subtitle']))
    
    # Plot calibration curve
    print(f"\\nGenerating calibration curve...")
    plot_calibration_curve(
        y_test, y_test_proba_uncal, y_test_proba_cal,
        save_path='outputs/calibration_curve.png'
    )
    
    # Plot ROC curve
    print(f"Generating ROC curve...")
    plot_roc_curve_with_threshold(
        y_test, y_test_proba_cal, threshold=optimal_threshold,
        save_path='outputs/roc_curve.png'
    )
    
    # Save calibrated model
    metadata = {
        'calibration_method': args.calibration_method,
        'optimal_threshold': optimal_threshold,
        'cost_fp': args.cost_fp,
        'cost_fn': args.cost_fn,
        'brier_score_uncalibrated': float(brier_uncal),
        'brier_score_calibrated': float(brier_cal),
        'ece_uncalibrated': float(ece_uncal),
        'ece_calibrated': float(ece_cal),
    }
    
    clf.save(args.output, metadata=metadata)
    
    print(f"\\nâœ… Calibrated model saved to {args.output}")
    print(f"   Calibration: {args.calibration_method}")
    print(f"   Threshold: {optimal_threshold:.3f}")
    print(f"   Brier improvement: {brier_uncal - brier_cal:.4f}")
    print(f"   ECE improvement: {ece_uncal - ece_cal:.4f}")


if __name__ == '__main__':
    main()
```

---

### Resumo das MudanÃ§as

| Arquivo | FunÃ§Ãµes Afetadas | Tipo MudanÃ§a | Linhas |
|---------|------------------|-------------|--------|
| `app/ml/subtitle_classifier.py` **(ESTENDER)** | `calibrate()`, `predict_proba_calibrated()`, `select_optimal_threshold()` | Adicionar calibraÃ§Ã£o + threshold tuning | +250 |
| `app/ml/calibration_utils.py` **(NOVO)** | `compute_expected_calibration_error()`, `plot_calibration_curve()`, `plot_roc_curve_with_threshold()` | Utils de calibraÃ§Ã£o | +180 |
| `scripts/calibrate_and_tune_threshold.py` **(NOVO)** | Script CLI de calibraÃ§Ã£o | Calibrar + tunar threshold | +150 |
| **TOTAL** | | | **~580 linhas** |

---

## 5ï¸âƒ£ Plano de ValidaÃ§Ã£o

### Como Medir Impacto?

**MÃ©trica Principal**: **Brier Score + ECE** (calibraÃ§Ã£o) + **Precision/Recall** (threshold Ã³timo)

---

### MÃ©todo

**1. Baseline (Post-Sprint 06 - uncalibrated)**

```bash
$ python evaluate_model.py --model models/subtitle_classifier.pkl --dataset test_dataset/

Esperado:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ POST-SPRINT-06 BASELINE (uncalibrated)  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Precision: 97%                          â”‚
â”‚ Recall: 97%                             â”‚
â”‚ F1: 97%                                 â”‚
â”‚ Threshold: 0.75 (max F1)                â”‚
â”‚                                         â”‚
â”‚ Calibration:                            â”‚
â”‚   Brier Score: 0.08                     â”‚
â”‚   ECE: 0.12                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

**2. Calibrar + Tunar Threshold (Sprint 07)**

```bash
$ python scripts/calibrate_and_tune_threshold.py \\
    --model models/subtitle_classifier.pkl \\
    --dataset data/features.csv \\
    --output models/subtitle_classifier_calibrated.pkl \\
    --calibration-method platt \\
    --cost-fn 3.0  # FN custa 3Ã— mais que FP

Esperado:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CALIBRATION (Platt Scaling)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Brier Score Before: 0.08                â”‚
â”‚ Brier Score After:  0.04  (-0.04) âœ…    â”‚
â”‚ ECE Before: 0.12                        â”‚
â”‚ ECE After:  0.05  (-0.07) âœ…            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ THRESHOLD TUNING (cost-based)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Old Threshold: 0.75 (max F1)            â”‚
â”‚ New Threshold: 0.68 (min cost, FN=3Ã—FP) â”‚
â”‚                                         â”‚
â”‚ Precision: 98% (+1%) âœ…                 â”‚
â”‚ Recall: 97% (mantÃ©m) âœ…                 â”‚
â”‚ F1: 97.5% (+0.5%) âœ…                    â”‚
â”‚ Total Cost: 12.5 (vs 15.0 antes)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

**3. A/B Test: Uncalibrated vs Calibrated**

```bash
# Uncalibrated (Sprint 06)
$ python measure_baseline.py --model models/subtitle_classifier.pkl

# Calibrated (Sprint 07)
$ python measure_baseline.py --model models/subtitle_classifier_calibrated.pkl
```

---

**4. AnÃ¡lise Visual**

```bash
# Gerar plots
$ python scripts/calibrate_and_tune_threshold.py --model ... --dataset ...

# Outputs:
#   outputs/calibration_curve.png  â†’ Reliability diagram
#   outputs/roc_curve.png          â†’ ROC com threshold marcado
```

**AnÃ¡lise esperada:**

**Calibration Curve (Reliability Diagram):**
```
Antes: Pontos longe da diagonal (descalibrado)
  - proba=0.80 â†’ acurÃ¡cia real=0.60 (overconfident)
  - proba=0.50 â†’ acurÃ¡cia real=0.70 (underconfident)

Depois: Pontos prÃ³ximos da diagonal (calibrado)
  - proba=0.80 â†’ acurÃ¡cia real=0.78 âœ…
  - proba=0.50 â†’ acurÃ¡cia real=0.52 âœ…
```

**ROC Curve:**
```
AUC: 0.987 (mantÃ©m, calibraÃ§Ã£o nÃ£o muda AUC)
Threshold marcado: 0.68 (vs 0.75 antes)
```

---

### MÃ©trica de ValidaÃ§Ã£o

| MÃ©trica | Threshold | Status |
|---------|-----------|--------|
| **Î” Brier Score** | â‰¤ -0.02 | âœ… Aceita sprint |
| **Î” ECE** | â‰¤ -0.05 | âœ… Aceita sprint |
| **Precision** | â‰¥ 97% (no drop) | âœ… Aceita sprint |
| **Recall** | â‰¥ 97% (no drop) | âœ… Aceita sprint |
| **F1 Score** | â‰¥ 97% | âœ… Aceita sprint |
| **Calibration curve** | Pontos prÃ³ximos diagonal | âœ… Visual check |

---

## 6ï¸âƒ£ Risco & Trade-offs

### Riscos Identificados

| Risco | Probabilidade | Impacto | MitigaÃ§Ã£o |
|-------|--------------|--------|-----------|
| **Calibration set pequeno** (<50 samples) | 30% | MÃ‰DIO | Usar >80 samples; se nÃ£o, usar Platt (mais robusto) |
| **Overfitting no threshold** (tunar em trainset) | 20% | ALTO | SEMPRE tunar em validation set separado! |
| **Custo FP/FN mal estimado** | 40% | MÃ‰DIO | Validar com stakeholders; fazer sensitivity analysis |
| **CalibraÃ§Ã£o piora AUC** | 5% | BAIXO | CalibraÃ§Ã£o preserva ranking (nÃ£o deve piorar AUC) |

---

### Trade-offs

#### Trade-off 1: Platt vs Isotonic

**OpÃ§Ã£o A**: Platt Scaling â† **RECOMENDADO (LogReg)**
- âœ… RÃ¡pido (2 parÃ¢metros)
- âœ… Funciona bem com LogisticRegression
- âœ… Robusto com poucos dados (50+ samples ok)
- âŒ Assume forma paramÃ©trica (sigmoid)

**OpÃ§Ã£o B**: Isotonic Regression
- âœ… Mais flexÃ­vel (captura nÃ£o-linearidades)
- âœ… Funciona bem com XGBoost
- âŒ Precisa de mais dados (100+ samples)
- âŒ Pode overfit

â†’ **DecisÃ£o**: Platt para LogReg (Sprint 06), Isotonic se usar XGBoost.

---

#### Trade-off 2: MÃ©trica para Threshold Tuning

**OpÃ§Ã£o A**: Max F1 â† **Sprint 06 baseline**
- âœ… Simples
- âŒ NÃ£o considera custo FP vs FN

**OpÃ§Ã£o B**: Min Cost (weighted) â† **RECOMENDADO Sprint 07**
- âœ… CustomizÃ¡vel (cost_fp, cost_fn)
- âœ… Alinhado com negÃ³cio
- âŒ Requer estimativa de custo

**OpÃ§Ã£o C**: Max Youden's J (TPR - FPR)
- âœ… Maximiza "distÃ¢ncia" do random classifier
- âŒ DÃ¡ peso igual a FP e FN (como F1)

â†’ **DecisÃ£o**: Min Cost (OpÃ§Ã£o B), mas permitir F1, Youden como alternativas.

---

#### Trade-off 3: Threshold Step Size

**OpÃ§Ã£o A**: Step 0.05 (Sprint 06)
- âœ… RÃ¡pido (20 thresholds testados)
- âŒ Pode perder Ã³timo

**OpÃ§Ã£o B**: Step 0.01 â† **Sprint 07**
- âœ… Mais preciso (100 thresholds)
- âœ… NegligÃ­vel overhead (<1s)

â†’ **DecisÃ£o**: Step 0.01 (mais preciso, custo baixo).

---

## 7ï¸âƒ£ CritÃ©rio de Aceite da Sprint

> **âš ï¸ CORREÃ‡ÃƒO P1 (FIX_OCR.md - Alignment com Meta do Produto)**  
> CritÃ©rios originais (Precision/Recall â‰¥97%) eram **self-blocking** e nÃ£o alinhados com meta do produto.  
> **Meta do produto**: Precision â‰¥90%, Recall â‰¥85%, FPR <3%  
> **CritÃ©rios Sprint 07 revisados**: NÃ£o regredir de Sprint 06, garantir FPR <3% via threshold tuning

### Criterios TÃ©cnicos de AceitaÃ§Ã£o

```
âœ… CRÃTICO (MUST HAVE)
  â–¡ calibrate() implementado (Platt recomendado, isotonic se Nâ‰¥500)
  â–¡ predict_proba_calibrated() implementado
  â–¡ select_optimal_threshold() implementado (cost-based + FPR constraint)
  â–¡ Split Ãºnico disjunto: train/cal/val/test (SEM vazamento, por vÃ­deo!)
  â–¡ Modelo treinado DO ZERO no script (nÃ£o carregar pronto)
  â–¡ Calibration set SEPARADO (nunca visto no treino)
  â–¡ Threshold tunado em validation set SEPARADO (nunca visto no treino/cal)
  â–¡ Calibration melhora: Brier Score â‰¤ Baseline, ECE â‰¤ Baseline
  â–¡ ROC-AUC reportado antes/depois (sem assumir que mantÃ©m)
  â–¡ **FPR <3%** via threshold tuning (meta CRÃTICA do produto) âœ…
  â–¡ No regression vs Sprint 06: Precision Â± 1pp, Recall Â± 2pp

âœ… IMPORTANTE (SHOULD HAVE)
  â–¡ Calibration curve plotada (reliability diagram)
  â–¡ ROC curve plotada (threshold marcado, FPR<3% destacado)
  â–¡ Precision: â‰¥ 90% (meta do produto, nÃ£o 97%)
  â–¡ Recall: â‰¥ 85% (meta do produto, nÃ£o 97%)
  â–¡ F1: â‰¥ 87% (derivado de precision/recall metas)
  â–¡ Threshold selecionado via custo customizado com constraint FPR<3%

âœ… NICE TO HAVE (COULD HAVE)
  â–¡ Monitoramento de calibraÃ§Ã£o em produÃ§Ã£o
  â–¡ Sensitivity analysis (variar cost_fp/cost_fn)
  â–¡ Comparison Platt vs Isotonic (se N_cal â‰¥ 500)
```

### DefiniÃ§Ã£o de "Sucesso" para Sprint 07

**Requisito de AprovaÃ§Ã£o (REVISADO - alinhado com meta do produto):**

1. âœ… CÃ³digo completo (calibraÃ§Ã£o + threshold tuning)
2. âœ… Calibration set independente (nÃ£o usado em treino, split por vÃ­deo)
3. âœ… Calibration melhora: Brier Score e ECE nÃ£o pioram vs baseline
4. âœ… **FPR <3%** via threshold tuning (CRÃTICO) âœ…
5. âœ… Precision: â‰¥ 90% (meta do produto, nÃ£o 97%)
6. âœ… Recall: â‰¥ 85% (meta do produto, nÃ£o 97%)
7. âœ… Threshold: selecionado via custo + constraint FPR<3%
8. âœ… Calibration curve: confiÃ¡vel (ECE â‰¤ 0.10)
9. âœ… ROC curve: AUC â‰¥ 0.95 (alta discriminaÃ§Ã£o)
10. âœ… CÃ³digo review aprovado
11. âœ… Testes unitÃ¡rios: test_calibration.py (coverage 90%)

**Nota sobre metas 97%/97%:**  
Metas originais (97% precision/ 97% recall) eram aspiracionais mas **bloqueiam roadmap** se nÃ£o atingidas.  
Sprint 07 Ã© aprovada se atingir meta do produto (â‰¥90%/â‰¥85%) + FPR<3%.  
Metas >95% sÃ£o "stretch goals" (nice-to-have, nÃ£o blockers).

---

### Checklist de ImplementaÃ§Ã£o

```
Code Implementation:
  â˜ app/ml/subtitle_classifier.py estendido (~250 linhas)
    â˜ calibrate() implementado (Platt + Isotonic + 'auto')
    â˜ Auto-select: Platt se N_cal < 100, Isotonic se >= 100
    â˜ _calibrate_proba() helper implementado
    â˜ predict_proba_calibrated() implementado
    â˜ select_optimal_threshold() implementado (cost/f1/youden)
    â˜ ValidaÃ§Ã£o de calibration set separado
    â˜ Guardar n_calibration_samples no metadata
  â˜ app/ml/calibration_utils.py criado (~180 linhas)
    â˜ compute_expected_calibration_error() implementado
    â˜ Suporte para strategy='uniform' e 'quantile'
    â˜ plot_calibration_curve() implementado
    â˜ plot_roc_curve_with_threshold() implementado
  â˜ scripts/calibrate_and_tune_threshold.py criado (~180 linhas)
    â˜ CLI de calibraÃ§Ã£o + threshold tuning
    â˜ Split ÃšNICO disjunto: train / cal / val / test
    â˜ Treinar modelo DO ZERO (nÃ£o carregar pronto)
    â˜ Reportar ROC-AUC e PR-AUC antes/depois
    â˜ Reportar ECE uniform e quantile
    â˜ Gerar plots (calibration curve, ROC curve)
    â˜ Metadata completo (N_cal, AUCs, ECEs, costs)

Validation:
  â˜ Baseline uncalibrated medido (Brier, ECE)
  â˜ CalibraÃ§Ã£o aplicada (Platt ou Isotonic)
  â˜ Threshold tunado (cost-based)
  â˜ ValidaÃ§Ã£o em test set:
    â˜ Brier Score melhora â‰¥ -0.02
    â˜ ECE melhora â‰¥ -0.05
    â˜ Precision â‰¥ 97%
    â˜ Recall â‰¥ 97%
    â˜ F1 â‰¥ 97%
  â˜ Calibration curve plotada e analisada
  â˜ ROC curve plotada (threshold marcado)

Testing:
  â˜ Testes escritos:
    â˜ test_calibration.py (calibrate + predict_proba_calibrated)
    â˜ test_threshold_selection.py (select_optimal_threshold)
    â˜ test_calibration_utils.py (ECE, plots)
  â˜ Coverage â‰¥ 90%

Documentation:
  â˜ Docstrings completos
  â˜ README: instruÃ§Ãµes de calibraÃ§Ã£o
  â˜ Calibration report (comparativo uncal vs cal)

Deployment:
  â˜ Code review feito
  â˜ A/B test Sprint 06 (uncalibrated) vs Sprint 07 (calibrated)
  â˜ Brier/ECE validados
  â˜ Precision/Recall mantidos
  â˜ AprovaÃ§Ã£o de PM/Tech Lead
  â˜ Merge para main
  â˜ Deploy em produÃ§Ã£o (10% trÃ¡fego)
  â˜ Monitoramento 48h (calibraÃ§Ã£o + mÃ©tricas)
  â˜ 100% rollout se Brier < 0.05
```

---

## ğŸ“‹ Resumo da Sprint

| Aspecto | Detalhe |
|---------|---------|
| **Objetivo** | Calibrar probabilidades e otimizar threshold via ROC curve |
| **Problema** | Probabilidades nÃ£o-calibradas (Brier=0.08) + threshold nÃ£o-Ã³timo (max F1 apenas) |
| **SoluÃ§Ã£o** | Platt Scaling/Isotonic + threshold tuning via custo customizado |
| **Impacto** | +1-3% precision/recall, -0.04 Brier, -0.07 ECE |
| **Arquitetura** | Classificador â†’ Calibrador â†’ proba_calibrated â†’ threshold Ã³timo â†’ Decision |
| **Risco** | BAIXO (calibraÃ§Ã£o Ã© step padrÃ£o, bem validado) |
| **EsforÃ§o** | ~6-8h (calibraÃ§Ã£o 40%, threshold tuning 40%, validation 20%) |
| **LatÃªncia** | +0.1-0.5ms (calibraÃ§Ã£o adicional, negligÃ­vel) |
| **Linhas de cÃ³digo** | ~580 linhas (extensÃ£o classifier + utils + script) |
| **CalibraÃ§Ã£o** | **Platt Scaling (LogReg) ou Isotonic (XGBoost)** |
| **Threshold** | **Selecionado via min cost (customizÃ¡vel: FP, FN weights)** |
| **MÃ©tricas** | **Brier Score, ECE (calibraÃ§Ã£o) + Precision/Recall (threshold)** |
| **DependÃªncias** | Sprint 06 (classificador treinado), Calibration set (50+ vÃ­deos) |
| **PrÃ³xima Sprint** | Sprint 08 (Validation, Regression Testing & Production) |

---

## ğŸš€ PrÃ³ximos Passos

1. âœ… Sprint 07 documentada
2. â³ **Implementar calibraÃ§Ã£o** (Platt Scaling)
3. â³ **Implementar threshold tuning** (via custo)
4. â³ **Validar no test set** (Brier -0.02, ECE -0.05)
5. ğŸ“Š **Gerar calibration curve** (reliability diagram)
6. ğŸ“Š **Gerar ROC curve** (threshold marcado)
7. â¡ï¸ Proceder para Sprint 08 (Validation & Production)

---

**Nota Final:**

Sprint 07 Ã© **refinamento final**:
- Remove descalibraÃ§Ã£o (Brier 0.08 â†’ 0.04)
- Otimiza threshold via custo (alinhado com negÃ³cio)
- Prepara sistema para produÃ§Ã£o (probabilidades confiÃ¡veis)

**Ganho esperado: +1-3%** em precision/recall, mas **impacto real Ã© confiabilidade**.

Probabilidades calibradas = **decisÃµes mais corretas** = **melhor UX** = **menos erros custosos**.

Sprint 08 validarÃ¡ todo o sistema (Sprints 01-07) em hold-out final e prepararÃ¡ deploy.
