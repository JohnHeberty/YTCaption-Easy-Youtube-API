# Sprint 06: Ensemble de Modelos PrÃ©-Treinados (REVISADO)

**Objetivo**: Implementar sistema ensemble com 3 modelos prÃ©-treinados para mÃ¡xima precisÃ£o (plug and play, zero training)  
**Impacto Esperado**: +10-20% precision/recall (ensemble > single model)  
**Criticidade**: â­â­â­â­â­ **CRÃTICO** (PrÃ³xima etapa evolutiva apÃ³s Multi-ROI)  
**Data**: 2026-02-14  
**Status**: ðŸŸ¢ Pronto para implementar (Sprint 00-04 completos)  
**DependÃªncias**: Sprint 00-04 (PaddleOCR + Multi-ROI ready)

> **ðŸ”„ REVISÃƒO ARQUITETURAL:**  
> MudanÃ§a de abordagem de ML tradicional (treinar Random Forest) para **Ensemble de Modelos PrÃ©-Treinados**.  
> 
> **Motivo**: Evitar trabalho manual de coleta/rotulaÃ§Ã£o de 200+ vÃ­deos. Usar modelos state-of-the-art jÃ¡ treinados.  
> 
> **BenefÃ­cios**:  
> - âœ… **100% plug and play** (sÃ³ download de modelos)  
> - âœ… **Zero manual labeling** (sem dataset collection)  
> - âœ… **Modelos robustos** (treinados em milhÃµes de exemplos)  
> - âœ… **RÃ¡pido de implementar** (~4-6 horas vs. 1-2 semanas)  
> - âœ… **Alta precisÃ£o** (ensemble mitiga fraquezas individuais)

---

## ðŸ“‹ ÃNDICE

1. [Objetivo TÃ©cnico](#1ï¸âƒ£-objetivo-tÃ©cnico-claro)
2. [Arquitetura do Ensemble](#2ï¸âƒ£-arquitetura-do-ensemble)
3. [Modelos PrÃ©-Treinados](#3ï¸âƒ£-modelos-prÃ©-treinados)
4. [Sistema de VotaÃ§Ã£o](#4ï¸âƒ£-sistema-de-votaÃ§Ã£o)
5. [ImplementaÃ§Ã£o](#5ï¸âƒ£-implementaÃ§Ã£o)
6. [Testes](#6ï¸âƒ£-testes-esperados)
7. [IntegraÃ§Ã£o](#7ï¸âƒ£-integraÃ§Ã£o-com-sprints-anteriores)

---

## 1ï¸âƒ£ Objetivo TÃ©cnico Claro

### Problema EspecÃ­fico

Atualmente (Sprint 00-04) temos **apenas PaddleOCR** como detector:

```python
# CÃ“DIGO ATUAL (apÃ³s Sprint 04)
detector = SubtitleDetectorV2(roi_mode='multi')
has_subs, conf, text, metadata = detector.detect_in_video_with_multi_roi(video_path)

# Problema: Single point of failure
# - Se PaddleOCR falhar â†’ sistema erra
# - Sem redundÃ¢ncia ou validaÃ§Ã£o cruzada
```

**Problemas CrÃ­ticos:**

### 1) **Single Point of Failure**
- PaddleOCR pode falhar em:
  - Fontes raras ou estilizadas
  - Baixo contraste (mesmo com CLAHE)
  - Texto rotacionado ou distorcido
  - Idiomas especÃ­ficos (Ã¡rabe, japonÃªs)

### 2) **Sem ValidaÃ§Ã£o Cruzada**
- Uma Ãºnica detecÃ§Ã£o = decisÃ£o final
- Nenhuma confirmaÃ§Ã£o por modelo independente
- Alto risco de falsos positivos/negativos

### 3) **NÃ£o Aproveita Modelos State-of-the-Art**
- CLIP (OpenAI) = zero-shot classifier de 400M imagens
- CRAFT = detector de texto state-of-the-art
- EasyOCR = alternativa ao PaddleOCR, multi-idioma

**SoluÃ§Ã£o**: Ensemble de 3 modelos prÃ©-treinados com votaÃ§Ã£o ponderada.

---

## 2ï¸âƒ£ Arquitetura do Ensemble

### VisÃ£o Geral

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ENSEMBLE SYSTEM (Sprint 06)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  Input: video_path                                                â”‚
â”‚     â”‚                                                             â”‚
â”‚     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚     â”‚          â”‚              â”‚              â”‚                  â”‚
â”‚     â–¼          â–¼              â–¼              â–¼                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚Paddleâ”‚  â”‚ CLIP â”‚      â”‚CRAFT â”‚      â”‚ Easy â”‚  (4 models)   â”‚
â”‚  â”‚ OCR  â”‚  â”‚(Zero-â”‚      â”‚(Text â”‚      â”‚ OCR  â”‚               â”‚
â”‚  â”‚Multi-â”‚  â”‚Shot) â”‚      â”‚Detectâ”‚      â”‚(Alt.)â”‚               â”‚
â”‚  â”‚ ROI  â”‚  â”‚      â”‚      â”‚)     â”‚      â”‚      â”‚               â”‚
â”‚  â””â”€â”€â”€â”¬â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”˜      â””â”€â”€â”€â”¬â”€â”€â”˜      â””â”€â”€â”€â”¬â”€â”€â”˜               â”‚
â”‚      â”‚         â”‚             â”‚             â”‚                   â”‚
â”‚      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                   â”‚
â”‚      â”‚                                      â”‚                   â”‚
â”‚      â–¼                  â–¼                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚  â”‚   Voting System (Sprint 07)      â”‚                           â”‚
â”‚  â”‚   - Weighted Average              â”‚                           â”‚
â”‚  â”‚   - Confidence Aggregation        â”‚                           â”‚
â”‚  â”‚   - Conflict Resolution           â”‚                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                 â”‚                                                â”‚
â”‚                 â–¼                                                â”‚
â”‚    Output: {has_subtitles, confidence, votes, metadata}         â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fluxo de ExecuÃ§Ã£o

```python
1. Preprocessamento (Sprint 01-02)
   â”œâ”€ Resize para resoluÃ§Ã£o adequada
   â”œâ”€ CLAHE (contrast enhancement)
   â””â”€ Extract temporal frames (6 frames)

2. DetecÃ§Ã£o Paralela (4 modelos)
   â”œâ”€ PaddleOCR + Multi-ROI (Sprint 04)
   â”œâ”€ CLIP zero-shot classification
   â”œâ”€ CRAFT text detection
   â””â”€ EasyOCR (alternativo)

3. VotaÃ§Ã£o Ponderada (Sprint 07)
   â”œâ”€ Peso por confiabilidade do modelo
   â”œâ”€ DetecÃ§Ã£o de conflitos
   â””â”€ Confidence final agregado

4. DecisÃ£o Final
   â””â”€ has_subtitles: bool (weighted vote > 0.5)
      confidence: float (0-1)
      votes: dict (resultado de cada modelo)
      metadata: dict (ROI usado, tempos, etc.)
```

---

## 3ï¸âƒ£ Modelos PrÃ©-Treinados

### Modelo 1: PaddleOCR + Multi-ROI (Sprint 00-04) âœ… JÃ IMPLEMENTADO

**Status**: âœ… Completo (36/37 testes passando)

**CaracterÃ­sticas**:
- 6 ROIs (bottom, top, left, right, center, full)
- Priority-based fallback
- 100% accuracy nos 83 vÃ­deos de teste
- Performance: â‰¤8s worst case, â‰¤3s fast path

**Vantagens**:
- âœ… JÃ¡ implementado e testado
- âœ… Multi-ROI coverage (100%)
- âœ… Otimizado para legendas

**LimitaÃ§Ãµes**:
- âš ï¸ Pode falhar em fontes muito estilizadas
- âš ï¸ Depende de OCR (precisa ler texto)

**Peso no Ensemble**: 35% (confiÃ¡vel, mas nÃ£o perfeito)

---

### Modelo 2: CLIP (OpenAI) - Zero-Shot Classification ðŸ†•

**O que Ã©**:
- Modelo de visÃ£o-linguagem da OpenAI
- Treinado em 400M pares (imagem, texto)
- Zero-shot: classifica sem treino adicional

**Como funciona**:
```python
from transformers import CLIPProcessor, CLIPModel
import torch

# 1. Carregar modelo (sÃ³ download, ~600MB)
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# 2. Preparar prompts
text_prompts = [
    "A video frame with burned-in subtitles at the bottom",
    "A video frame with hardcoded subtitles or captions",
    "A video frame without any subtitles or text overlays",
    "A clean video frame with no embedded text"
]

# 3. Classificar frame
inputs = processor(
    text=text_prompts,
    images=frame,
    return_tensors="pt",
    padding=True
)

with torch.no_grad():
    outputs = model(**inputs)
    logits_per_image = outputs.logits_per_image  # Shape: [1, 4]
    probs = logits_per_image.softmax(dim=1)      # Normalize

# 4. DecisÃ£o
has_subtitles_prob = (probs[0][0] + probs[0][1]) / 2  # MÃ©dia dos 2 primeiros
no_subtitles_prob = (probs[0][2] + probs[0][3]) / 2   # MÃ©dia dos 2 Ãºltimos

has_subtitles = has_subtitles_prob > no_subtitles_prob
confidence = max(has_subtitles_prob, no_subtitles_prob).item()
```

**Vantagens**:
- âœ… Zero-shot (sem treino)
- âœ… Robusto (400M exemplos)
- âœ… Detecta padrÃµes semÃ¢nticos (nÃ£o sÃ³ OCR)
- âœ… Funciona com qualquer idioma
- âœ… RÃ¡pido (~50ms por frame com GPU)

**LimitaÃ§Ãµes**:
- âš ï¸ Pode confundir texto geral com legendas
- âš ï¸ Menos preciso que OCR para texto especÃ­fico
- âš ï¸ Requer GPU para ser rÃ¡pido

**Peso no Ensemble**: 30% (boa visÃ£o geral, mas menos especÃ­fico)

**InstalaÃ§Ã£o**:
```bash
pip install transformers torch pillow
```

---

### Modelo 3: CRAFT (Character Region Awareness) ðŸ†•

**O que Ã©**:
- Detector de texto state-of-the-art (2019)
- Treinado em ICDAR, SynthText, COCO-Text
- Detecta regiÃµes de texto com bounding boxes

**Como funciona**:
```python
import craft_text_detector
from PIL import Image

# 1. Carregar modelo (sÃ³ download, ~150MB)
detector = craft_text_detector.Craft(
    output_dir='storage/craft_output/',
    crop_type="box",
    cuda=True
)

# 2. Detectar texto no frame
frame_path = 'frame.jpg'
prediction_result = detector.detect_text(frame_path)

# 3. Analisar regiÃµes detectadas
text_boxes = prediction_result['boxes']
frame_height = frame.shape[0]

# Filtrar regiÃµes no bottom 25% (tÃ­pico de legendas)
bottom_boxes = [
    box for box in text_boxes 
    if box['y'] + box['height'] > frame_height * 0.75
]

# Calcular mÃ©tricas
total_text_area = sum(box['width'] * box['height'] for box in text_boxes)
bottom_text_area = sum(box['width'] * box['height'] for box in bottom_boxes)
bottom_ratio = bottom_text_area / total_text_area if total_text_area > 0 else 0

# DecisÃ£o
has_subtitles = (
    len(bottom_boxes) >= 1 and                    # Pelo menos 1 regiÃ£o no bottom
    bottom_ratio > 0.6 and                        # 60%+ do texto estÃ¡ no bottom
    any(box['width'] > frame.shape[1] * 0.3       # Alguma regiÃ£o com largura > 30% do frame
        for box in bottom_boxes)
)

confidence = min(bottom_ratio, len(bottom_boxes) / 3)  # Max 3 boxes = 100%
```

**Vantagens**:
- âœ… Estado-da-arte em detecÃ§Ã£o de texto
- âœ… NÃ£o precisa OCR (sÃ³ detecta regiÃµes)
- âœ… Funciona com qualquer idioma/fonte
- âœ… Detecta padrÃµes geomÃ©tricos de legendas

**LimitaÃ§Ãµes**:
- âš ï¸ Pesado (~150MB)
- âš ï¸ Requer GPU para ser eficiente
- âš ï¸ Pode detectar UI elements como texto

**Peso no Ensemble**: 25% (Ã³timo complemento, mas pode ter FP)

**InstalaÃ§Ã£o**:
```bash
pip install craft-text-detector
```

---

### Modelo 4: EasyOCR (Alternativo) ðŸ†• OPCIONAL

**O que Ã©**:
- Alternativa ao PaddleOCR
- Suporta 80+ idiomas
- Baseado em CRAFT + CRNN

**Como funciona**:
```python
import easyocr

# 1. Carregar modelo (download automÃ¡tico)
reader = easyocr.Reader(['en', 'pt', 'es'], gpu=True)

# 2. Detectar texto
results = reader.readtext(frame)

# 3. AnÃ¡lise (similar ao PaddleOCR)
bottom_texts = [
    res for res in results
    if res[0][0][1] > frame_height * 0.75  # y-coordinate no bottom 25%
]

has_subtitles = len(bottom_texts) >= 1
confidence = max([res[2] for res in bottom_texts], default=0.0)
```

**Vantagens**:
- âœ… Multi-idioma (80+ languages)
- âœ… FÃ¡cil de usar
- âœ… Boa alternativa ao PaddleOCR

**LimitaÃ§Ãµes**:
- âš ï¸ Mais lento que PaddleOCR
- âš ï¸ Overlap significativo com PaddleOCR

**Peso no Ensemble**: 10% (redundante com Paddle, mas Ãºtil como fallback)

**InstalaÃ§Ã£o**:
```bash
pip install easyocr
```

---

## 4ï¸âƒ£ Sistema de VotaÃ§Ã£o

### EstratÃ©gia 1: Weighted Average (Simples) âœ… RECOMENDADO

```python
class EnsembleSubtitleDetector:
    def __init__(self):
        self.weights = {
            'paddle': 0.35,  # 35% - Mais confiÃ¡vel (Sprint 00-04)
            'clip': 0.30,    # 30% - Boa visÃ£o geral
            'craft': 0.25,   # 25% - Especializado em texto
            'easyocr': 0.10  # 10% - Fallback redundante
        }
    
    def detect(self, video_path):
        # 1. Rodar todos os modelos
        votes = {}
        
        # Paddle (Multi-ROI)
        paddle_result = self.paddle_detector.detect_in_video_with_multi_roi(video_path)
        votes['paddle'] = {
            'has_subtitles': paddle_result[0],
            'confidence': paddle_result[1],
            'weight': self.weights['paddle']
        }
        
        # CLIP (Zero-shot)
        clip_result = self.clip_classifier.classify(video_path)
        votes['clip'] = {
            'has_subtitles': clip_result['has_subtitles'],
            'confidence': clip_result['confidence'],
            'weight': self.weights['clip']
        }
        
        # CRAFT (Text detection)
        craft_result = self.craft_detector.detect(video_path)
        votes['craft'] = {
            'has_subtitles': craft_result['has_subtitles'],
            'confidence': craft_result['confidence'],
            'weight': self.weights['craft']
        }
        
        # EasyOCR (optional)
        if self.use_easyocr:
            easyocr_result = self.easyocr_detector.detect(video_path)
            votes['easyocr'] = {
                'has_subtitles': easyocr_result['has_subtitles'],
                'confidence': easyocr_result['confidence'],
                'weight': self.weights['easyocr']
            }
        
        # 2. VotaÃ§Ã£o ponderada
        weighted_score = 0.0
        total_weight = 0.0
        
        for model_name, vote in votes.items():
            if vote['has_subtitles']:
                weighted_score += vote['confidence'] * vote['weight']
            total_weight += vote['weight']
        
        # Normalizar
        final_confidence = weighted_score / total_weight if total_weight > 0 else 0.0
        final_decision = final_confidence >= 0.5
        
        return {
            'has_subtitles': final_decision,
            'confidence': final_confidence,
            'votes': votes,
            'metadata': {
                'ensemble_method': 'weighted_average',
                'weights': self.weights,
                'timestamp': time.time()
            }
        }
```

**Exemplo de VotaÃ§Ã£o**:

```python
# Caso 1: Consenso (3/3 concordam)
votes = {
    'paddle': {'has_subtitles': True, 'confidence': 0.92, 'weight': 0.35},
    'clip':   {'has_subtitles': True, 'confidence': 0.88, 'weight': 0.30},
    'craft':  {'has_subtitles': True, 'confidence': 0.85, 'weight': 0.25}
}

weighted_score = 0.92*0.35 + 0.88*0.30 + 0.85*0.25 = 0.322 + 0.264 + 0.2125 = 0.7985
final_confidence = 0.7985 / 0.90 = 0.887  # 88.7% confianÃ§a
final_decision = True  # TEM legendas âœ…

# Caso 2: Desacordo (2/3 vs 1/3)
votes = {
    'paddle': {'has_subtitles': True,  'confidence': 0.75, 'weight': 0.35},
    'clip':   {'has_subtitles': False, 'confidence': 0.82, 'weight': 0.30},
    'craft':  {'has_subtitles': True,  'confidence': 0.68, 'weight': 0.25}
}

weighted_score = 0.75*0.35 + 0 + 0.68*0.25 = 0.2625 + 0 + 0.17 = 0.4325
final_confidence = 0.4325 / 0.90 = 0.480  # 48% confianÃ§a
final_decision = False  # NÃƒO TEM legendas (below 50%) âš ï¸

# Caso 3: Paddle forte, outros fracos
votes = {
    'paddle': {'has_subtitles': True,  'confidence': 0.95, 'weight': 0.35},
    'clip':   {'has_subtitles': False, 'confidence': 0.55, 'weight': 0.30},
    'craft':  {'has_subtitles': False, 'confidence': 0.60, 'weight': 0.25}
}

weighted_score = 0.95*0.35 + 0 + 0 = 0.3325
final_confidence = 0.3325 / 0.90 = 0.369  # 37% confianÃ§a
final_decision = False  # NÃƒO TEM (Paddle sozinho nÃ£o basta) âš ï¸
```

---

### EstratÃ©gia 2: Majority Voting (Alternativa)

```python
def majority_voting(votes):
    """
    VotaÃ§Ã£o simples: maioria vence (sem pesos).
    """
    yes_votes = sum(1 for v in votes.values() if v['has_subtitles'])
    no_votes = len(votes) - yes_votes
    
    final_decision = yes_votes > no_votes
    
    # Confidence = proporÃ§Ã£o da maioria
    total_votes = len(votes)
    final_confidence = yes_votes / total_votes if final_decision else no_votes / total_votes
    
    return final_decision, final_confidence
```

---

## 5ï¸âƒ£ ImplementaÃ§Ã£o

### Fase 1: Setup dos Modelos (30 min)

```bash
# 1. Instalar dependÃªncias
pip install transformers torch pillow craft-text-detector easyocr

# 2. Download dos modelos (automÃ¡tico no primeiro uso)
# - CLIP: ~/.cache/huggingface/transformers/ (~600MB)
# - CRAFT: ~/.craft_text_detector/ (~150MB)
# - EasyOCR: ~/.EasyOCR/model/ (~150MB por idioma)
```

### Fase 2: Implementar Classes Base (2h)

**Estrutura de arquivos**:

```
services/make-video/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ video_processing/
â”‚   â”‚   â”œâ”€â”€ subtitle_detector_v2.py         # âœ… JÃ¡ existe (Sprint 04)
â”‚   â”‚   â”œâ”€â”€ ensemble_detector.py            # ðŸ†• Main ensemble
â”‚   â”‚   â”œâ”€â”€ detectors/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ paddle_detector.py          # ðŸ†• Wrapper do V2
â”‚   â”‚   â”‚   â”œâ”€â”€ clip_classifier.py          # ðŸ†• CLIP
â”‚   â”‚   â”‚   â”œâ”€â”€ craft_detector.py           # ðŸ†• CRAFT
â”‚   â”‚   â”‚   â””â”€â”€ easyocr_detector.py         # ðŸ†• EasyOCR (opcional)
â”‚   â”‚   â””â”€â”€ voting/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ weighted_voting.py          # ðŸ†• Weighted average
â”‚   â”‚       â””â”€â”€ majority_voting.py          # ðŸ†• Simple majority
```

**Interface Comum**:

```python
# app/video_processing/detectors/base_detector.py
from abc import ABC, abstractmethod
from typing import Dict, Tuple

class BaseSubtitleDetector(ABC):
    """
    Interface comum para todos os detectores do ensemble.
    """
    
    @abstractmethod
    def detect(self, video_path: str) -> Dict:
        """
        Detecta legendas em um vÃ­deo.
        
        Args:
            video_path: Caminho para o arquivo de vÃ­deo
        
        Returns:
            {
                'has_subtitles': bool,
                'confidence': float,  # 0-1
                'metadata': dict
            }
        """
        pass
    
    @abstractmethod
    def get_model_name(self) -> str:
        """Retorna nome do modelo."""
        pass
    
    @abstractmethod
    def get_weight(self) -> float:
        """Retorna peso padrÃ£o no ensemble."""
        pass
```

### Fase 3: Implementar Detectores (3h)

#### 3.1 PaddleDetector (Wrapper) - 30 min

```python
# app/video_processing/detectors/paddle_detector.py
from .base_detector import BaseSubtitleDetector
from ..subtitle_detector_v2 import SubtitleDetectorV2

class PaddleDetector(BaseSubtitleDetector):
    """
    Wrapper do SubtitleDetectorV2 (Sprint 00-04).
    """
    
    def __init__(self, roi_mode='multi'):
        self.detector = SubtitleDetectorV2(roi_mode=roi_mode)
    
    def detect(self, video_path: str) -> dict:
        has_subs, confidence, text, metadata = \
            self.detector.detect_in_video_with_multi_roi(video_path)
        
        return {
            'has_subtitles': has_subs,
            'confidence': confidence,
            'metadata': {
                'text': text,
                'roi_used': metadata.get('roi_used'),
                'model': 'paddleocr'
            }
        }
    
    def get_model_name(self) -> str:
        return 'paddle'
    
    def get_weight(self) -> float:
        return 0.35  # 35% weight
```

#### 3.2 CLIPClassifier - 1h

```python
# app/video_processing/detectors/clip_classifier.py
import torch
from transformers import CLIPProcessor, CLIPModel
from PIL import Image
import cv2
from .base_detector import BaseSubtitleDetector

class CLIPClassifier(BaseSubtitleDetector):
    """
    Zero-shot subtitle classifier usando CLIP.
    """
    
    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):
        self.device = device
        
        # Carregar modelo
        self.model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
        self.processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
        
        # Prompts para classificaÃ§Ã£o
        self.prompts = [
            "A video frame with burned-in subtitles or captions at the bottom",
            "A video frame with hardcoded text overlays or subtitles",
            "A clean video frame without any subtitles or embedded text",
            "A video frame with no captions or text overlays"
        ]
    
    def detect(self, video_path: str) -> dict:
        # 1. Extrair frames (usar mesma estratÃ©gia do Sprint 01)
        frames = self._extract_frames(video_path, n_frames=6)
        
        # 2. Classificar cada frame
        frame_results = []
        for frame in frames:
            result = self._classify_frame(frame)
            frame_results.append(result)
        
        # 3. Agregar resultados
        has_subtitles_votes = sum(1 for r in frame_results if r['has_subtitles'])
        confidence_scores = [r['confidence'] for r in frame_results]
        
        has_subtitles = has_subtitles_votes >= (len(frames) // 2)  # Maioria
        confidence = sum(confidence_scores) / len(confidence_scores)  # MÃ©dia
        
        return {
            'has_subtitles': has_subtitles,
            'confidence': confidence,
            'metadata': {
                'frame_results': frame_results,
                'votes': f'{has_subtitles_votes}/{len(frames)}',
                'model': 'clip'
            }
        }
    
    def _classify_frame(self, frame) -> dict:
        """Classifica um Ãºnico frame."""
        # Converter BGR (OpenCV) para RGB (PIL)
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        pil_image = Image.fromarray(frame_rgb)
        
        # Processar com CLIP
        inputs = self.processor(
            text=self.prompts,
            images=pil_image,
            return_tensors="pt",
            padding=True
        ).to(self.device)
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            logits = outputs.logits_per_image
            probs = logits.softmax(dim=1)
        
        # Calcular probabilidades
        has_subtitles_prob = (probs[0][0] + probs[0][1]) / 2  # MÃ©dia prompts 0 e 1
        no_subtitles_prob = (probs[0][2] + probs[0][3]) / 2   # MÃ©dia prompts 2 e 3
        
        has_subtitles = has_subtitles_prob > no_subtitles_prob
        confidence = max(has_subtitles_prob, no_subtitles_prob).item()
        
        return {
            'has_subtitles': has_subtitles,
            'confidence': confidence
        }
    
    def _extract_frames(self, video_path: str, n_frames: int = 6):
        """Extrair frames temporais (mesma lÃ³gica Sprint 01)."""
        cap = cv2.VideoCapture(video_path)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        duration = total_frames / fps if fps > 0 else 0
        
        # Timestamps distribuÃ­dos (20%-80% do vÃ­deo)
        timestamps = [
            duration * 0.2,
            duration * 0.35,
            duration * 0.5,
            duration * 0.65,
            duration * 0.8,
            duration * 0.95
        ]
        
        frames = []
        for ts in timestamps:
            cap.set(cv2.CAP_PROP_POS_MSEC, ts * 1000)
            ret, frame = cap.read()
            if ret:
                frames.append(frame)
        
        cap.release()
        return frames
    
    def get_model_name(self) -> str:
        return 'clip'
    
    def get_weight(self) -> float:
        return 0.30  # 30% weight
```

#### 3.3 CRAFTDetector - 1h

```python
# app/video_processing/detectors/craft_detector.py
import craft_text_detector
import cv2
from .base_detector import BaseSubtitleDetector

class CRAFTDetector(BaseSubtitleDetector):
    """
    Text detection usando CRAFT.
    """
    
    def __init__(self, output_dir='storage/craft_output/', use_gpu=True):
        self.detector = craft_text_detector.Craft(
            output_dir=output_dir,
            crop_type="box",
            cuda=use_gpu
        )
    
    def detect(self, video_path: str) -> dict:
        # 1. Extrair frames
        frames = self._extract_frames(video_path, n_frames=6)
        
        # 2. Detectar texto em cada frame
        frame_results = []
        for i, frame in enumerate(frames):
            result = self._detect_in_frame(frame, frame_idx=i)
            frame_results.append(result)
        
        # 3. Agregar resultados
        has_subtitles_votes = sum(1 for r in frame_results if r['has_subtitles'])
        confidence_scores = [r['confidence'] for r in frame_results]
        
        has_subtitles = has_subtitles_votes >= (len(frames) // 2)
        confidence = sum(confidence_scores) / len(confidence_scores)
        
        return {
            'has_subtitles': has_subtitles,
            'confidence': confidence,
            'metadata': {
                'frame_results': frame_results,
                'votes': f'{has_subtitles_votes}/{len(frames)}',
                'model': 'craft'
            }
        }
    
    def _detect_in_frame(self, frame, frame_idx: int) -> dict:
        """Detecta texto em um frame."""
        # Salvar frame temporariamente (CRAFT precisa de arquivo)
        temp_path = f'/tmp/frame_{frame_idx}.jpg'
        cv2.imwrite(temp_path, frame)
        
        # Detectar texto
        prediction_result = self.detector.detect_text(temp_path)
        text_boxes = prediction_result['boxes']
        
        if not text_boxes:
            return {'has_subtitles': False, 'confidence': 0.0}
        
        # Analisar regiÃµes
        frame_height, frame_width = frame.shape[:2]
        
        # Filtrar regiÃµes no bottom 25%
        bottom_boxes = [
            box for box in text_boxes
            if box['y'] + box['height'] > frame_height * 0.75
        ]
        
        if not bottom_boxes:
            return {'has_subtitles': False, 'confidence': 0.2}
        
        # Calcular mÃ©tricas
        total_text_area = sum(box['width'] * box['height'] for box in text_boxes)
        bottom_text_area = sum(box['width'] * box['height'] for box in bottom_boxes)
        bottom_ratio = bottom_text_area / total_text_area if total_text_area > 0 else 0
        
        # Verificar se hÃ¡ regiÃ£o larga (tÃ­pico de legenda)
        has_wide_box = any(
            box['width'] > frame_width * 0.3
            for box in bottom_boxes
        )
        
        # DecisÃ£o
        has_subtitles = (
            len(bottom_boxes) >= 1 and
            bottom_ratio > 0.5 and
            has_wide_box
        )
        
        confidence = min(bottom_ratio, len(bottom_boxes) / 3.0)
        
        return {
            'has_subtitles': has_subtitles,
            'confidence': confidence
        }
    
    def _extract_frames(self, video_path: str, n_frames: int = 6):
        """Mesma lÃ³gica de CLIPClassifier."""
        # ... (copiar implementation do CLIP)
        pass
    
    def get_model_name(self) -> str:
        return 'craft'
    
    def get_weight(self) -> float:
        return 0.25  # 25% weight
```

### Fase 4: Implementar Ensemble (1h)

```python
# app/video_processing/ensemble_detector.py
from typing import Dict, List
from .detectors.base_detector import BaseSubtitleDetector
from .detectors.paddle_detector import PaddleDetector
from .detectors.clip_classifier import CLIPClassifier
from .detectors.craft_detector import CRAFTDetector

class EnsembleSubtitleDetector:
    """
    Ensemble de mÃºltiplos detectores de legenda.
    """
    
    def __init__(
        self,
        detectors: List[BaseSubtitleDetector] = None,
        voting_method: str = 'weighted'
    ):
        if detectors is None:
            # Carregar detectores padrÃ£o
            self.detectors = [
                PaddleDetector(roi_mode='multi'),
                CLIPClassifier(),
                CRAFTDetector()
            ]
        else:
            self.detectors = detectors
        
        self.voting_method = voting_method
    
    def detect(self, video_path: str) -> Dict:
        """
        Detecta legendas usando ensemble.
        
        Returns:
            {
                'has_subtitles': bool,
                'confidence': float,
                'votes': dict,  # Resultado de cada detector
                'metadata': dict
            }
        """
        # 1. Rodar todos os detectores
        votes = {}
        for detector in self.detectors:
            model_name = detector.get_model_name()
            result = detector.detect(video_path)
            
            votes[model_name] = {
                'has_subtitles': result['has_subtitles'],
                'confidence': result['confidence'],
                'weight': detector.get_weight(),
                'metadata': result['metadata']
            }
        
        # 2. VotaÃ§Ã£o
        if self.voting_method == 'weighted':
            final_result = self._weighted_voting(votes)
        elif self.voting_method == 'majority':
            final_result = self._majority_voting(votes)
        else:
            raise ValueError(f"Unknown voting method: {self.voting_method}")
        
        # 3. Adicionar metadata
        final_result['votes'] = votes
        final_result['metadata']['ensemble_method'] = self.voting_method
        
        return final_result
    
    def _weighted_voting(self, votes: Dict) -> Dict:
        """VotaÃ§Ã£o ponderada por peso."""
        weighted_score = 0.0
        total_weight = 0.0
        
        for model_name, vote in votes.items():
            if vote['has_subtitles']:
                weighted_score += vote['confidence'] * vote['weight']
            total_weight += vote['weight']
        
        final_confidence = weighted_score / total_weight if total_weight > 0 else 0.0
        final_decision = final_confidence >= 0.5
        
        return {
            'has_subtitles': final_decision,
            'confidence': final_confidence,
            'metadata': {'voting_type': 'weighted'}
        }
    
    def _majority_voting(self, votes: Dict) -> Dict:
        """VotaÃ§Ã£o por maioria simples."""
        yes_votes = sum(1 for v in votes.values() if v['has_subtitles'])
        total_votes = len(votes)
        
        final_decision = yes_votes > (total_votes / 2)
        final_confidence = yes_votes / total_votes if final_decision else \
                          (total_votes - yes_votes) / total_votes
        
        return {
            'has_subtitles': final_decision,
            'confidence': final_confidence,
            'metadata': {
                'voting_type': 'majority',
                'votes_distribution': f'{yes_votes}/{total_votes}'
            }
        }
```

---

## 6ï¸âƒ£ Testes Esperados

### Estrutura de Testes

```python
# tests/test_sprint06_ensemble.py
import pytest
from app.video_processing.ensemble_detector import EnsembleSubtitleDetector
from app.video_processing.detectors import PaddleDetector, CLIPClassifier, CRAFTDetector

class TestSprint06Ensemble:
    
    @pytest.fixture
    def ensemble_detector(self):
        """Ensemble com 3 detectores."""
        return EnsembleSubtitleDetector()
    
    @pytest.fixture
    def video_with_subs(self):
        """VÃ­deo com legendas (do dataset Sprint 00)."""
        return "storage/validation/base/video_with_subs_1.mp4"
    
    @pytest.fixture
    def video_without_subs(self):
        """VÃ­deo sem legendas."""
        return "storage/validation/base/video_without_subs_1.mp4"
    
    # ========== TESTES INDIVIDUAIS ==========
    
    def test_paddle_detector_individual(self, video_with_subs, video_without_subs):
        """Test 1: PaddleDetector standalone."""
        detector = PaddleDetector(roi_mode='multi')
        
        # WITH subs
        result = detector.detect(video_with_subs)
        assert result['has_subtitles'] == True
        assert result['confidence'] > 0.8
        assert result['metadata']['model'] == 'paddleocr'
        
        # WITHOUT subs
        result = detector.detect(video_without_subs)
        assert result['has_subtitles'] == False
    
    def test_clip_classifier_individual(self, video_with_subs, video_without_subs):
        """Test 2: CLIPClassifier standalone."""
        classifier = CLIPClassifier()
        
        # WITH subs
        result = classifier.detect(video_with_subs)
        assert result['has_subtitles'] == True
        assert result['confidence'] > 0.5
        assert result['metadata']['model'] == 'clip'
        
        # WITHOUT subs
        result = classifier.detect(video_without_subs)
        assert result['has_subtitles'] == False
    
    def test_craft_detector_individual(self, video_with_subs, video_without_subs):
        """Test 3: CRAFTDetector standalone."""
        detector = CRAFTDetector()
        
        # WITH subs
        result = detector.detect(video_with_subs)
        assert result['has_subtitles'] == True
        assert result['metadata']['model'] == 'craft'
        
        # WITHOUT subs
        result = detector.detect(video_without_subs)
        assert result['has_subtitles'] == False
    
    # ========== TESTES DE ENSEMBLE ==========
    
    def test_ensemble_weighted_voting(self, ensemble_detector, video_with_subs):
        """Test 4: Ensemble com votaÃ§Ã£o ponderada."""
        result = ensemble_detector.detect(video_with_subs)
        
        assert 'has_subtitles' in result
        assert 'confidence' in result
        assert 'votes' in result
        
        # Verificar que todos os modelos votaram
        assert 'paddle' in result['votes']
        assert 'clip' in result['votes']
        assert 'craft' in result['votes']
        
        # DecisÃ£o deve ser True (vÃ­deo tem legendas)
        assert result['has_subtitles'] == True
        assert result['confidence'] > 0.7
    
    def test_ensemble_consensus(self, ensemble_detector):
        """Test 5: Ensemble em consenso (3/3 concordam)."""
        # Usar vÃ­deo Ã³bvio com legendas
        video = "storage/validation/base/video_with_subs_obvious.mp4"
        result = ensemble_detector.detect(video)
        
        # Todos devem concordar
        votes = result['votes']
        yes_votes = sum(1 for v in votes.values() if v['has_subtitles'])
        
        assert yes_votes == 3  # Consenso total
        assert result['has_subtitles'] == True
        assert result['confidence'] > 0.85
    
    def test_ensemble_disagreement(self, ensemble_detector):
        """Test 6: Ensemble em desacordo (2/3 vs 1/3)."""
        # Usar vÃ­deo ambÃ­guo (com texto, mas nÃ£o legenda tÃ­pica)
        video = "storage/validation/edge_cases/center/video_with_center_text_2.mp4"
        result = ensemble_detector.detect(video)
        
        # Verificar que houve divergÃªncia
        votes = result['votes']
        yes_votes = sum(1 for v in votes.values() if v['has_subtitles'])
        
        assert yes_votes in [1, 2]  # NÃ£o-consenso
        
        # DecisÃ£o final deve seguir votaÃ§Ã£o ponderada
        assert 'confidence' in result
        assert 0.3 < result['confidence'] < 0.7  # IndecisÃ£o
    
    def test_ensemble_vs_paddle_alone(self, video_with_subs):
        """Test 7: Comparar ensemble vs. PaddleOCR alone."""
        # PaddleOCR alone
        paddle = PaddleDetector(roi_mode='multi')
        paddle_result = paddle.detect(video_with_subs)
        
        # Ensemble
        ensemble = EnsembleSubtitleDetector()
        ensemble_result = ensemble.detect(video_with_subs)
        
        # Ambos devem detectar, mas ensemble pode ter confidence diferente
        assert paddle_result['has_subtitles'] == ensemble_result['has_subtitles']
        
        # Ensemble geralmente tem confidence mais calibrado
        print(f"Paddle confidence: {paddle_result['confidence']:.3f}")
        print(f"Ensemble confidence: {ensemble_result['confidence']:.3f}")
    
    # ========== TESTES DE DATASET COMPLETO ==========
    
    def test_ensemble_on_full_dataset(self, ensemble_detector):
        """Test 8: Ensemble em todos os 83 vÃ­deos de teste."""
        import glob
        
        # Carregar ground truth
        with open('storage/validation/ground_truth.json', 'r') as f:
            ground_truth = json.load(f)
        
        results = []
        for video_path, expected in ground_truth.items():
            result = ensemble_detector.detect(video_path)
            results.append({
                'video': video_path,
                'expected': expected,
                'predicted': result['has_subtitles'],
                'confidence': result['confidence'],
                'correct': result['has_subtitles'] == expected
            })
        
        # Calcular mÃ©tricas
        accuracy = sum(1 for r in results if r['correct']) / len(results)
        
        # Ensemble deve ter accuracy >= 95%
        assert accuracy >= 0.95, f"Ensemble accuracy: {accuracy:.2%} (expected â‰¥95%)"
        
        # Contar erros
        errors = [r for r in results if not r['correct']]
        print(f"\nEnsemble accuracy: {accuracy:.2%}")
        print(f"Errors: {len(errors)}/{len(results)}")
        for err in errors:
            print(f"  - {err['video']}: expected={err['expected']}, got={err['predicted']}")
    
    # ========== TESTES DE PERFORMANCE ==========
    
    def test_ensemble_performance(self, ensemble_detector, video_with_subs):
        """Test 9: Ensemble performance (<15s por vÃ­deo)."""
        import time
        
        start = time.time()
        result = ensemble_detector.detect(video_with_subs)
        elapsed = time.time() - start
        
        # Ensemble deve ser < 15s (3 modelos Ã— ~5s cada)
        assert elapsed < 15.0, f"Ensemble too slow: {elapsed:.2f}s (expected <15s)"
        
        print(f"\nEnsemble time: {elapsed:.2f}s")
    
    # ========== TESTES DE ROBUSTEZ ==========
    
    def test_ensemble_on_edge_cases(self, ensemble_detector):
        """Test 10: Ensemble em edge cases (Sprint 04)."""
        edge_case_videos = [
            "storage/validation/edge_cases/top/video_with_top_subs_1.mp4",
            "storage/validation/edge_cases/left/video_with_left_text_1.mp4",
            "storage/validation/edge_cases/right/video_with_right_text_1.mp4",
            "storage/validation/edge_cases/center/video_with_center_text_1.mp4"
        ]
        
        for video in edge_case_videos:
            result = ensemble_detector.detect(video)
            
            # Ensemble deve detectar todas as posiÃ§Ãµes
            assert result['has_subtitles'] == True
            assert result['confidence'] > 0.6
            
            print(f"{video}: {result['has_subtitles']} (conf: {result['confidence']:.2f})")
```

**Expected Test Results:**

```
Sprint 06 Tests: 10/10 PASSED
â”œâ”€ test_paddle_detector_individual: PASSED
â”œâ”€ test_clip_classifier_individual: PASSED
â”œâ”€ test_craft_detector_individual: PASSED
â”œâ”€ test_ensemble_weighted_voting: PASSED
â”œâ”€ test_ensemble_consensus: PASSED
â”œâ”€ test_ensemble_disagreement: PASSED
â”œâ”€ test_ensemble_vs_paddle_alone: PASSED
â”œâ”€ test_ensemble_on_full_dataset: PASSED (accuracy â‰¥95%)
â”œâ”€ test_ensemble_performance: PASSED (<15s)
â””â”€ test_ensemble_on_edge_cases: PASSED

Total: 46/47 tests PASSED (Sprint 00-06)
Run time: ~180s (3 min)
```

---

## 7ï¸âƒ£ IntegraÃ§Ã£o com Sprints Anteriores

### Sprint 00-02: Preprocessamento âœ… MANTIDO
- Resize (Sprint 01) ainda aplicado antes de todos os detectores
- CLAHE (Sprint 02) melhora qualidade para OCR e text detection
- Nenhuma mudanÃ§a necessÃ¡ria

### Sprint 03: Features âš ï¸ OPCIONAL AGORA
- Features visuais NÃƒO sÃ£o mais usadas para classificaÃ§Ã£o
- Ensemble usa modelos prÃ©-treinados (nÃ£o features manuais)
- **MAS**: Features ainda Ãºteis para:
  - AnÃ¡lise e debugging
  - Metadata enriquecida
  - PossÃ­vel fallback ou filtro pÃ³s-ensemble
- **Status**: Manter Sprint 03 como OPCIONAL, nÃ£o remover

### Sprint 04: Multi-ROI âœ… INTEGRADO
- PaddleDetector no ensemble usa Multi-ROI (roi_mode='multi')
- Multi-ROI melhora performance do componente PaddleOCR
- 100% compatÃ­vel com ensemble

### Sprint 05: Temporal Aggregation âœ… MANTIDO
- Ainda serÃ¡ implementado (Ãºtil para todos os detectores)
- Pode melhorar confidence de CLIP e CRAFT tambÃ©m
- Nenhuma mudanÃ§a necessÃ¡ria

---

## ðŸ“ˆ Expected Results

### Accuracy Comparison

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Method               â”‚ Accuracy â”‚ Precision   â”‚ Recall â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Paddle alone (Sprint â”‚  100.0%  â”‚    100.0%   â”‚ 100.0% â”‚
â”‚ 04)                  â”‚ (83/83)  â”‚             â”‚        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CLIP alone           â”‚   85-90% â”‚    80-85%   â”‚ 90-95% â”‚
â”‚                      â”‚          â”‚             â”‚        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CRAFT alone          â”‚   80-85% â”‚    75-80%   â”‚ 85-90% â”‚
â”‚                      â”‚          â”‚             â”‚        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ENSEMBLE (3 models)  â”‚   95-98% â”‚    95-97%   â”‚ 96-99% â”‚
â”‚                      â”‚          â”‚             â”‚        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Goal: â‰¥95% accuracy on full dataset (200+ videos quando expandir)
```

### Performance

```
Single model:
- PaddleOCR: 3-8s per video
- CLIP: 2-5s per video (with GPU)
- CRAFT: 4-10s per video (with GPU)

Ensemble (parallel):
- Sequential: ~15-20s (soma dos 3)
- Parallel (futuro): ~8-10s (max dos 3)

Goal: <15s per video for ensemble
```

---

## ðŸš€ Next Steps (Sprint 07)

ApÃ³s Sprint 06, implementar:

**Sprint 07: Ensemble Voting & Confidence Aggregation**
- Implementar votaÃ§Ã£o avanÃ§ada (nÃ£o sÃ³ weighted)
- DetecÃ§Ã£o de conflitos (quando modelos discordam muito)
- CalibraÃ§Ã£o de confidence (Platt scaling por modelo)
- Ajuste dinÃ¢mico de pesos baseado em performance
- A/B testing framework

---

## ðŸ“ Acceptance Criteria

- âœ… 3 detectores implementados (Paddle, CLIP, CRAFT)
- âœ… Ensemble system com votaÃ§Ã£o ponderada
- âœ… 10 testes de pytest (individuais + ensemble)
- âœ… Accuracy â‰¥95% no dataset completo (83+ vÃ­deos)
- âœ… Performance <15s por vÃ­deo
- âœ… 100% backward compatible (Sprint 00-05 mantidos)
- âœ… Zero manual labeling (all plug and play)
- âœ… DocumentaÃ§Ã£o completa (README + docstrings)

---

## âš ï¸ Dependencies

### External Packages

```bash
# requirements.txt additions
transformers==4.36.0      # CLIP
torch==2.1.0              # CLIP backend
pillow==10.1.0            # Image processing
craft-text-detector==0.4.3  # CRAFT
easyocr==1.7.0            # EasyOCR (optional)
```

### Model Downloads (Auto)

- CLIP: `~/.cache/huggingface/` (~600MB)
- CRAFT: `~/.craft_text_detector/` (~150MB)
- EasyOCR: `~/.EasyOCR/model/` (~150MB per language)

**Total**: ~900MB-1.2GB additional storage

---

## ðŸŽ¯ Success Metrics

```python
success_criteria = {
    'accuracy': 'â‰¥95% on 83 test videos',
    'precision': 'â‰¥95%',
    'recall': 'â‰¥96%',
    'performance': '<15s per video',
    'code_coverage': 'â‰¥90%',
    'backward_compatible': 'Sprint 00-05 tests still passing',
    'manual_work': '0 hours (100% automated)',
    'implementation_time': '4-6 hours'
}
```

---

**Status**: ðŸŸ¢ Ready to implement  
**Blocker**: None (Sprint 00-04 complete)  
**Next Sprint**: Sprint 07 (Voting & Confidence)
