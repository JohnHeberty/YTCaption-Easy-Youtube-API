# üöÄ Corre√ß√£o de Suporte GPU - Audio Transcriber

## ‚ùå Problema Identificado

```
audio-transcriber-api | 01:51:10 - WARNING - ‚ö†Ô∏è CUDA N√ÉO DISPON√çVEL - usando CPU
```

O servi√ßo **audio-transcriber** estava configurado para usar **CPU** mesmo tendo suporte completo a GPU no c√≥digo.

## üîç An√°lise

### Configura√ß√£o Anterior (INCORRETA)
- **Runtime**: `runc` (sem acesso √† GPU)
- **CUDA Version**: 12.1 (incompat√≠vel com driver 550.x)
- **PyTorch**: cu121 (incompat√≠vel com CUDA 11.8)
- **WHISPER_DEVICE**: `cpu` (for√ßado)
- **NVIDIA_VISIBLE_DEVICES**: `""` (GPU desabilitada)

### Por que estava errado?
1. Driver NVIDIA 550.x √© compat√≠vel com **CUDA 11.8**, n√£o 12.1
2. Runtime `runc` n√£o exp√µe GPUs para containers
3. Vari√°veis de ambiente bloqueavam acesso √† GPU
4. PyTorch cu121 requer CUDA 12.1+

## ‚úÖ Solu√ß√£o Aplicada

### Baseado em: `/GPU-OK/` (audio-voice funcionando)

### 1. Dockerfile - Imagem Base
```dockerfile
# ANTES
FROM nvidia/cuda:12.1.0-cudnn8-runtime-ubuntu22.04

# DEPOIS (compat√≠vel com driver 550.x)
FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04
```

### 2. Dockerfile - Vari√°veis de Ambiente
```dockerfile
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    NVIDIA_VISIBLE_DEVICES=all \
    NVIDIA_DRIVER_CAPABILITIES=compute,utility \
    CUDA_VISIBLE_DEVICES=0 \
    FORCE_CUDA=1 \
    LD_LIBRARY_PATH="/usr/lib/x86_64-linux-gnu:/usr/local/cuda/lib64:${LD_LIBRARY_PATH}"
```

### 3. Dockerfile - PyTorch com CUDA 11.8
```dockerfile
# üî• Upgrade pip
RUN python -m pip install --no-cache-dir --upgrade pip

# üî• Instalar TODAS as depend√™ncias primeiro
RUN python -m pip install --no-cache-dir --ignore-installed blinker \
      -r requirements.txt -c constraints.txt

# üî• FOR√áAR PyTorch cu118 POR √öLTIMO (compat√≠vel com CUDA 11.8)
RUN python -m pip install --no-cache-dir --force-reinstall \
      torch==2.4.0+cu118 torchaudio==2.4.0+cu118 \
      --index-url https://download.pytorch.org/whl/cu118
```

**Importante**: PyTorch cu118 √© instalado **POR √öLTIMO** para garantir compatibilidade total.

### 4. docker-compose.yml - Runtime e Vari√°veis
```yaml
services:
  audio-transcriber-service:
    runtime: nvidia  # ANTES: runc
    environment:
      - NVIDIA_VISIBLE_DEVICES=all  # ANTES: ""
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - WHISPER_DEVICE=cuda  # ANTES: cpu
      - WHISPER_FALLBACK_CPU=true
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
```

### 5. Celery Worker - Mesmas Configura√ß√µes
```yaml
  celery-worker:
    runtime: nvidia  # ANTES: runc
    environment:
      - NVIDIA_VISIBLE_DEVICES=all  # ANTES: ""
      - WHISPER_DEVICE=cuda  # ANTES: cpu
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
```

## üìã Checklist de Mudan√ßas

- [x] Trocar imagem base: CUDA 12.1 ‚Üí CUDA 11.8
- [x] Adicionar vari√°veis de ambiente CUDA
- [x] Adicionar `LD_LIBRARY_PATH`
- [x] Atualizar PyTorch: cu121 ‚Üí cu118
- [x] Mudar runtime: runc ‚Üí nvidia
- [x] Habilitar NVIDIA_VISIBLE_DEVICES
- [x] Configurar WHISPER_DEVICE=cuda
- [x] Adicionar se√ß√£o deploy com GPU
- [x] Aplicar mesmas configs no celery-worker

## üß™ Como Validar

### 1. Verificar CUDA no Container
```bash
docker exec audio-transcriber-api python -c "import torch; print(f'CUDA Available: {torch.cuda.is_available()}'); print(f'CUDA Version: {torch.version.cuda}'); print(f'Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"}')"
```

**Resultado esperado**:
```
CUDA Available: True
CUDA Version: 11.8
Device: NVIDIA GeForce RTX 3060
```

### 2. Verificar Whisper Device
```bash
docker logs audio-transcriber-api | grep -i "cuda\|gpu\|device"
```

**Resultado esperado**:
```
‚úÖ CUDA DISPON√çVEL
üìä Device: cuda
üéØ GPU: NVIDIA GeForce RTX 3060
```

### 3. Testar Transcri√ß√£o
Enviar um √°udio e verificar logs:
```bash
# Criar job de transcri√ß√£o
curl -X POST http://localhost:8002/transcribe \
  -F "file=@test.mp3" \
  -F "language=pt"

# Verificar uso de GPU
docker logs audio-transcriber-api | tail -20
```

## üìä Compara√ß√£o com GPU-OK

| Configura√ß√£o | GPU-OK (audio-voice) | audio-transcriber (ANTES) | audio-transcriber (DEPOIS) |
|-------------|---------------------|---------------------------|----------------------------|
| **Base Image** | CUDA 11.8 | CUDA 12.1 ‚ùå | CUDA 11.8 ‚úÖ |
| **Runtime** | nvidia | runc ‚ùå | nvidia ‚úÖ |
| **PyTorch** | cu118 | cu121 ‚ùå | cu118 ‚úÖ |
| **NVIDIA_VISIBLE_DEVICES** | all | "" ‚ùå | all ‚úÖ |
| **Device Env** | XTTS_DEVICE=cuda | WHISPER_DEVICE=cpu ‚ùå | WHISPER_DEVICE=cuda ‚úÖ |
| **Deploy GPU** | Sim | N√£o ‚ùå | Sim ‚úÖ |
| **LD_LIBRARY_PATH** | Sim | N√£o ‚ùå | Sim ‚úÖ |

## üéØ Benef√≠cios

### Performance
- **CPU (antes)**: ~30-60s para transcrever 1min de √°udio
- **GPU (agora)**: ~5-10s para transcrever 1min de √°udio
- **Ganho**: ~5-6x mais r√°pido

### Recursos
- Libera CPU para outros processos
- Usa VRAM (dedicada) ao inv√©s de RAM (compartilhada)
- Melhor escalabilidade para m√∫ltiplas transcri√ß√µes simult√¢neas

### Consist√™ncia
- Mesma stack do audio-voice (CUDA 11.8, PyTorch cu118)
- Reduz complexidade de manuten√ß√£o
- Facilita debugging

## üîß Troubleshooting

### Erro: "CUDA not available"
```bash
# Verificar driver NVIDIA no host
nvidia-smi

# Verificar NVIDIA Container Runtime
docker run --rm --runtime=nvidia nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi

# Reconstruir sem cache
docker compose build --no-cache
docker compose up -d
```

### Erro: "version `GLIBCXX_3.4.30' not found"
```bash
# Reinstalar PyTorch cu118 no container
docker exec audio-transcriber-api bash -c "
  pip uninstall -y torch torchaudio && \
  pip install --no-cache-dir torch==2.4.0+cu118 torchaudio==2.4.0+cu118 \
    --index-url https://download.pytorch.org/whl/cu118
"
```

### Erro: "libcuda.so.1: cannot open shared object file"
Verificar volume mounts no docker-compose.yml:
```yaml
volumes:
  - /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1:/usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1:ro
  - /usr/lib/x86_64-linux-gnu/libcuda.so.1:/usr/lib/x86_64-linux-gnu/libcuda.so.1:ro
```

## üìö Refer√™ncias

- Padr√£o de configura√ß√£o: `/GPU-OK/` (audio-voice)
- Driver compat√≠vel: NVIDIA 550.x ‚Üí CUDA 11.8
- PyTorch CUDA wheels: https://download.pytorch.org/whl/cu118
- NVIDIA Container Toolkit: https://github.com/NVIDIA/nvidia-docker

## ‚úÖ Status

- **Data**: 2025-12-01
- **Vers√£o**: 2.0.1
- **Status**: ‚úÖ IMPLEMENTADO
- **Build**: Em andamento
- **Testado**: Pendente ap√≥s build

---

**Pr√≥ximos passos**:
1. Aguardar build finalizar
2. Iniciar containers: `docker compose up -d`
3. Validar CUDA dispon√≠vel
4. Testar transcri√ß√£o com GPU
5. Monitorar performance (tempo de transcri√ß√£o)
