"""
Sistema de limpeza de jobs √≥rf√£os e gerenciamento de Dead Letter Queue.
Implementa resili√™ncia autom√°tica e recupera√ß√£o de falhas.
"""
import logging
import asyncio
from datetime import datetime, timedelta
from typing import List, Dict, Any
from celery import Celery

from ..domain.interfaces import IJobStore
from ..domain.models import Job, JobStatus

logger = logging.getLogger(__name__)


class OrphanJobCleaner:
    """
    Limpa jobs √≥rf√£os automaticamente.
    
    Jobs √≥rf√£os s√£o aqueles que:
    - Est√£o em PROCESSING h√° muito tempo (> timeout)
    - Est√£o em QUEUED mas nunca foram processados
    - Tem worker que morreu durante processamento
    """
    
    def __init__(
        self,
        job_store: IJobStore,
        processing_timeout_minutes: int = 10,
        queued_timeout_minutes: int = 30
    ):
        """
        Args:
            job_store: Store de jobs
            processing_timeout_minutes: Timeout para jobs em PROCESSING
            queued_timeout_minutes: Timeout para jobs em QUEUED
        """
        self.job_store = job_store
        self.processing_timeout = timedelta(minutes=processing_timeout_minutes)
        self.queued_timeout = timedelta(minutes=queued_timeout_minutes)
    
    async def cleanup_orphans(self) -> Dict[str, Any]:
        """
        Executa limpeza de jobs √≥rf√£os.
        
        Returns:
            Dict com estat√≠sticas da limpeza
        """
        logger.info("üßπ Iniciando limpeza de jobs √≥rf√£os...")
        
        stats = {
            "checked": 0,
            "orphans_found": 0,
            "requeued": 0,
            "failed": 0,
            "errors": []
        }
        
        try:
            # Busca jobs em PROCESSING
            processing_jobs = self.job_store.list_jobs(status=JobStatus.PROCESSING)
            stats["checked"] += len(processing_jobs)
            
            for job in processing_jobs:
                try:
                    if self._is_orphan_processing(job):
                        stats["orphans_found"] += 1
                        await self._handle_orphan_job(job, stats)
                except Exception as e:
                    logger.error(f"Erro ao processar job √≥rf√£o {job.id}: {e}")
                    stats["errors"].append(f"{job.id}: {str(e)}")
            
            # Busca jobs em QUEUED muito antigos
            queued_jobs = self.job_store.list_jobs(status=JobStatus.QUEUED)
            stats["checked"] += len(queued_jobs)
            
            for job in queued_jobs:
                try:
                    if self._is_orphan_queued(job):
                        stats["orphans_found"] += 1
                        await self._handle_stale_queued_job(job, stats)
                except Exception as e:
                    logger.error(f"Erro ao processar job enfileirado √≥rf√£o {job.id}: {e}")
                    stats["errors"].append(f"{job.id}: {str(e)}")
            
            logger.info(
                f"‚úÖ Limpeza conclu√≠da: {stats['orphans_found']} √≥rf√£os encontrados, "
                f"{stats['requeued']} reenfileirados, {stats['failed']} falhados"
            )
            
        except Exception as e:
            logger.error(f"‚ùå Erro durante limpeza de √≥rf√£os: {e}")
            stats["errors"].append(f"cleanup_error: {str(e)}")
        
        return stats
    
    def _is_orphan_processing(self, job: Job) -> bool:
        """Verifica se job em PROCESSING √© √≥rf√£o"""
        if not job.started_at:
            # Job em PROCESSING sem started_at √© definitivamente √≥rf√£o
            return True
        
        age = datetime.now() - job.started_at
        return age > self.processing_timeout
    
    def _is_orphan_queued(self, job: Job) -> bool:
        """Verifica se job em QUEUED √© √≥rf√£o"""
        age = datetime.now() - job.created_at
        return age > self.queued_timeout
    
    async def _handle_orphan_job(self, job: Job, stats: Dict) -> None:
        """
        Trata job √≥rf√£o em PROCESSING.
        
        Estrat√©gia:
        - Se retry_count < max_retries: reenfileira
        - Sen√£o: marca como FAILED
        """
        retry_count = getattr(job, 'retry_count', 0)
        max_retries = 2
        
        if retry_count < max_retries:
            # Reenfileira job
            job.status = JobStatus.QUEUED
            job.retry_count = retry_count + 1
            job.error_message = f"Job √≥rf√£o detectado e reenfileirado (tentativa {retry_count + 1})"
            job.progress = 0.0
            job.started_at = None
            
            self.job_store.update_job(job)
            stats["requeued"] += 1
            
            logger.warning(f"‚ôªÔ∏è Job √≥rf√£o {job.id} reenfileirado (tentativa {retry_count + 1})")
        else:
            # Excedeu retries, marca como falho
            job.status = JobStatus.FAILED
            job.error_message = f"Job √≥rf√£o ap√≥s {retry_count} tentativas. Poss√≠vel crash de worker ou timeout."
            job.progress = 0.0
            
            self.job_store.update_job(job)
            stats["failed"] += 1
            
            logger.error(f"‚ùå Job √≥rf√£o {job.id} marcado como FAILED ap√≥s {retry_count} tentativas")
    
    async def _handle_stale_queued_job(self, job: Job, stats: Dict) -> None:
        """Trata job muito antigo em QUEUED"""
        age = datetime.now() - job.created_at
        
        job.status = JobStatus.FAILED
        job.error_message = f"Job permaneceu em fila por {age} sem ser processado. Poss√≠vel problema no worker."
        job.progress = 0.0
        
        self.job_store.update_job(job)
        stats["failed"] += 1
        
        logger.error(f"‚ùå Job em fila √≥rf√£o {job.id} marcado como FAILED (idade: {age})")


class DeadLetterQueueManager:
    """
    Gerencia Dead Letter Queue (DLQ) para jobs que falharam permanentemente.
    """
    
    def __init__(self, job_store: IJobStore):
        self.job_store = job_store
        self.dlq_prefix = "dlq:"
    
    def send_to_dlq(self, job: Job, reason: str) -> None:
        """
        Envia job para Dead Letter Queue.
        
        Args:
            job: Job que falhou
            reason: Motivo da falha permanente
        """
        try:
            job.status = JobStatus.FAILED
            job.error_message = f"[DLQ] {reason}"
            job.dlq_at = datetime.now()
            
            # Salva com prefixo DLQ
            dlq_job_id = f"{self.dlq_prefix}{job.id}"
            
            # TODO: Implementar store separado para DLQ
            # Por enquanto, usa mesmo store com prefixo
            self.job_store.save_job(job)
            
            logger.warning(f"üì™ Job {job.id} enviado para DLQ: {reason}")
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao enviar job {job.id} para DLQ: {e}")
    
    def list_dlq_jobs(self, limit: int = 100) -> List[Job]:
        """
        Lista jobs na Dead Letter Queue.
        
        Args:
            limit: N√∫mero m√°ximo de jobs a retornar
        
        Returns:
            Lista de jobs na DLQ
        """
        try:
            # TODO: Implementar busca espec√≠fica de DLQ
            failed_jobs = self.job_store.list_jobs(status=JobStatus.FAILED)
            return failed_jobs[:limit]
        except Exception as e:
            logger.error(f"‚ùå Erro ao listar DLQ: {e}")
            return []
    
    def retry_dlq_job(self, job_id: str) -> bool:
        """
        Retenta processar job da DLQ.
        
        Args:
            job_id: ID do job na DLQ
        
        Returns:
            True se job foi reenfileirado
        """
        try:
            job = self.job_store.get_job(job_id)
            if not job:
                logger.error(f"Job {job_id} n√£o encontrado na DLQ")
                return False
            
            # Reseta job para QUEUED
            job.status = JobStatus.QUEUED
            job.error_message = None
            job.retry_count = getattr(job, 'retry_count', 0) + 1
            job.progress = 0.0
            
            self.job_store.update_job(job)
            
            logger.info(f"‚ôªÔ∏è Job {job_id} da DLQ reenfileirado manualmente")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao retentar job {job_id} da DLQ: {e}")
            return False


async def run_orphan_cleanup_loop(
    job_store: IJobStore,
    interval_minutes: int = 5,
    celery_app: Celery = None
):
    """
    Loop cont√≠nuo de limpeza de √≥rf√£os (para rodar como background task).
    
    Args:
        job_store: Store de jobs
        interval_minutes: Intervalo entre limpezas
        celery_app: App Celery (opcional, para agendar com Beat)
    """
    cleaner = OrphanJobCleaner(job_store)
    
    logger.info(f"üßπ Iniciando loop de limpeza de √≥rf√£os (intervalo: {interval_minutes}min)")
    
    while True:
        try:
            stats = await cleaner.cleanup_orphans()
            
            # Log estat√≠sticas
            if stats["orphans_found"] > 0:
                logger.warning(
                    f"‚ö†Ô∏è √ìrf√£os detectados: {stats['orphans_found']} "
                    f"(reenfileirados: {stats['requeued']}, falhados: {stats['failed']})"
                )
            
        except Exception as e:
            logger.error(f"‚ùå Erro no loop de limpeza de √≥rf√£os: {e}")
        
        # Aguarda pr√≥xima itera√ß√£o
        await asyncio.sleep(interval_minutes * 60)
