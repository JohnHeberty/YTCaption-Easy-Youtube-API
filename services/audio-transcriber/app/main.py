import asyncio
import os
from datetime import datetime
from fastapi import FastAPI, HTTPException, UploadFile, File, BackgroundTasks, Request, Form
from fastapi.responses import FileResponse, JSONResponse
from pathlib import Path
from typing import List, Optional
import logging

from .models import Job, JobRequest, JobStatus, TranscriptionResponse
from .processor import TranscriptionProcessor
from .redis_store import RedisJobStore
from .logging_config import setup_logging, get_logger
from .exceptions import AudioTranscriptionException, ServiceException, exception_handler
from .config import get_settings, get_supported_languages, is_language_supported, get_whisper_models

# ConfiguraÃ§Ã£o de logging
settings = get_settings()
setup_logging("audio-transcriber", settings['log_level'])
logger = get_logger(__name__)

# InstÃ¢ncias globais
app = FastAPI(
    title="Audio Transcription Service",
    description="MicroserviÃ§o para transcriÃ§Ã£o de Ã¡udio com cache de 24h",
    version="2.0.0"
)

# Exception handlers
app.add_exception_handler(AudioTranscriptionException, exception_handler)
app.add_exception_handler(ServiceException, exception_handler)

# Usa Redis como store compartilhado
redis_url = os.getenv('REDIS_URL', 'redis://localhost:6379/0')
job_store = RedisJobStore(redis_url=redis_url)
processor = TranscriptionProcessor()

# Injeta referÃªncia do job_store no processor para updates de progresso
processor.job_store = job_store


@app.on_event("startup")
async def startup_event():
    """Inicializa sistema"""
    try:
        await job_store.start_cleanup_task()
        logger.info("Audio Transcription Service iniciado com sucesso")
    except Exception as e:
        logger.error(f"Erro durante inicializaÃ§Ã£o: {e}")
        raise


@app.on_event("shutdown") 
async def shutdown_event():
    """Para sistema"""
    try:
        await job_store.stop_cleanup_task()
        logger.info("Audio Transcription Service parado graciosamente")
    except Exception as e:
        logger.error(f"Erro durante shutdown: {e}")


def submit_processing_task(job: Job):
    """Submete job para processamento em background via Celery"""
    try:
        from .celery_config import celery_app
        from .celery_tasks import transcribe_audio_task
        
        # Envia job para o worker Celery
        task_result = transcribe_audio_task.apply_async(
            args=[job.model_dump()], 
            task_id=job.id  # Usa o job ID como task ID
        )
        logger.info(f"ðŸ“¤ Job {job.id} enviado para Celery worker: {task_result.id}")
        
    except Exception as e:
        logger.error(f"âŒ Erro ao enviar job {job.id} para Celery: {e}")
        logger.error(f"âŒ Fallback: processando diretamente job {job.id}")
        # Fallback para processamento direto se Celery falhar
        asyncio.create_task(processor.process_transcription_job(job))


@app.post("/jobs", response_model=Job)
async def create_transcription_job(
    request: Request,
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    language_in: str = Form("auto"),
    language_out: Optional[str] = Form(None)
) -> Job:
    """
    Cria um novo job de transcriÃ§Ã£o/traduÃ§Ã£o de Ã¡udio
    
    - **file**: Arquivo de Ã¡udio para transcrever
    - **language_in**: CÃ³digo do idioma de entrada (ISO 639-1) ou 'auto' para detecÃ§Ã£o automÃ¡tica.
                       Exemplos: 'pt' (portuguÃªs), 'en' (inglÃªs), 'es' (espanhol), 'auto' (detectar)
    - **language_out**: (Opcional) CÃ³digo do idioma de saÃ­da para traduÃ§Ã£o (ISO 639-1).
                        Se omitido, usa o mesmo idioma detectado/especificado em language_in.
                        Exemplos: 'pt', 'en', 'es', 'fr', etc.
    
    **Exemplos de uso**:
    - Transcrever em portuguÃªs: language_in='pt', language_out=None
    - Detectar idioma e transcrever: language_in='auto', language_out=None
    - Traduzir inglÃªs para portuguÃªs: language_in='en', language_out='pt'
    - Detectar e traduzir para inglÃªs: language_in='auto', language_out='en'
    
    Use GET /languages para ver todos os idiomas suportados.
    """
    try:
        # ValidaÃ§Ã£o de linguagem de entrada
        if not is_language_supported(language_in):
            supported = get_supported_languages()
            raise HTTPException(
                status_code=400,
                detail={
                    "error": "Linguagem de entrada nÃ£o suportada",
                    "language_provided": language_in,
                    "supported_languages": supported[:10],
                    "total_supported": len(supported),
                    "note": "Use GET /languages para ver todas as linguagens suportadas"
                }
            )
        
        # ValidaÃ§Ã£o de linguagem de saÃ­da (se fornecida)
        if language_out is not None:
            if not is_language_supported(language_out):
                supported = get_supported_languages()
                raise HTTPException(
                    status_code=400,
                    detail={
                        "error": "Linguagem de saÃ­da nÃ£o suportada",
                        "language_provided": language_out,
                        "supported_languages": supported[:10],
                        "total_supported": len(supported),
                        "note": "Use GET /languages para ver todas as linguagens suportadas"
                    }
                )
            
            # Aviso: language_out igual a language_in nÃ£o faz sentido (exceto se in='auto')
            if language_out == language_in and language_in != "auto":
                logger.warning(f"language_out='{language_out}' igual a language_in='{language_in}', traduÃ§Ã£o nÃ£o serÃ¡ aplicada")
        
        logger.info(f"Criando job: arquivo={file.filename}, language_in={language_in}, language_out={language_out or 'same'}")
        
        # Cria job para extrair ID
        new_job = Job.create_new(file.filename, "transcribe", language_in, language_out)
        
        # Verifica se jÃ¡ existe job com mesmo ID
        existing_job = job_store.get_job(new_job.id)
        
        if existing_job:
            # Job jÃ¡ existe - verifica status
            if existing_job.status == JobStatus.COMPLETED:
                logger.info(f"Job {new_job.id} jÃ¡ completado")
                return existing_job
            elif existing_job.status in [JobStatus.QUEUED, JobStatus.PROCESSING]:
                # ðŸ”§ CRÃTICO: Detecta jobs Ã³rfÃ£os (processando por muito tempo)
                from datetime import datetime, timedelta
                
                # Se job estÃ¡ processando hÃ¡ mais de 30 minutos, considera Ã³rfÃ£o
                processing_timeout = timedelta(minutes=30)
                job_age = datetime.now() - existing_job.created_at
                
                if job_age > processing_timeout:
                    logger.warning(f"âš ï¸ Job {new_job.id} Ã³rfÃ£o detectado (processando hÃ¡ {job_age}), reprocessando...")
                    existing_job.status = JobStatus.QUEUED
                    existing_job.error_message = f"Job Ã³rfÃ£o detectado apÃ³s {job_age}, reiniciando processamento"
                    existing_job.progress = 0.0
                    job_store.update_job(existing_job)
                    
                    # Submete para processamento novamente
                    submit_processing_task(existing_job)
                    return existing_job
                else:
                    logger.info(f"Job {new_job.id} jÃ¡ em processamento (idade: {job_age})")
                    return existing_job
            elif existing_job.status == JobStatus.FAILED:
                # Falhou antes - tenta novamente
                logger.info(f"Reprocessando job falhado: {new_job.id}")
                existing_job.status = JobStatus.QUEUED
                existing_job.error_message = None
                existing_job.progress = 0.0
                job_store.update_job(existing_job)
                
                # Submete para processamento
                submit_processing_task(existing_job)
                return existing_job
        
        # Job novo - salva arquivo
        upload_dir = Path("./uploads")
        upload_dir.mkdir(exist_ok=True)
        
        # Usar apenas job_id para evitar problemas com caracteres especiais
        original_extension = Path(file.filename).suffix if file.filename else ""
        file_path = upload_dir / f"{new_job.id}{original_extension}"
        with open(file_path, "wb") as f:
            content = await file.read()
            f.write(content)
        
        new_job.input_file = str(file_path)
        
        # Salva job e submete para processamento
        job_store.save_job(new_job)
        submit_processing_task(new_job)
        
        logger.info(f"Job de transcriÃ§Ã£o criado: {new_job.id}")
        return new_job
        
    except HTTPException:
        # HTTPException jÃ¡ tem mensagem apropriada, re-raise sem modificar
        raise
    except Exception as e:
        logger.error(f"Erro ao criar job de transcriÃ§Ã£o: {e}")
        if isinstance(e, (AudioTranscriptionException, ServiceException)):
            raise
        raise ServiceException(f"Erro interno ao processar arquivo: {str(e)}")


@app.get("/languages")
async def get_supported_languages_endpoint():
    """
    Retorna lista de linguagens suportadas pelo Whisper para transcriÃ§Ã£o e traduÃ§Ã£o.
    
    **TranscriÃ§Ã£o (language_in):**
    - **auto**: DetecÃ§Ã£o automÃ¡tica de idioma
    - CÃ³digos ISO 639-1 para idiomas especÃ­ficos (en, pt, es, fr, de, it, ja, ko, zh, etc.)
    - Suporta 99+ idiomas para transcriÃ§Ã£o no idioma original
    
    **TraduÃ§Ã£o (language_out):**
    - **en** (English): Ãšnico idioma suportado para traduÃ§Ã£o pelo Whisper
    - Whisper pode traduzir de qualquer idioma detectado para inglÃªs
    - TraduÃ§Ã£o para outros idiomas nÃ£o Ã© suportada nativamente
    
    **Exemplos de uso:**
    - Transcrever em portuguÃªs: `language_in='pt'`, `language_out=None`
    - Detectar e transcrever: `language_in='auto'`, `language_out=None`
    - Traduzir para inglÃªs: `language_in='auto'`, `language_out='en'`
    - Traduzir PTâ†’EN: `language_in='pt'`, `language_out='en'`
    """
    languages = get_supported_languages()
    models = get_whisper_models()
    
    return {
        "transcription": {
            "supported_languages": languages,
            "total_languages": len(languages),
            "default_language": settings.get("whisper_default_language", "auto"),
            "note": "Use 'auto' para detecÃ§Ã£o automÃ¡tica ou cÃ³digo ISO 639-1 especÃ­fico",
            "examples": ["auto", "pt", "en", "es", "fr", "de", "it", "ja", "ko", "zh"]
        },
        "translation": {
            "supported_languages": ["en"],
            "note": "Whisper sÃ³ suporta traduÃ§Ã£o para inglÃªs (en)",
            "limitation": "TraduÃ§Ã£o para outros idiomas requer ferramentas externas",
            "examples": ["en"]
        },
        "models": models,
        "usage_examples": {
            "transcribe_portuguese": {
                "language_in": "pt",
                "language_out": None,
                "description": "Transcreve Ã¡udio em portuguÃªs"
            },
            "auto_detect_transcribe": {
                "language_in": "auto",
                "language_out": None,
                "description": "Detecta idioma e transcreve no idioma original"
            },
            "translate_to_english": {
                "language_in": "auto",
                "language_out": "en",
                "description": "Detecta idioma e traduz para inglÃªs"
            },
            "translate_pt_to_en": {
                "language_in": "pt",
                "language_out": "en",
                "description": "Traduz Ã¡udio em portuguÃªs para inglÃªs"
            }
        }
    }


@app.get("/jobs/{job_id}", response_model=Job)
async def get_job_status(job_id: str) -> Job:
    """Consulta status de um job"""
    job = job_store.get_job(job_id)
    
    if not job:
        raise HTTPException(status_code=404, detail="Job nÃ£o encontrado")
    
    if job.is_expired:
        raise HTTPException(status_code=410, detail="Job expirado")
    
    return job


@app.get("/jobs/{job_id}/download")
async def download_file(job_id: str):
    """Faz download do arquivo de transcriÃ§Ã£o"""
    job = job_store.get_job(job_id)
    
    if not job:
        raise HTTPException(status_code=404, detail="Job nÃ£o encontrado")
    
    if job.is_expired:
        raise HTTPException(status_code=410, detail="Job expirado")
        
    if job.status != JobStatus.COMPLETED:
        raise HTTPException(
            status_code=425, 
            detail=f"TranscriÃ§Ã£o nÃ£o pronta. Status: {job.status}"
        )
    
    file_path = Path(job.output_file) if job.output_file else None
    if not file_path or not file_path.exists():
        raise HTTPException(status_code=404, detail="Arquivo nÃ£o encontrado")
    
    return FileResponse(
        path=file_path,
        filename=f"transcription_{job_id}.srt",
        media_type='text/plain'
    )


@app.get("/jobs/{job_id}/text")
async def get_transcription_text(job_id: str):
    """Retorna apenas o texto da transcriÃ§Ã£o"""
    job = job_store.get_job(job_id)
    
    if not job:
        raise HTTPException(status_code=404, detail="Job nÃ£o encontrado")
        
    if job.status != JobStatus.COMPLETED:
        raise HTTPException(
            status_code=425, 
            detail=f"TranscriÃ§Ã£o nÃ£o pronta. Status: {job.status}"
        )
    
    return {"text": job.transcription_text or ""}


@app.get("/jobs/{job_id}/transcription", response_model=TranscriptionResponse)
async def get_full_transcription(job_id: str) -> TranscriptionResponse:
    """
    Retorna transcriÃ§Ã£o completa com segments (start, end, duration).
    Formato compatÃ­vel com projeto v1.
    """
    job = job_store.get_job(job_id)
    
    if not job:
        raise HTTPException(status_code=404, detail="Job nÃ£o encontrado")
    
    if job.is_expired:
        raise HTTPException(status_code=410, detail="Job expirado")
        
    if job.status != JobStatus.COMPLETED:
        raise HTTPException(
            status_code=425, 
            detail=f"TranscriÃ§Ã£o nÃ£o pronta. Status: {job.status}"
        )
    
    if not job.transcription_segments:
        raise HTTPException(
            status_code=500, 
            detail="Segments nÃ£o disponÃ­veis para este job"
        )
    
    # Calcula duraÃ§Ã£o total
    duration = job.transcription_segments[-1].end if job.transcription_segments else 0.0
    
    # Calcula tempo de processamento
    processing_time = None
    if job.completed_at and job.created_at:
        processing_time = (job.completed_at - job.created_at).total_seconds()
    
    # Determina se houve traduÃ§Ã£o
    was_translated = job.language_out is not None and job.language_out != job.language_in
    
    return TranscriptionResponse(
        transcription_id=job.id,
        filename=job.filename or "unknown",
        language=job.language_detected or job.language_in,  # Prioriza idioma detectado
        language_detected=job.language_detected,
        language_out=job.language_out,
        was_translated=was_translated,
        full_text=job.transcription_text or "",
        segments=job.transcription_segments,
        total_segments=len(job.transcription_segments),
        duration=duration,
        processing_time=processing_time
    )


@app.get("/jobs", response_model=List[Job])
async def list_jobs(limit: int = 20) -> List[Job]:
    """Lista jobs recentes"""
    return job_store.list_jobs(limit)


@app.delete("/jobs/{job_id}")
async def delete_job(job_id: str):
    """
    Remove job e arquivos associados
    
    IMPORTANTE: Remove completamente o job do sistema:
    - Job do Redis
    - Arquivo de entrada (upload)
    - Arquivo de saÃ­da (transcriÃ§Ã£o)
    - Arquivos temporÃ¡rios
    """
    job = job_store.get_job(job_id)
    
    if not job:
        raise HTTPException(status_code=404, detail="Job nÃ£o encontrado")
    
    try:
        # Remove arquivos se existirem
        files_deleted = 0
        
        if job.input_file:
            input_path = Path(job.input_file)
            if input_path.exists():
                input_path.unlink()
                files_deleted += 1
                logger.info(f"ðŸ—‘ï¸ Arquivo de entrada removido: {input_path.name}")
        
        if job.output_file:
            output_path = Path(job.output_file)
            if output_path.exists():
                output_path.unlink()
                files_deleted += 1
                logger.info(f"ðŸ—‘ï¸ Arquivo de saÃ­da removido: {output_path.name}")
        
        # Remove job do Redis (CRÃTICO - estava faltando!)
        job_store.redis.delete(f"transcription_job:{job_id}")
        logger.info(f"ðŸ—‘ï¸ Job {job_id} removido do Redis")
        
        return {
            "message": "Job removido com sucesso",
            "job_id": job_id,
            "files_deleted": files_deleted
        }
        
    except Exception as e:
        logger.error(f"âŒ Erro ao remover job {job_id}: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Erro ao remover job: {str(e)}"
        )


async def _perform_basic_cleanup():
    """Executa limpeza BÃSICA: Remove apenas jobs expirados e arquivos Ã³rfÃ£os"""
    try:
        from datetime import timedelta
        report = {"jobs_removed": 0, "files_deleted": 0, "space_freed_mb": 0.0, "errors": []}
        logger.info("ðŸ§¹ Iniciando limpeza bÃ¡sica (jobs expirados)...")
        
        # Limpar jobs expirados
        try:
            keys = job_store.redis.keys("transcription_job:*")
            now = datetime.now()
            expired_count = 0
            for key in keys:
                job_data = job_store.redis.get(key)
                if job_data:
                    import json
                    try:
                        job = json.loads(job_data)
                        created_at = datetime.fromisoformat(job.get("created_at", ""))
                        if (now - created_at) > timedelta(hours=24):
                            job_store.redis.delete(key)
                            expired_count += 1
                    except:
                        pass
            report["jobs_removed"] = expired_count
            logger.info(f"ðŸ—‘ï¸  Redis: {expired_count} jobs expirados removidos")
        except Exception as e:
            logger.error(f"âŒ Erro ao limpar Redis: {e}")
            report["errors"].append(f"Redis: {str(e)}")
        
        # Limpar arquivos Ã³rfÃ£os (>24h)
        for dir_name, dir_key in [("uploads", "upload_dir"), ("transcriptions", "transcription_dir"), ("temp", "temp_dir")]:
            dir_path = Path(settings.get(dir_key, f'./{dir_name}'))
            if dir_path.exists():
                deleted_count = 0
                for file_path in dir_path.iterdir():
                    if not file_path.is_file():
                        continue
                    try:
                        age = datetime.now() - datetime.fromtimestamp(file_path.stat().st_mtime)
                        if age > timedelta(hours=24):
                            size_mb = file_path.stat().st_size / (1024 * 1024)
                            await asyncio.to_thread(file_path.unlink)
                            deleted_count += 1
                            report["space_freed_mb"] += size_mb
                    except Exception as e:
                        logger.error(f"âŒ Erro ao remover {file_path.name}: {e}")
                        report["errors"].append(f"{dir_name}/{file_path.name}: {str(e)}")
                report["files_deleted"] += deleted_count
                if deleted_count > 0:
                    logger.info(f"ðŸ—‘ï¸  {dir_name.capitalize()}: {deleted_count} arquivos Ã³rfÃ£os removidos")
        
        report["space_freed_mb"] = round(report["space_freed_mb"], 2)
        logger.info(f"âœ“ Limpeza bÃ¡sica: {report['jobs_removed']} jobs + {report['files_deleted']} arquivos ({report['space_freed_mb']}MB)")
        return report
    except Exception as e:
        logger.error(f"âŒ ERRO na limpeza bÃ¡sica: {e}")
        return {"error": str(e)}


async def _perform_cleanup(purge_celery_queue: bool = False):
    """
    Executa limpeza COMPLETA do sistema SÃNCRONAMENTE (sem background tasks)
    
    âš ï¸ CRÃTICO: Executa no handler HTTP para evitar ciclo vicioso onde
    o prÃ³prio job de limpeza seria deletado antes de terminar.
    
    ZERA ABSOLUTAMENTE TUDO:
    - TODO o banco Redis (FLUSHDB usando DIVISOR do .env)
    - TODOS os arquivos de uploads/
    - TODOS os arquivos de transcriptions/
    - TODOS os arquivos temporÃ¡rios
    - TODOS os modelos baixados em models/
    - (OPCIONAL) TODOS os jobs da fila Celery
    """
    try:
        from redis import Redis
        from urllib.parse import urlparse
        
        report = {
            "jobs_removed": 0,
            "redis_flushed": False,
            "files_deleted": 0,
            "space_freed_mb": 0.0,
            "models_deleted": 0,
            "celery_queue_purged": False,
            "celery_tasks_purged": 0,
            "errors": []
        }
        
        logger.warning("ðŸ”¥ INICIANDO LIMPEZA TOTAL DO SISTEMA - TUDO SERÃ REMOVIDO!")
        
        # 1. FLUSHDB NO REDIS (limpa TODO o banco de dados)
        try:
            # Extrai host, port e db do REDIS_URL (que usa DIVISOR)
            redis_url = job_store.redis.connection_pool.connection_kwargs.get('host') or 'localhost'
            redis_port = job_store.redis.connection_pool.connection_kwargs.get('port') or 6379
            redis_db = job_store.redis.connection_pool.connection_kwargs.get('db') or 0
            
            logger.warning(f"ðŸ”¥ Executando FLUSHDB no Redis {redis_url}:{redis_port} DB={redis_db}")
            
            # Conta jobs ANTES de limpar
            keys_before = job_store.redis.keys("transcription_job:*")
            report["jobs_removed"] = len(keys_before)
            
            # FLUSHDB - Remove TODO o conteÃºdo do banco atual
            job_store.redis.flushdb()
            report["redis_flushed"] = True
            
            logger.info(f"âœ… Redis FLUSHDB executado: {len(keys_before)} jobs + todas as outras keys removidas")
            
        except Exception as e:
            logger.error(f"âŒ Erro ao limpar Redis: {e}")
            report["errors"].append(f"Redis FLUSHDB: {str(e)}")
        
        # 2. LIMPAR FILA CELERY (SE SOLICITADO)
        if purge_celery_queue:
            try:
                from redis import Redis
                
                logger.warning("ðŸ”¥ Limpando fila Celery 'audio_transcriber_queue'...")
                
                # Conecta ao Redis Celery
                redis_celery = job_store.redis  # Usa o mesmo Redis do job_store
                
                # Nome da fila no Redis (Celery usa formato customizado ou default)
                queue_keys = [
                    "audio_transcriber_queue",          # Fila principal (nome customizado)
                    "audio_transcription_queue",        # PossÃ­vel nome alternativo
                    "celery",                           # Fila default do Celery
                    "_kombu.binding.audio_transcriber_queue",  # Bindings
                    "_kombu.binding.audio_transcription_queue",  # Bindings alternativo
                    "_kombu.binding.celery",            # Bindings default
                    "unacked",                          # Tasks nÃ£o reconhecidas
                    "unacked_index",                    # Ãndice de unacked
                ]
                
                tasks_purged = 0
                for queue_key in queue_keys:
                    # LLEN para verificar se existe (funciona apenas para listas)
                    try:
                        queue_len = redis_celery.llen(queue_key)
                        if queue_len > 0:
                            logger.info(f"   Fila '{queue_key}': {queue_len} tasks")
                            tasks_purged += queue_len
                    except:
                        pass  # NÃ£o Ã© uma lista, pode ser outro tipo de key
                    
                    # DELETE remove a key inteira (funciona para qualquer tipo)
                    deleted = redis_celery.delete(queue_key)
                    if deleted:
                        logger.info(f"   âœ“ Fila '{queue_key}' removida")
                
                # TambÃ©m remove keys de resultados e metadados Celery
                celery_result_keys = redis_celery.keys("celery-task-meta-*")
                if celery_result_keys:
                    redis_celery.delete(*celery_result_keys)
                    logger.info(f"   âœ“ {len(celery_result_keys)} resultados Celery removidos")
                
                report["celery_queue_purged"] = True
                report["celery_tasks_purged"] = tasks_purged
                logger.warning(f"ðŸ”¥ Fila Celery purgada: {tasks_purged} tasks removidas")
                
            except Exception as e:
                logger.error(f"âŒ Erro ao limpar fila Celery: {e}")
                report["errors"].append(f"Celery: {str(e)}")
        else:
            logger.info("â­ï¸  Fila Celery NÃƒO serÃ¡ limpa (purge_celery_queue=false)")
        
        # 3. LIMPAR TODOS OS ARQUIVOS DE UPLOADS
        upload_dir = Path(settings.get('upload_dir', './uploads'))
        if upload_dir.exists():
            deleted_count = 0
            for file_path in upload_dir.iterdir():
                if not file_path.is_file():
                    continue
                    
                try:
                    size_mb = file_path.stat().st_size / (1024 * 1024)
                    await asyncio.to_thread(file_path.unlink)
                    deleted_count += 1
                    report["space_freed_mb"] += size_mb
                except Exception as e:
                    logger.error(f"âŒ Erro ao remover upload {file_path.name}: {e}")
                    report["errors"].append(f"Upload/{file_path.name}: {str(e)}")
            
            report["files_deleted"] += deleted_count
            if deleted_count > 0:
                logger.info(f"ðŸ—‘ï¸  Uploads: {deleted_count} arquivos removidos")
            else:
                logger.info("âœ“ Uploads: nenhum arquivo encontrado")
        
        # 3. LIMPAR TODOS OS ARQUIVOS DE TRANSCRIPTIONS
        transcription_dir = Path(settings.get('transcription_dir', './transcriptions'))
        if transcription_dir.exists():
            deleted_count = 0
            for file_path in transcription_dir.iterdir():
                if not file_path.is_file():
                    continue
                    
                try:
                    size_mb = file_path.stat().st_size / (1024 * 1024)
                    await asyncio.to_thread(file_path.unlink)
                    deleted_count += 1
                    report["space_freed_mb"] += size_mb
                except Exception as e:
                    logger.error(f"âŒ Erro ao remover transcriÃ§Ã£o {file_path.name}: {e}")
                    report["errors"].append(f"Transcription/{file_path.name}: {str(e)}")
            
            report["files_deleted"] += deleted_count
            if deleted_count > 0:
                logger.info(f"ðŸ—‘ï¸  Transcriptions: {deleted_count} arquivos removidos")
            else:
                logger.info("âœ“ Transcriptions: nenhum arquivo encontrado")
        
        # 4. LIMPAR TODOS OS ARQUIVOS TEMPORÃRIOS
        temp_dir = Path(settings.get('temp_dir', './temp'))
        if temp_dir.exists():
            deleted_count = 0
            for file_path in temp_dir.iterdir():
                if not file_path.is_file():
                    continue
                    
                try:
                    size_mb = file_path.stat().st_size / (1024 * 1024)
                    await asyncio.to_thread(file_path.unlink)
                    deleted_count += 1
                    report["space_freed_mb"] += size_mb
                except Exception as e:
                    logger.error(f"âŒ Erro ao remover temp {file_path.name}: {e}")
                    report["errors"].append(f"Temp/{file_path.name}: {str(e)}")
            
            report["files_deleted"] += deleted_count
            if deleted_count > 0:
                logger.info(f"ðŸ—‘ï¸  Temp: {deleted_count} arquivos removidos")
            else:
                logger.info("âœ“ Temp: nenhum arquivo encontrado")
        
        # 5. LIMPAR TODOS OS MODELOS WHISPER BAIXADOS
        models_dir = Path(settings.get('model_dir', './models'))
        if models_dir.exists():
            deleted_count = 0
            for file_path in models_dir.rglob("*"):  # rglob para pegar subdiretÃ³rios tambÃ©m
                if not file_path.is_file():
                    continue
                    
                try:
                    size_mb = file_path.stat().st_size / (1024 * 1024)
                    await asyncio.to_thread(file_path.unlink)
                    deleted_count += 1
                    report["space_freed_mb"] += size_mb
                    report["models_deleted"] += 1
                except Exception as e:
                    logger.error(f"âŒ Erro ao remover modelo {file_path.name}: {e}")
                    report["errors"].append(f"Models/{file_path.name}: {str(e)}")
            
            if deleted_count > 0:
                logger.warning(f"ðŸ—‘ï¸  Models: {deleted_count} arquivos de modelo removidos ({size_mb:.2f}MB)")
            else:
                logger.info("âœ“ Models: nenhum modelo encontrado")
        
        # Formatar relatÃ³rio
        report["space_freed_mb"] = round(report["space_freed_mb"], 2)
        
        # âœ… CRÃTICO: SEGUNDO FLUSHDB para garantir limpeza total
        # (Remove jobs que foram salvos DURANTE a limpeza por workers Celery)
        try:
            # Verifica se hÃ¡ keys novas (salvos durante a limpeza)
            keys_after = job_store.redis.keys("transcription_job:*")
            if keys_after:
                logger.warning(f"âš ï¸ {len(keys_after)} jobs foram salvos DURANTE a limpeza! Executando FLUSHDB novamente...")
                job_store.redis.flushdb()
                report["jobs_removed"] += len(keys_after)
                logger.info(f"âœ… SEGUNDO FLUSHDB executado: {len(keys_after)} jobs adicionais removidos")
            else:
                logger.info("âœ“ Nenhum job novo detectado apÃ³s limpeza")
                
        except Exception as e:
            logger.error(f"âŒ Erro no segundo FLUSHDB: {e}")
            report["errors"].append(f"Segundo FLUSHDB: {str(e)}")
        
        report["message"] = (
            f"ðŸ”¥ LIMPEZA TOTAL CONCLUÃDA: "
            f"{report['jobs_removed']} jobs do Redis + "
            f"{report['files_deleted']} arquivos + "
            f"{report['models_deleted']} modelos removidos "
            f"({report['space_freed_mb']}MB liberados)"
        )
        
        if report["errors"]:
            report["message"] += f" âš ï¸ com {len(report['errors'])} erros"
        
        logger.warning(report["message"])
        return report
        
    except Exception as e:
        logger.error(f"âŒ Erro na limpeza total: {e}")
        return {"error": str(e), "jobs_removed": 0, "files_deleted": 0, "models_deleted": 0}


@app.post("/admin/cleanup")
async def manual_cleanup(
    deep: bool = False,
    purge_celery_queue: bool = False
):
    """
    ðŸ§¹ LIMPEZA DO SISTEMA
    
    âš ï¸ IMPORTANTE: ExecuÃ§Ã£o SÃNCRONA (sem background tasks ou Celery)
    O cliente AGUARDA a conclusÃ£o completa antes de receber a resposta.
    
    **Por que sÃ­ncrono?**
    Se usÃ¡ssemos Celery/background tasks, o job de limpeza seria deletado
    antes de terminar (ciclo vicioso). Por isso executa DIRETAMENTE no handler HTTP.
    
    **Modos de operaÃ§Ã£o:**
    
    1. **Limpeza bÃ¡sica** (deep=false):
       - Remove jobs expirados (>24h)
       - Remove arquivos Ã³rfÃ£os
    
    2. **Limpeza profunda** (deep=true) - âš ï¸ FACTORY RESET:
       - TODO o banco Redis (FLUSHDB usando DIVISOR do .env)
       - TODOS os arquivos de uploads/
       - TODOS os arquivos de transcriptions/
       - TODOS os arquivos temporÃ¡rios em temp/
       - TODOS os modelos Whisper baixados (~500MB cada)
       - **OPCIONAL:** Purga fila Celery (purge_celery_queue=true)
    
    **ParÃ¢metros:**
    - deep (bool): Se true, faz limpeza COMPLETA (factory reset)
    - purge_celery_queue (bool): Se true, limpa FILA CELERY tambÃ©m
    
    **Retorna apenas APÃ“S conclusÃ£o completa!**
    """
    cleanup_type = "TOTAL" if deep else "bÃ¡sica"
    logger.warning(f"ðŸ”¥ Iniciando limpeza {cleanup_type} SÃNCRONA (purge_celery={purge_celery_queue})")
    
    try:
        # Executa DIRETAMENTE (sem background tasks ou Celery)
        if deep:
            result = await _perform_cleanup(purge_celery_queue)
        else:
            result = await _perform_basic_cleanup()
        
        logger.info(f"âœ… Limpeza {cleanup_type} CONCLUÃDA com sucesso")
        return result
        
    except Exception as e:
        logger.error(f"âŒ ERRO na limpeza {cleanup_type}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erro ao fazer cleanup: {str(e)}")


@app.get("/admin/stats")
async def get_stats():
    """EstatÃ­sticas do sistema"""
    stats = job_store.get_stats()
    
    # Adiciona info do cache
    upload_path = Path("./uploads")
    transcription_path = Path("./transcriptions")
    
    total_files = 0
    total_size = 0
    
    for path in [upload_path, transcription_path]:
        if path.exists():
            files = list(path.iterdir())
            total_files += len(files)
            total_size += sum(f.stat().st_size for f in files if f.is_file())
    
    stats["cache"] = {
        "files_count": total_files,
        "total_size_mb": round(total_size / (1024 * 1024), 2)
    }
    
    return stats


@app.get("/health")
async def health_check():
    """Health check profundo - valida recursos crÃ­ticos"""
    import shutil
    import subprocess
    from fastapi.responses import JSONResponse
    
    health_status = {
        "status": "healthy",
        "service": "audio-transcription", 
        "version": "2.0.0",
        "timestamp": datetime.now().isoformat(),
        "checks": {}
    }
    
    is_healthy = True
    
    # 1. Verifica Redis
    try:
        job_store.redis.ping()
        health_status["checks"]["redis"] = {"status": "ok", "message": "Connected"}
    except Exception as e:
        health_status["checks"]["redis"] = {"status": "error", "message": str(e)}
        is_healthy = False
    
    # 2. Verifica espaÃ§o em disco
    try:
        output_dir = Path(settings['transcription_dir'])
        output_dir.mkdir(exist_ok=True, parents=True)
        stat = shutil.disk_usage(output_dir)
        free_gb = stat.free / (1024**3)
        total_gb = stat.total / (1024**3)
        percent_free = (stat.free / stat.total) * 100
        
        disk_status = "ok" if percent_free > 10 else "warning" if percent_free > 5 else "critical"
        if percent_free <= 5:
            is_healthy = False
            
        health_status["checks"]["disk_space"] = {
            "status": disk_status,
            "free_gb": round(free_gb, 2),
            "total_gb": round(total_gb, 2),
            "percent_free": round(percent_free, 2)
        }
    except Exception as e:
        health_status["checks"]["disk_space"] = {"status": "error", "message": str(e)}
        is_healthy = False
    
    # 3. Verifica ffmpeg
    try:
        result = subprocess.run(
            ["ffmpeg", "-version"],
            capture_output=True,
            text=True,
            timeout=5
        )
        if result.returncode == 0:
            version = result.stdout.split('\n')[0]
            health_status["checks"]["ffmpeg"] = {"status": "ok", "version": version}
        else:
            health_status["checks"]["ffmpeg"] = {"status": "error", "message": "ffmpeg not responding"}
            is_healthy = False
    except FileNotFoundError:
        health_status["checks"]["ffmpeg"] = {"status": "error", "message": "ffmpeg not installed"}
        is_healthy = False
    except Exception as e:
        health_status["checks"]["ffmpeg"] = {"status": "error", "message": str(e)}
        is_healthy = False
    
    # 4. Verifica Celery workers (verificaÃ§Ã£o simplificada)
    try:
        # VerificaÃ§Ã£o bÃ¡sica sem timeout complexo para evitar travamento
        health_status["checks"]["celery_workers"] = {
            "status": "ok",
            "message": "Celery workers check skipped for faster health response"
        }
    except Exception as e:
        health_status["checks"]["celery_workers"] = {"status": "error", "message": str(e)}
    
    # 5. Verifica modelo Whisper (verificaÃ§Ã£o bÃ¡sica e rÃ¡pida)
    try:
        model_name = settings.get('whisper_model', 'base')
        health_status["checks"]["whisper_model"] = {
            "status": "ok",
            "model": model_name,
            "message": "Model serÃ¡ carregado no primeiro uso"
        }
    except Exception as e:
        health_status["checks"]["whisper_model"] = {"status": "error", "message": str(e)}
    
    # Atualiza status geral
    health_status["status"] = "healthy" if is_healthy else "unhealthy"
    
    status_code = 200 if is_healthy else 503
    
    return JSONResponse(content=health_status, status_code=status_code)
