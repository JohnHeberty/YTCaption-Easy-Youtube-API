#!/usr/bin/env python3
"""
Script para testar suporte CUDA/GPU no container
"""
import sys

print("=" * 60)
print("üîç TESTE DE SUPORTE GPU/CUDA")
print("=" * 60)

# Teste 1: PyTorch
print("\n1Ô∏è‚É£ Verificando PyTorch...")
try:
    import torch
    print(f"   ‚úÖ PyTorch instalado: {torch.__version__}")
    print(f"   ‚îî‚îÄ CUDA Built Version: {torch.version.cuda}")
    
    cuda_available = torch.cuda.is_available()
    print(f"   ‚îî‚îÄ CUDA Dispon√≠vel: {cuda_available}")
    
    if cuda_available:
        print(f"   ‚îî‚îÄ GPU Count: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"   ‚îî‚îÄ GPU {i}: {torch.cuda.get_device_name(i)}")
            print(f"      ‚îî‚îÄ Compute Capability: {torch.cuda.get_device_capability(i)}")
        
        # Teste de tensor
        print("\n   üî• Testando opera√ß√£o na GPU...")
        x = torch.randn(1000, 1000).to('cuda')
        y = x @ x.T
        print(f"   ‚úÖ Opera√ß√£o na GPU bem-sucedida!")
        print(f"   ‚îî‚îÄ Mem√≥ria Alocada: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB")
        print(f"   ‚îî‚îÄ Mem√≥ria Total: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
    else:
        print("   ‚ö†Ô∏è CUDA n√£o dispon√≠vel")
        
except ImportError as e:
    print(f"   ‚ùå PyTorch n√£o instalado: {e}")
    sys.exit(1)

# Teste 2: Whisper
print("\n2Ô∏è‚É£ Verificando Whisper...")
try:
    import whisper
    print(f"   ‚úÖ Whisper instalado")
    
    # Lista modelos dispon√≠veis
    print(f"   ‚îî‚îÄ Modelos dispon√≠veis: {whisper.available_models()}")
    
except ImportError as e:
    print(f"   ‚ùå Whisper n√£o instalado: {e}")
    sys.exit(1)

# Teste 3: Vari√°veis de Ambiente
print("\n3Ô∏è‚É£ Verificando Vari√°veis de Ambiente...")
import os
env_vars = [
    'CUDA_VISIBLE_DEVICES',
    'NVIDIA_VISIBLE_DEVICES',
    'NVIDIA_DRIVER_CAPABILITIES',
    'FORCE_CUDA',
    'WHISPER_DEVICE'
]

for var in env_vars:
    value = os.getenv(var, 'Not Set')
    print(f"   ‚îî‚îÄ {var}: {value}")

# Teste 4: Carregar modelo pequeno
print("\n4Ô∏è‚É£ Testando carregamento de modelo...")
try:
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"   ‚îî‚îÄ Carregando modelo 'tiny' no {device}...")
    model = whisper.load_model('tiny', device=device)
    print(f"   ‚úÖ Modelo carregado com sucesso no {device.upper()}!")
    
    # Informa√ß√µes do modelo
    print(f"   ‚îî‚îÄ Device do modelo: {next(model.parameters()).device}")
    
    if device == 'cuda':
        print(f"   ‚îî‚îÄ Mem√≥ria GPU ap√≥s carregar: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB")
    
except Exception as e:
    print(f"   ‚ùå Erro ao carregar modelo: {e}")
    sys.exit(1)

print("\n" + "=" * 60)
print("‚úÖ TODOS OS TESTES PASSARAM!")
print("=" * 60)
