#!/bin/bash
# ValidaÃ§Ã£o de Suporte GPU - Audio Transcriber
# Executar apÃ³s: docker compose up -d

set -e

echo "ğŸ” =========================================="
echo "   VALIDAÃ‡ÃƒO DE GPU - AUDIO TRANSCRIBER"
echo "=========================================="
echo ""

# Cores
GREEN='\033[0;32m'
RED='\033[0;31m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# FunÃ§Ã£o de checagem
check_status() {
    if [ $? -eq 0 ]; then
        echo -e "${GREEN}âœ… PASSOU${NC}"
        return 0
    else
        echo -e "${RED}âŒ FALHOU${NC}"
        return 1
    fi
}

# 1. Container estÃ¡ rodando?
echo "ğŸ“¦ 1. Verificando se container estÃ¡ rodando..."
docker ps | grep audio-transcriber-api > /dev/null
check_status

# 2. PyTorch instalado?
echo ""
echo "ğŸ 2. Verificando instalaÃ§Ã£o do PyTorch..."
docker exec audio-transcriber-api python -c "import torch; print(f'PyTorch: {torch.__version__}')" 2>/dev/null
check_status

# 3. CUDA disponÃ­vel?
echo ""
echo "ğŸ® 3. Verificando CUDA disponÃ­vel no PyTorch..."
CUDA_CHECK=$(docker exec audio-transcriber-api python -c "import torch; print(torch.cuda.is_available())" 2>/dev/null)
echo "CUDA Available: $CUDA_CHECK"
if [ "$CUDA_CHECK" == "True" ]; then
    echo -e "${GREEN}âœ… CUDA DISPONÃVEL${NC}"
else
    echo -e "${RED}âŒ CUDA NÃƒO DISPONÃVEL${NC}"
    echo ""
    echo "ğŸ”§ Troubleshooting:"
    echo "   1. Verificar nvidia-smi no host: nvidia-smi"
    echo "   2. Verificar runtime: docker exec audio-transcriber-api bash -c 'ls /usr/lib/x86_64-linux-gnu/libcuda*'"
    echo "   3. Verificar variÃ¡veis: docker exec audio-transcriber-api env | grep NVIDIA"
    echo "   4. Reconstruir: docker compose down && docker compose build --no-cache && docker compose up -d"
    exit 1
fi

# 4. Qual versÃ£o CUDA?
echo ""
echo "ğŸ“Š 4. Verificando versÃ£o CUDA..."
docker exec audio-transcriber-api python -c "import torch; print(f'CUDA Version: {torch.version.cuda}')" 2>/dev/null
check_status

# 5. Qual GPU?
echo ""
echo "ğŸ¯ 5. Identificando GPU..."
GPU_NAME=$(docker exec audio-transcriber-api python -c "import torch; print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A')" 2>/dev/null)
echo "GPU: $GPU_NAME"
if [ "$GPU_NAME" != "N/A" ]; then
    echo -e "${GREEN}âœ… GPU DETECTADA: $GPU_NAME${NC}"
else
    echo -e "${RED}âŒ GPU NÃƒO DETECTADA${NC}"
fi

# 6. Verificar logs do container
echo ""
echo "ğŸ“‹ 6. Verificando logs (Ãºltimas 20 linhas)..."
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
docker logs audio-transcriber-api --tail 20 | grep -i "cuda\|gpu\|device\|warning\|error" || echo "Nenhuma mensagem relevante encontrada"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# 7. Verificar Whisper Device Config
echo ""
echo "âš™ï¸  7. Verificando configuraÃ§Ã£o Whisper Device..."
WHISPER_DEVICE=$(docker exec audio-transcriber-api bash -c 'echo $WHISPER_DEVICE' 2>/dev/null)
echo "WHISPER_DEVICE: $WHISPER_DEVICE"
if [ "$WHISPER_DEVICE" == "cuda" ]; then
    echo -e "${GREEN}âœ… CONFIGURADO PARA USAR GPU${NC}"
else
    echo -e "${YELLOW}âš ï¸  CONFIGURADO PARA: $WHISPER_DEVICE${NC}"
fi

# 8. Verificar variÃ¡veis NVIDIA
echo ""
echo "ğŸ”§ 8. Verificando variÃ¡veis NVIDIA..."
docker exec audio-transcriber-api env | grep NVIDIA || echo "VariÃ¡veis NVIDIA nÃ£o encontradas"

# 9. Teste rÃ¡pido de GPU
echo ""
echo "ğŸ§ª 9. Teste rÃ¡pido de alocaÃ§Ã£o GPU..."
docker exec audio-transcriber-api python -c "
import torch
if torch.cuda.is_available():
    try:
        x = torch.randn(100, 100).cuda()
        y = x @ x.T
        print(f'âœ… AlocaÃ§Ã£o GPU bem-sucedida')
        print(f'   Tensor shape: {y.shape}')
        print(f'   Device: {y.device}')
        del x, y
        torch.cuda.empty_cache()
    except Exception as e:
        print(f'âŒ Erro ao alocar GPU: {e}')
else:
    print('âŒ CUDA nÃ£o disponÃ­vel para teste')
" 2>/dev/null
check_status

# 10. Verificar memÃ³ria GPU
echo ""
echo "ğŸ’¾ 10. Verificando memÃ³ria GPU..."
docker exec audio-transcriber-api python -c "
import torch
if torch.cuda.is_available():
    allocated = torch.cuda.memory_allocated(0) / 1024**2  # MB
    reserved = torch.cuda.memory_reserved(0) / 1024**2    # MB
    total = torch.cuda.get_device_properties(0).total_memory / 1024**2  # MB
    print(f'   Alocada: {allocated:.2f} MB')
    print(f'   Reservada: {reserved:.2f} MB')
    print(f'   Total: {total:.2f} MB')
    print(f'   Livre: {total - allocated:.2f} MB')
else:
    print('   N/A - CUDA nÃ£o disponÃ­vel')
" 2>/dev/null

echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo -e "${GREEN}âœ… VALIDAÃ‡ÃƒO COMPLETA!${NC}"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""
echo "ğŸ“Œ PrÃ³ximos passos:"
echo "   1. Testar transcriÃ§Ã£o: curl -X POST http://localhost:8005/jobs -F 'file=@test.mp3' -F 'language_in=pt'"
echo "   2. Monitorar logs: docker logs -f audio-transcriber-api"
echo "   3. Verificar performance com nvidia-smi durante transcriÃ§Ã£o"
echo ""
