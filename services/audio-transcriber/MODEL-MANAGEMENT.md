# üîã Gerenciamento de Modelo Whisper - Economia de Recursos

## üìã Vis√£o Geral

O servi√ßo **audio-transcriber** agora possui endpoints para **gerenciar o carregamento/descarregamento do modelo Whisper** na mem√≥ria RAM e GPU/VRAM, permitindo **economia de recursos energ√©ticos** e **redu√ß√£o da pegada de carbono** quando o servi√ßo est√° idle.

---

## üÜï Novos Endpoints

### 1. **POST /model/unload** - Descarregar Modelo

Descarrega o modelo Whisper da mem√≥ria/GPU para economia de recursos.

**Quando usar:**
- ‚úÖ Ap√≥s processar batch de transcri√ß√µes
- ‚úÖ Durante per√≠odos de inatividade (sem tasks)
- ‚úÖ Para reduzir consumo energ√©tico quando idle
- ‚úÖ Sustentabilidade: reduzir pegada de carbono

**Request:**
```bash
curl -X POST http://localhost:8002/model/unload
```

**Response (Sucesso):**
```json
{
  "success": true,
  "message": "‚úÖ Modelo 'base' descarregado com sucesso do CUDA. Recursos liberados...",
  "memory_freed": {
    "ram_mb": 150.0,
    "vram_mb": 142.5
  },
  "device_was": "cuda",
  "model_name": "base"
}
```

**Benef√≠cios:**
- üîã **Economia de energia**: Libera GPU/CPU quando n√£o h√° tasks
- ‚ôªÔ∏è **Sustentabilidade**: Reduz emiss√µes de CO‚ÇÇ
- üíæ **Mem√≥ria**: Libera ~150MB a 3GB de RAM + VRAM
- ‚ö° **Seguro**: Modelo √© recarregado automaticamente na pr√≥xima task

---

### 2. **POST /model/load** - Carregar Modelo

Carrega o modelo Whisper explicitamente na mem√≥ria/GPU.

**Quando usar:**
- ‚úÖ Antes de processar m√∫ltiplas transcri√ß√µes (batch)
- ‚úÖ Ap√≥s descarregar com `/model/unload`
- ‚úÖ Para garantir primeira transcri√ß√£o sem delay de carregamento
- ‚úÖ Preparar sistema para per√≠odo de alta demanda

**Request:**
```bash
curl -X POST http://localhost:8002/model/load
```

**Response (Sucesso):**
```json
{
  "success": true,
  "message": "‚úÖ Modelo 'base' carregado com sucesso no CUDA. Sistema pronto...",
  "memory_used": {
    "ram_mb": 150.0,
    "vram_mb": 145.8
  },
  "device": "cuda",
  "model_name": "base"
}
```

**Benef√≠cios:**
- üöÄ **Performance**: Primeira transcri√ß√£o mais r√°pida
- ‚è±Ô∏è **Lat√™ncia**: Elimina delay de carregamento
- üìä **Previsibilidade**: Sistema sempre pronto

---

### 3. **GET /model/status** - Status do Modelo

Consulta status atual do modelo Whisper.

**Request:**
```bash
curl http://localhost:8002/model/status
```

**Response (Modelo Carregado na GPU):**
```json
{
  "loaded": true,
  "model_name": "base",
  "device": "cuda",
  "memory": {
    "vram_mb": 145.8,
    "vram_reserved_mb": 256.0,
    "cuda_available": true
  },
  "gpu_info": {
    "name": "NVIDIA GeForce RTX 3060",
    "device_count": 1,
    "cuda_version": "12.1"
  }
}
```

**Response (Modelo Descarregado):**
```json
{
  "loaded": false,
  "model_name": "base",
  "device": null,
  "memory": {
    "vram_mb": 0.0,
    "cuda_available": true
  }
}
```

**Benef√≠cios:**
- üìä **Monitoramento**: Verificar estado atual do modelo
- üîç **Debugging**: Diagnosticar problemas de mem√≥ria
- üìà **Observabilidade**: Integrar com dashboards

---

## üéØ Casos de Uso

### 1. **Economia de Recursos em Idle**

Cen√°rio: Servi√ßo rodando 24/7 mas com transcri√ß√µes apenas durante hor√°rio comercial.

```bash
# Durante a noite/final de semana (sem transcri√ß√µes)
curl -X POST http://localhost:8002/model/unload
# ‚úÖ Libera ~150MB RAM + VRAM
# ‚úÖ Reduz consumo de energia
# ‚úÖ Menor pegada de carbono

# Antes de iniciar expediente
curl -X POST http://localhost:8002/model/load
# ‚úÖ Sistema pronto para trabalhar
```

### 2. **Processamento de Batch**

Cen√°rio: Processar 100 transcri√ß√µes de uma vez.

```bash
# 1. Carrega modelo explicitamente
curl -X POST http://localhost:8002/model/load

# 2. Submete todas as 100 transcri√ß√µes
for i in {1..100}; do
  curl -X POST http://localhost:8002/jobs \
    -F "file=@audio_${i}.mp3" \
    -F "language_in=auto"
done

# 3. Ap√≥s concluir todas, libera recursos
curl -X POST http://localhost:8002/model/unload
```

### 3. **Monitoramento Cont√≠nuo**

```bash
# Verificar status a cada 5 minutos
watch -n 300 'curl -s http://localhost:8002/model/status | jq'
```

---

## ‚öôÔ∏è Configura√ß√£o

### Vari√°vel de Ambiente: `WHISPER_PRELOAD_MODEL`

Controla se o modelo √© carregado automaticamente no startup do servi√ßo.

**Valores:**
- `true` (padr√£o): Carrega modelo no startup
- `false`: Modelo s√≥ √© carregado quando necess√°rio (primeira task)

**Configurar no `.env`:**
```bash
# Carregar modelo no startup (comportamento padr√£o)
WHISPER_PRELOAD_MODEL=true

# OU desabilitar pr√©-carregamento (economia m√°xima)
WHISPER_PRELOAD_MODEL=false
```

**Configurar no docker-compose.yml:**
```yaml
services:
  audio-transcriber:
    environment:
      - WHISPER_PRELOAD_MODEL=false  # Economia de recursos no startup
```

---

## üîÑ Comportamento Autom√°tico

### ‚úÖ Carregamento Sob Demanda (Lazy Loading)

O modelo **sempre ser√° carregado automaticamente** quando necess√°rio, mesmo que:
- Servi√ßo inicie com `WHISPER_PRELOAD_MODEL=false`
- Modelo seja descarregado com `/model/unload`
- Houver falha no carregamento inicial

**Exemplo:**
```bash
# 1. Descarrega modelo
curl -X POST http://localhost:8002/model/unload
# ‚úÖ Modelo descarregado, mem√≥ria liberada

# 2. Nova transcri√ß√£o √© criada
curl -X POST http://localhost:8002/jobs -F "file=@audio.mp3"
# ‚úÖ Modelo √© carregado AUTOMATICAMENTE antes de processar
# ‚úÖ Transcri√ß√£o funciona normalmente
```

**N√£o h√° risco de falha!** O carregamento sob demanda garante que o servi√ßo sempre funcionar√°.

---

## üìä Uso de Mem√≥ria por Modelo

| Modelo | RAM (estimado) | VRAM (GPU) | Qualidade | Velocidade |
|--------|----------------|------------|-----------|------------|
| `tiny` | ~75 MB | ~70 MB | ‚≠ê | ‚ö°‚ö°‚ö° |
| `base` | ~150 MB | ~140 MB | ‚≠ê‚≠ê | ‚ö°‚ö°‚ö° |
| `small` | ~500 MB | ~460 MB | ‚≠ê‚≠ê‚≠ê | ‚ö°‚ö° |
| `medium` | ~1.5 GB | ~1.4 GB | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö° |
| `large` | ~3 GB | ~2.9 GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | üêå |

**Recomenda√ß√µes:**
- **Produ√ß√£o geral**: `base` (bom equil√≠brio)
- **Alta qualidade**: `small` ou `medium`
- **Recursos limitados**: `tiny`
- **M√°xima precis√£o**: `large` (requer GPU potente)

---

## üåç Impacto Ambiental

### Por que gerenciar o modelo importa?

**Consumo de GPU em idle:**
- GPU ociosa com modelo carregado: ~20-50W
- GPU sem modelo (idle real): ~5-10W
- **Economia**: ~15-40W por hora

**C√°lculo anual (servidor 24/7):**
```
Economia por hora: 25W (m√©dia)
Horas ociosas por dia: 16h (67%)
Dias por ano: 365

Economia anual: 25W √ó 16h √ó 365 = 146 kWh/ano
Redu√ß√£o CO‚ÇÇ: ~73 kg/ano (m√©dia grid el√©trico)
```

**Escalando para 10 servidores:**
- Economia: 1.460 kWh/ano
- Redu√ß√£o CO‚ÇÇ: 730 kg/ano (equivalente a ~350 √°rvores plantadas)

---

## üöÄ Guia R√°pido

### Cen√°rio 1: Uso Normal (24/7 com tasks espor√°dicas)
```bash
# Deixar WHISPER_PRELOAD_MODEL=true (padr√£o)
# Modelo sempre carregado, pronto para uso imediato
```

### Cen√°rio 2: Economia M√°xima (per√≠odos idle longos)
```bash
# Configurar WHISPER_PRELOAD_MODEL=false
# Usar cron job para descarregar √† noite:

# Crontab: descarregar √†s 20h (ap√≥s expediente)
0 20 * * * curl -X POST http://localhost:8002/model/unload

# Crontab: carregar √†s 7h (antes do expediente)
0 7 * * * curl -X POST http://localhost:8002/model/load
```

### Cen√°rio 3: Processamento Batch
```bash
# Script de processamento:
#!/bin/bash

# 1. Carrega modelo
curl -X POST http://localhost:8002/model/load

# 2. Processa arquivos
for file in *.mp3; do
  curl -X POST http://localhost:8002/jobs -F "file=@$file"
done

# 3. Aguarda conclus√£o (polling)
# ... (seu c√≥digo de aguardar jobs)

# 4. Descarrega modelo
curl -X POST http://localhost:8002/model/unload
```

---

## üìù Notas Importantes

### ‚úÖ Seguran√ßa
- Descarregar modelo **N√ÉO afeta** tasks em execu√ß√£o
- Tasks em fila ser√£o processadas normalmente (modelo recarrega automaticamente)
- Opera√ß√£o √© **idempotente** (pode chamar m√∫ltiplas vezes sem erro)

### ‚ö†Ô∏è Performance
- Primeira transcri√ß√£o ap√≥s `unload` ter√° **+3-10s de delay** (carregamento)
- Transcri√ß√µes subsequentes: lat√™ncia normal
- GPU demora mais para carregar que CPU (~5-10s vs ~2-3s)

### üîß Troubleshooting
- Se `/model/unload` falhar: Verificar se h√° tasks em processamento
- Se `/model/load` falhar: Verificar logs de GPU/CUDA
- Se modelo n√£o carregar automaticamente: Verificar espa√ßo em disco/mem√≥ria

---

## üìû Suporte

Para d√∫vidas ou problemas:
1. Verificar logs do container: `docker logs audio-transcriber`
2. Consultar status: `GET /model/status`
3. Health check: `GET /health`

**Data de cria√ß√£o**: 04/11/2025  
**Vers√£o do servi√ßo**: 2.0.0
