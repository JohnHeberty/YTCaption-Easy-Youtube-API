# üöÄ PLANEJAMENTO DE MIGRA√á√ÉO F5-TTS pt-BR

**Objetivo:** Migrar completamente a arquitetura para usar o modelo customizado pt-BR `model_last.safetensors` (1.35 GB)  
**GPU Target:** GTX 1050 Ti (4GB VRAM)  
**Status Atual:** Modelo incompat√≠vel com `f5-tts` pip - requer instala√ß√£o do reposit√≥rio original  
**Data In√≠cio:** 26/11/2025

---

## üìä VIS√ÉO GERAL DAS SPRINTS

```
Sprint 1: An√°lise e Prepara√ß√£o          [3-4 horas]  ‚¨ú TODO
Sprint 2: Instala√ß√£o F5-TTS Original    [2-3 horas]  ‚¨ú TODO
Sprint 3: Adapta√ß√£o de C√≥digo           [4-5 horas]  ‚¨ú TODO
Sprint 4: Testes e Otimiza√ß√£o           [3-4 horas]  ‚¨ú TODO
Sprint 5: Documenta√ß√£o e Deploy         [2-3 horas]  ‚¨ú TODO

TOTAL ESTIMADO: 14-19 horas
```

---

# üéØ SPRINT 1: AN√ÅLISE E PREPARA√á√ÉO

**Dura√ß√£o:** 3-4 horas  
**Objetivo:** Entender completamente o modelo pt-BR e preparar ambiente para migra√ß√£o

## 1.1. An√°lise Profunda do Modelo pt-BR

### Tarefas:

- [ ] **1.1.1. Inspecionar estrutura do checkpoint**
  ```bash
  # Dentro do container
  python3 << EOF
  import torch
  checkpoint = torch.load('/app/models/f5tts/pt-br/model_last.safetensors')
  print("Keys:", checkpoint.keys())
  print("\nModel state dict keys (primeiras 20):")
  for i, key in enumerate(list(checkpoint.get('model_state_dict', checkpoint).keys())[:20]):
      print(f"  {i+1}. {key}")
  print("\nShapes:")
  for key, value in list(checkpoint.get('model_state_dict', checkpoint).items())[:5]:
      print(f"  {key}: {value.shape}")
  EOF
  ```

- [ ] **1.1.2. Identificar metadados do treinamento**
  ```bash
  # Verificar se h√° informa√ß√µes sobre:
  # - Vers√£o do F5-TTS usada
  # - Hiperpar√¢metros de treinamento
  # - Dataset utilizado
  # - N√∫mero de steps/epochs
  ```

- [ ] **1.1.3. Buscar informa√ß√µes sobre o modelo**
  - Procurar README, documenta√ß√£o ou paper relacionado
  - Verificar se h√° vocab.txt ou outros arquivos auxiliares no diret√≥rio
  - Documentar origem e caracter√≠sticas do modelo

### Entreg√°veis:
- `MODELO-PT-BR-ANALISE.md` com todas as informa√ß√µes coletadas

---

## 1.2. Pesquisa de Compatibilidade ‚úÖ CONCLU√çDO

### Tarefas:

- [x] **1.2.1. Verificar vers√µes do F5-TTS**
  - Clonado reposit√≥rio oficial: commit 3eecd94, v1.1.9
  - Estrutura moderna `transformer_blocks` confirmada
  - Suporte para configura√ß√µes customizadas identificado

- [x] **1.2.2. Testar carregamento do modelo**
  - Criado `test_model_compatibility.py` e `test_final_compatibility.py`
  - Identificadas todas as configura√ß√µes necess√°rias
  - ‚úÖ **SUCESSO TOTAL**: Zero missing keys, zero unexpected keys

- [x] **1.2.3. Documentar configura√ß√µes pt-BR**
  - Criado `CONFIGURACOES-MODELO-PT-BR.md`
  - Todas as dimens√µes mapeadas:
    - `dim=1024, depth=22, heads=16, dim_head=64`
    - `ff_mult=2, mel_dim=100`
    - `text_num_embeds=2545, text_dim=512`
    - `conv_layers=4`

### Entreg√°veis:
- ‚úÖ `CONFIGURACOES-MODELO-PT-BR.md` - Documenta√ß√£o completa
- ‚úÖ `test_final_compatibility.py` - Teste validado
- ‚úÖ Modelo pt-BR 100% compat√≠vel com F5-TTS reposit√≥rio oficial

---

## 1.3. Backup e Prepara√ß√£o do Ambiente

### Tarefas:

- [ ] **1.3.1. Backup completo do servi√ßo atual**
  ```bash
  cd /home/john/YTCaption-Easy-Youtube-API/services/audio-voice
  tar -czf ~/backup-audio-voice-$(date +%Y%m%d-%H%M%S).tar.gz \
      app/ Dockerfile docker-compose.yml requirements.txt .env
  ```

- [ ] **1.3.2. Criar branch Git para migra√ß√£o**
  ```bash
  cd /home/john/YTCaption-Easy-Youtube-API
  git checkout -b feature/f5tts-ptbr-migration
  git add -A
  git commit -m "checkpoint: estado antes migra√ß√£o F5-TTS pt-BR"
  ```

- [ ] **1.3.3. Documentar estado atual**
  - Vers√µes atuais de todas as depend√™ncias
  - Configura√ß√µes Docker atuais
  - Testes manuais que funcionam (se houver)

### Entreg√°veis:
- Backup seguro do servi√ßo
- Branch Git dedicada
- Documenta√ß√£o do estado inicial

---

# üîß SPRINT 2: INSTALA√á√ÉO F5-TTS ORIGINAL

**Dura√ß√£o:** 2-3 horas  
**Objetivo:** Instalar F5-TTS do reposit√≥rio original e garantir funcionamento b√°sico

## 2.1. Modificar Dockerfile

### Tarefas:

- [ ] **2.1.1. Criar novo Dockerfile com instala√ß√£o do repo**
  ```dockerfile
  # services/audio-voice/Dockerfile
  
  # ... [manter base CUDA existente] ...
  
  # Remover instala√ß√£o pip do f5-tts
  # Adicionar ap√≥s instala√ß√£o de requirements.txt:
  
  # Instalar F5-TTS do reposit√≥rio oficial
  RUN cd /tmp && \
      git clone https://github.com/SWivid/F5-TTS.git && \
      cd F5-TTS && \
      # Checkout do commit compat√≠vel (identificado na Sprint 1)
      git checkout <COMMIT_HASH_COMPATIVEL> && \
      pip install -e . && \
      cd / && rm -rf /tmp/F5-TTS
  
  # Ou, se precisar manter o repo:
  RUN mkdir -p /app/vendor && \
      cd /app/vendor && \
      git clone https://github.com/SWivid/F5-TTS.git && \
      cd F5-TTS && \
      git checkout <COMMIT_HASH> && \
      pip install -e .
  ```

- [ ] **2.1.2. Atualizar requirements.txt**
  ```bash
  # Remover ou comentar:
  # f5-tts
  
  # Adicionar depend√™ncias espec√≠ficas se necess√°rio
  # (baseado na an√°lise do requirements.txt do F5-TTS original)
  ```

- [ ] **2.1.3. Adicionar vari√°veis de ambiente**
  ```bash
  # .env ou docker-compose.yml
  F5TTS_REPO_PATH=/app/vendor/F5-TTS
  F5TTS_CUSTOM_COMMIT=<commit_hash>
  ```

### Entreg√°veis:
- Dockerfile atualizado e testado
- requirements.txt otimizado
- Build bem-sucedido da imagem

---

## 2.2. Testar Instala√ß√£o B√°sica

### Tarefas:

- [ ] **2.2.1. Build e test da nova imagem**
  ```bash
  cd /home/john/YTCaption-Easy-Youtube-API/services/audio-voice
  docker compose build audio-voice-service
  
  # Testar imports b√°sicos
  docker compose run --rm audio-voice-service python -c "
  from f5_tts.api import F5TTS
  from f5_tts.infer.utils_infer import load_model
  print('‚úÖ F5-TTS importado com sucesso')
  "
  ```

- [ ] **2.2.2. Verificar compatibilidade GPU**
  ```bash
  docker compose run --rm audio-voice-service python -c "
  import torch
  print(f'CUDA available: {torch.cuda.is_available()}')
  print(f'CUDA version: {torch.version.cuda}')
  print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else None}')
  "
  ```

- [ ] **2.2.3. Testar carregamento do modelo base**
  ```bash
  # Sem modelo customizado, apenas para validar instala√ß√£o
  docker compose run --rm audio-voice-service python << 'EOF'
  from f5_tts.api import F5TTS
  
  print("Carregando F5-TTS base...")
  model = F5TTS(
      model='F5TTS_Base',
      ode_method='euler',
      use_ema=True,
      device='cuda' if torch.cuda.is_available() else 'cpu'
  )
  print("‚úÖ Modelo base carregado com sucesso!")
  EOF
  ```

### Entreg√°veis:
- F5-TTS original funcionando
- GPU reconhecida e funcional
- Modelo base carregando sem erros

---

# üíª SPRINT 3: ADAPTA√á√ÉO DE C√ìDIGO

**Dura√ß√£o:** 4-5 horas  
**Objetivo:** Adaptar c√≥digo para carregar e usar o modelo pt-BR customizado

## 3.1. Criar Loader Customizado para o Modelo pt-BR

### Tarefas:

- [ ] **3.1.1. Criar m√≥dulo dedicado para o modelo pt-BR**
  ```python
  # services/audio-voice/app/models/ptbr_loader.py
  
  """
  Loader customizado para modelo F5-TTS pt-BR
  """
  import torch
  import logging
  from pathlib import Path
  from typing import Optional, Dict, Any
  from f5_tts.model import CFM  # ou classe correta
  from f5_tts.infer.utils_infer import load_checkpoint
  
  logger = logging.getLogger(__name__)
  
  class PTBRModelLoader:
      """
      Carregador especializado para modelo pt-BR fine-tunado
      """
      
      def __init__(
          self,
          checkpoint_path: Path,
          device: str = 'cuda',
          use_fp16: bool = True
      ):
          self.checkpoint_path = checkpoint_path
          self.device = device
          self.use_fp16 = use_fp16
          self.model = None
          
      def load(self) -> Any:
          """Carrega modelo com configura√ß√µes corretas"""
          logger.info(f"Loading pt-BR model from {self.checkpoint_path}")
          
          # Carregar checkpoint
          checkpoint = torch.load(
              self.checkpoint_path,
              map_location=self.device
          )
          
          # Detectar configura√ß√£o do modelo baseado no checkpoint
          config = self._infer_config_from_checkpoint(checkpoint)
          
          # Criar modelo com configura√ß√£o correta
          self.model = self._create_model(config)
          
          # Carregar pesos
          self._load_weights(checkpoint)
          
          # Otimiza√ß√µes
          self._apply_optimizations()
          
          return self.model
      
      def _infer_config_from_checkpoint(
          self, 
          checkpoint: Dict[str, Any]
      ) -> Dict[str, Any]:
          """Detecta configura√ß√£o baseado nas dimens√µes do modelo"""
          state_dict = checkpoint.get('model_state_dict', checkpoint)
          
          # Exemplo: detectar dim baseado em embeddings
          text_embed_shape = state_dict['transformer.text_embed.text_embed.weight'].shape
          vocab_size, text_dim = text_embed_shape
          
          input_proj_shape = state_dict['transformer.input_embed.proj.weight'].shape
          model_dim, input_dim = input_proj_shape
          
          config = {
              'vocab_size': vocab_size,
              'text_dim': text_dim,
              'model_dim': model_dim,
              'input_dim': input_dim,
              # ... outras configura√ß√µes
          }
          
          logger.info(f"Inferred config: {config}")
          return config
      
      def _create_model(self, config: Dict[str, Any]) -> Any:
          """Cria modelo com configura√ß√£o customizada"""
          # Implementar baseado na estrutura do F5-TTS
          pass
      
      def _load_weights(self, checkpoint: Dict[str, Any]):
          """Carrega pesos no modelo"""
          state_dict = checkpoint.get('model_state_dict', checkpoint)
          self.model.load_state_dict(state_dict, strict=False)
          logger.info("‚úÖ Weights loaded successfully")
      
      def _apply_optimizations(self):
          """Aplica otimiza√ß√µes para GTX 1050 Ti"""
          if self.device == 'cuda':
              if self.use_fp16:
                  self.model.half()
                  logger.info("‚úÖ Model converted to FP16")
              
              self.model.eval()
              torch.cuda.empty_cache()
  ```

- [ ] **3.1.2. Testar loader isoladamente**
  ```bash
  # Criar script de teste
  python test_ptbr_loader.py
  ```

- [ ] **3.1.3. Validar sa√≠da do modelo**
  - Verificar dimens√µes de output
  - Testar com √°udio de refer√™ncia simples
  - Confirmar que n√£o h√° erros de mem√≥ria

### Entreg√°veis:
- M√≥dulo `ptbr_loader.py` funcional
- Testes unit√°rios do loader
- Modelo pt-BR carregando sem erros

---

## 3.2. Integrar Loader com F5TTSClient

### Tarefas:

- [ ] **3.2.1. Modificar F5TTSClient para usar loader customizado**
  ```python
  # services/audio-voice/app/f5tts_client.py
  
  from .models.ptbr_loader import PTBRModelLoader
  
  class F5TTSClient:
      def _load_models(self):
          """Carrega modelo F5-TTS (customizado pt-BR ou HuggingFace padr√£o)"""
          try:
              logger.info(f"üì• Loading F5-TTS model: {self.model_name}")
              
              # Verificar se usa modelo customizado
              if self.custom_model_path and self.custom_model_path.exists():
                  logger.info("üáßüá∑ Using CUSTOM pt-BR model with specialized loader")
                  
                  # Usar loader customizado
                  loader = PTBRModelLoader(
                      checkpoint_path=self.custom_model_path,
                      device=self.device,
                      use_fp16=self.use_fp16
                  )
                  
                  self.f5tts = loader.load()
                  
                  # Wrapper para manter interface compat√≠vel
                  self.f5tts = F5TTSWrapper(
                      model=self.f5tts,
                      device=self.device,
                      sample_rate=self.sample_rate
                  )
                  
              else:
                  # Fallback: modelo HuggingFace padr√£o
                  logger.info("Using HuggingFace default model")
                  from f5_tts.api import F5TTS
                  
                  self.f5tts = F5TTS(
                      model='F5TTS_Base',
                      ode_method="euler",
                      use_ema=True,
                      device=self.device,
                      hf_cache_dir=str(self.hf_cache_dir)
                  )
              
              # Otimiza√ß√µes GPU (GTX 1050 Ti)
              self._apply_gpu_optimizations()
              
              logger.info("‚úÖ F5-TTS model loaded successfully")
              
          except Exception as e:
              logger.error(f"‚ùå Failed to load F5-TTS model: {e}", exc_info=True)
              raise OpenVoiceException(f"Model loading failed: {str(e)}") from e
  ```

- [ ] **3.2.2. Criar wrapper de compatibilidade**
  ```python
  # services/audio-voice/app/models/f5tts_wrapper.py
  
  class F5TTSWrapper:
      """
      Wrapper para manter interface consistente entre
      modelo customizado e API padr√£o do F5-TTS
      """
      
      def __init__(self, model, device, sample_rate):
          self.model = model
          self.device = device
          self.sample_rate = sample_rate
      
      def infer(self, ref_audio, ref_text, gen_text, **kwargs):
          """Interface unificada para infer√™ncia"""
          # Implementar baseado na API do F5-TTS
          pass
  ```

- [ ] **3.2.3. Atualizar OpenVoiceClient**
  ```python
  # services/audio-voice/app/openvoice_client.py
  
  # Aplicar mesmas mudan√ßas do F5TTSClient
  # Garantir que adapter tamb√©m use loader customizado
  ```

### Entreg√°veis:
- F5TTSClient usando loader customizado
- Wrapper de compatibilidade funcional
- OpenVoiceClient atualizado

---

## 3.3. Adaptar Pipeline de Infer√™ncia

### Tarefas:

- [ ] **3.3.1. Ajustar par√¢metros de infer√™ncia para pt-BR**
  ```python
  # Configura√ß√µes espec√≠ficas para o modelo pt-BR
  PTBR_INFERENCE_CONFIG = {
      'sample_rate': 24000,
      'nfe_step': 16,  # Reduzido para GTX 1050 Ti
      'cfg_strength': 2.0,
      'sway_sampling_coef': -1.0,
      'speed': 1.0,
      # ... outros par√¢metros
  }
  ```

- [ ] **3.3.2. Implementar pr√©-processamento de √°udio pt-BR**
  - Normaliza√ß√£o espec√≠fica
  - Dura√ß√£o m√°xima de refer√™ncia
  - Valida√ß√µes espec√≠ficas

- [ ] **3.3.3. Implementar p√≥s-processamento**
  - Normaliza√ß√£o de sa√≠da
  - Remo√ß√£o de sil√™ncio
  - Ajuste de volume

### Entreg√°veis:
- Pipeline de infer√™ncia otimizado para pt-BR
- Pr√©/p√≥s-processamento implementado
- Configura√ß√µes documentadas

---

# üß™ SPRINT 4: TESTES E OTIMIZA√á√ÉO

**Dura√ß√£o:** 3-4 horas  
**Objetivo:** Testar exaustivamente e otimizar para GTX 1050 Ti

## 4.1. Testes Funcionais

### Tarefas:

- [ ] **4.1.1. Criar suite de testes automatizados**
  ```python
  # services/audio-voice/tests/test_ptbr_model.py
  
  import pytest
  from app.f5tts_client import F5TTSClient
  
  def test_model_loads():
      """Testa carregamento do modelo pt-BR"""
      client = F5TTSClient()
      assert client.f5tts is not None
  
  def test_inference_simple():
      """Testa infer√™ncia simples"""
      client = F5TTSClient()
      audio, duration = client.generate_dubbing(
          text="Ol√°, como voc√™ est√°?",
          language="pt-BR"
      )
      assert audio is not None
      assert duration > 0
  
  def test_voice_cloning():
      """Testa clonagem de voz"""
      # Implementar teste com √°udio de refer√™ncia
      pass
  
  def test_memory_usage():
      """Verifica uso de mem√≥ria GPU"""
      import torch
      client = F5TTSClient()
      
      # Rodar infer√™ncia
      client.generate_dubbing("Teste de mem√≥ria", "pt-BR")
      
      # Verificar VRAM
      if torch.cuda.is_available():
          allocated = torch.cuda.memory_allocated(0) / (1024**3)
          assert allocated < 3.5, f"VRAM muito alta: {allocated:.2f} GB"
  ```

- [ ] **4.1.2. Testar com diferentes textos pt-BR**
  - Textos curtos (1-5 palavras)
  - Textos m√©dios (1-3 frases)
  - Textos longos (par√°grafos)
  - Caracteres especiais (√ß, √£, √µ, etc.)
  - Pontua√ß√£o variada

- [ ] **4.1.3. Testar clonagem de voz**
  - Diferentes vozes de refer√™ncia
  - Diferentes dura√ß√µes de refer√™ncia (3s, 10s, 30s)
  - Qualidade de √°udio variada

### Entreg√°veis:
- Suite de testes completa
- Relat√≥rio de testes funcionais
- Lista de casos edge identificados

---

## 4.2. Otimiza√ß√£o de Performance

### Tarefas:

- [ ] **4.2.1. Profiling de VRAM**
  ```bash
  # Durante testes, monitorar VRAM
  watch -n 0.5 nvidia-smi
  
  # Ou dentro do c√≥digo:
  python << 'EOF'
  import torch
  from app.f5tts_client import F5TTSClient
  
  torch.cuda.reset_peak_memory_stats()
  
  client = F5TTSClient()
  client.generate_dubbing("Teste de VRAM", "pt-BR")
  
  peak_mem = torch.cuda.max_memory_allocated(0) / (1024**3)
  print(f"Peak VRAM: {peak_mem:.2f} GB")
  EOF
  ```

- [ ] **4.2.2. Ajustar NFE steps baseado em testes**
  - Testar NFE: 8, 12, 16, 20, 24, 32
  - Medir: tempo, qualidade, VRAM
  - Encontrar sweet spot para GTX 1050 Ti

- [ ] **4.2.3. Implementar cache inteligente**
  ```python
  # Cache de embeddings de texto frequentes
  # Cache de √°udios de refer√™ncia processados
  # LRU cache para evitar reprocessamento
  ```

- [ ] **4.2.4. Otimizar batch processing**
  - Mesmo com batch_size=1, otimizar internamente
  - Pr√©-alocar tensors quando poss√≠vel
  - Evitar c√≥pias desnecess√°rias

### Entreg√°veis:
- Relat√≥rio de profiling de VRAM
- Configura√ß√µes otimizadas documentadas
- Benchmarks antes/depois

---

## 4.3. Testes de Estresse

### Tarefas:

- [ ] **4.3.1. Teste de carga sequencial**
  ```python
  # Gerar 100 √°udios seguidos
  # Verificar:
  # - Memory leaks
  # - Degrada√ß√£o de performance
  # - Estabilidade
  ```

- [ ] **4.3.2. Teste de textos extremos**
  - Texto vazio
  - Texto com 1000+ caracteres
  - Texto com emojis
  - Texto com n√∫meros
  - Texto misto (pt-BR + en)

- [ ] **4.3.3. Teste de recupera√ß√£o de erros**
  - Arquivo de refer√™ncia corrompido
  - GPU indispon√≠vel (CPU fallback)
  - Disco cheio
  - Out of memory

### Entreg√°veis:
- Relat√≥rio de testes de estresse
- Fixes de bugs encontrados
- Melhorias de robustez implementadas

---

# üìö SPRINT 5: DOCUMENTA√á√ÉO E DEPLOY

**Dura√ß√£o:** 2-3 horas  
**Objetivo:** Documentar tudo e preparar para produ√ß√£o

## 5.1. Documenta√ß√£o T√©cnica

### Tarefas:

- [ ] **5.1.1. Atualizar README.md**
  ```markdown
  # Audio Voice Service - F5-TTS pt-BR
  
  ## üáßüá∑ Modelo Portugu√™s Brasileiro
  
  Este servi√ßo usa um modelo F5-TTS fine-tunado para portugu√™s brasileiro,
  otimizado para GTX 1050 Ti (4GB VRAM).
  
  ### Caracter√≠sticas do Modelo
  - **Tamanho:** 1.35 GB
  - **Idioma:** Portugu√™s Brasileiro
  - **Sample Rate:** 24000 Hz
  - **Arquitetura:** F5-TTS v2 (transformer_blocks)
  
  ### Requisitos
  - NVIDIA GPU com 4GB+ VRAM
  - CUDA 12.1+
  - Docker com NVIDIA runtime
  
  ### Quick Start
  ```bash
  docker compose up -d audio-voice-service
  ```
  
  ### Testes
  ```bash
  docker compose run --rm audio-voice-service pytest
  ```
  ```

- [ ] **5.1.2. Documentar API endpoints**
  ```python
  # Adicionar docstrings detalhadas
  # Incluir exemplos de uso
  # Documentar par√¢metros pt-BR espec√≠ficos
  ```

- [ ] **5.1.3. Criar guia de troubleshooting**
  ```markdown
  # Troubleshooting
  
  ## Erro: Out of Memory
  - Reduzir NFE_STEP para 8
  - Usar textos mais curtos
  - Verificar outros processos usando GPU
  
  ## Erro: Modelo n√£o carrega
  - Verificar hash do arquivo model_last.safetensors
  - Confirmar vers√£o do F5-TTS instalada
  - Ver logs detalhados com DEBUG=true
  ```

### Entreg√°veis:
- README.md completo
- API documentada
- Guia de troubleshooting

---

## 5.2. Configura√ß√£o de Produ√ß√£o

### Tarefas:

- [ ] **5.2.1. Criar docker-compose.prod.yml**
  ```yaml
  services:
    audio-voice-service:
      build: .
      restart: always
      environment:
        - LOG_LEVEL=INFO
        - F5TTS_NFE_STEP=16
        - F5TTS_USE_FP16=true
      healthcheck:
        test: ["CMD", "curl", "-f", "http://localhost:8005/health"]
        interval: 30s
        timeout: 10s
        retries: 3
        start_period: 120s  # Modelo pt-BR leva mais tempo
      deploy:
        resources:
          limits:
            cpus: '4'
            memory: 8G
          reservations:
            devices:
              - driver: nvidia
                count: 1
                capabilities: [gpu]
  ```

- [ ] **5.2.2. Configurar logging estruturado**
  ```python
  # JSON logging para produ√ß√£o
  # M√©tricas de performance
  # Alertas de VRAM/CPU
  ```

- [ ] **5.2.3. Implementar health checks robustos**
  ```python
  @app.get("/health")
  async def health():
      return {
          "status": "healthy",
          "model": "f5tts-ptbr",
          "model_loaded": processor.tts_client.f5tts is not None,
          "gpu_available": torch.cuda.is_available(),
          "vram_used_gb": torch.cuda.memory_allocated(0) / (1024**3) if torch.cuda.is_available() else 0
      }
  
  @app.get("/readiness")
  async def readiness():
      # Teste real de infer√™ncia
      try:
          processor.tts_client.generate_dubbing("teste", "pt-BR")
          return {"ready": True}
      except Exception as e:
          raise HTTPException(status_code=503, detail=str(e))
  ```

### Entreg√°veis:
- Configura√ß√£o de produ√ß√£o
- Logging estruturado
- Health checks robustos

---

## 5.3. Deploy e Valida√ß√£o Final

### Tarefas:

- [ ] **5.3.1. Deploy em ambiente de staging**
  ```bash
  # Build production image
  docker compose -f docker-compose.prod.yml build
  
  # Deploy
  docker compose -f docker-compose.prod.yml up -d
  
  # Validar
  curl http://localhost:8005/health
  curl http://localhost:8005/readiness
  ```

- [ ] **5.3.2. Smoke tests em staging**
  ```bash
  # Testar endpoint principal
  curl -X POST http://localhost:8005/api/v1/clone-voice \
    -F "audio=@test_voice.mp3" \
    -F "text=Ol√°, este √© um teste de clonagem de voz em portugu√™s brasileiro"
  ```

- [ ] **5.3.3. Monitoramento inicial (24h)**
  - M√©tricas de CPU/GPU
  - Uso de mem√≥ria
  - Lat√™ncia de requisi√ß√µes
  - Taxa de erro

- [ ] **5.3.4. Rollout para produ√ß√£o**
  - Merge da branch de migra√ß√£o
  - Tag de release
  - Deploy gradual (canary/blue-green)

### Entreg√°veis:
- Servi√ßo em produ√ß√£o
- M√©tricas de monitoramento
- Documenta√ß√£o de rollback (se necess√°rio)

---

# üìã CHECKLIST FINAL

## Antes de Marcar como Conclu√≠do:

- [ ] ‚úÖ Modelo pt-BR carrega sem erros
- [ ] ‚úÖ Infer√™ncia funciona em portugu√™s brasileiro
- [ ] ‚úÖ Clonagem de voz funcional
- [ ] ‚úÖ VRAM ‚â§ 3.5 GB durante uso normal
- [ ] ‚úÖ Lat√™ncia aceit√°vel (< 10s para 1 frase)
- [ ] ‚úÖ Testes automatizados passando (>90% coverage)
- [ ] ‚úÖ Documenta√ß√£o completa
- [ ] ‚úÖ Logs estruturados e informativos
- [ ] ‚úÖ Health checks funcionais
- [ ] ‚úÖ Deploy em staging bem-sucedido
- [ ] ‚úÖ Monitoramento configurado
- [ ] ‚úÖ Plano de rollback documentado
- [ ] ‚úÖ Equipe treinada (se aplic√°vel)

---

# üìä M√âTRICAS DE SUCESSO

## KPIs T√©cnicos:

| M√©trica | Target | Como Medir |
|---------|--------|------------|
| VRAM Peak | ‚â§ 3.5 GB | `nvidia-smi` durante infer√™ncia |
| Lat√™ncia (1 frase) | < 10s | Benchmark automatizado |
| Taxa de Erro | < 1% | Logs de produ√ß√£o |
| Uptime | > 99% | Monitoramento 24/7 |
| Qualidade de √Åudio | MOS > 3.5 | Avalia√ß√£o humana |

## KPIs de Neg√≥cio:

| M√©trica | Target | Como Medir |
|---------|--------|------------|
| Ado√ß√£o pt-BR | > 50% das requisi√ß√µes | Analytics |
| Satisfa√ß√£o do Usu√°rio | > 4.0/5.0 | Feedback |
| Tempo de Processamento | Redu√ß√£o de 30% vs baseline | Compara√ß√£o com modelo anterior |

---

# üö® RISCOS E MITIGA√á√ïES

## Riscos Identificados:

| Risco | Probabilidade | Impacto | Mitiga√ß√£o |
|-------|---------------|---------|-----------|
| Modelo pt-BR incompat√≠vel com vers√£o atual | ALTA | ALTO | ‚úÖ Sprint 1 resolve isso com an√°lise profunda |
| OOM em GTX 1050 Ti | M√âDIA | ALTO | Otimiza√ß√µes FP16, NFE reduzido, testes de estresse |
| Performance ruim em pt-BR | BAIXA | M√âDIO | Testes extensivos antes de prod |
| Regress√£o em outros idiomas | BAIXA | M√âDIO | Testes de regress√£o na Sprint 4 |
| Instabilidade em produ√ß√£o | BAIXA | ALTO | Staging + monitoramento + rollback plan |

---

# üìû SUPORTE E ESCALA√á√ÉO

## Durante a Migra√ß√£o:

- **Sprint 1-2:** Pesquisa e setup - Escala√ß√£o: Lead Dev
- **Sprint 3-4:** Desenvolvimento - Escala√ß√£o: ML Engineer
- **Sprint 5:** Deploy - Escala√ß√£o: DevOps + SRE

## Canais:

- Issues t√©cnicos: GitHub Issues
- Discuss√µes: Slack #audio-voice-ptbr
- Emerg√™ncias: PagerDuty

---

# üéØ PR√ìXIMOS PASSOS AP√ìS CONCLUS√ÉO

## Melhorias Futuras:

1. **Otimiza√ß√£o Adicional:**
   - Quantiza√ß√£o INT8 para economia de VRAM
   - TensorRT para lat√™ncia menor
   - Streaming de √°udio em tempo real

2. **Novos Recursos:**
   - Suporte a m√∫ltiplos dialetos pt-BR
   - Voice mixing (combinar caracter√≠sticas)
   - Emo√ß√µes control√°veis

3. **Escalabilidade:**
   - Kubernetes deployment
   - Auto-scaling baseado em carga
   - Multi-GPU support

---

**√öltima Atualiza√ß√£o:** 26/11/2025  
**Vers√£o:** 1.0  
**Autor:** AI Senior Python & Deep Learning Expert  
**Status:** üìã PRONTO PARA EXECU√á√ÉO
