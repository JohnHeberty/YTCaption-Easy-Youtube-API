# üìä AN√ÅLISE COMPLETA DO MODELO PT-BR

**Data:** 26/11/2025  
**Arquivo:** `services/audio-voice/models/f5tts/pt-br/model_last.safetensors`  
**Tamanho:** 1.26 GB  
**Sprint:** 1.1 - An√°lise Profunda do Modelo

---

## üéØ RESUMO EXECUTIVO

‚úÖ **Modelo carregado com sucesso** usando `safetensors`  
‚úÖ **Estrutura identificada:** Usa `transformer_blocks` (vers√£o moderna do F5-TTS)  
‚úÖ **Total de tensores:** 364 par√¢metros  
‚úÖ **Arquitetura:** 22 transformer blocks  
‚ö†Ô∏è **Incompatibilidade confirmada:** Dimens√µes diferentes do F5-TTS pip atual

---

## üìÅ INFORMA√á√ïES DO ARQUIVO

```
Caminho: /app/models/f5tts/pt-br/model_last.safetensors
Tamanho: 1.26 GB (1,355,669,504 bytes)
Formato: SafeTensors
Metadados: Nenhum (arquivo n√£o cont√©m metadata adicional)
```

**Observa√ß√µes:**
- Apenas 1 arquivo no diret√≥rio (sem vocab.txt, config.json, etc.)
- Modelo standalone sem arquivos auxiliares
- Precisaremos inferir configura√ß√µes das dimens√µes dos tensors

---

## üèóÔ∏è ESTRUTURA DO MODELO

### Arquitetura Detectada

```
Tipo: F5-TTS v2 (transformer_blocks)
‚îî‚îÄ‚îÄ Usa transformer_blocks: ‚úÖ SIM
‚îî‚îÄ‚îÄ Usa layers (estrutura antiga): ‚ùå N√ÉO
‚îî‚îÄ‚îÄ N√∫mero de transformer_blocks: 22
```

### Componentes Principais

#### 1. **Input Embeddings**
```python
transformer.input_embed.proj.weight: (1024, 712)
                                     ^^^^  ^^^^
                                     dim   input_channels
```

**Dimens√µes Chave:**
- Model dimension: **1024**
- Input channels: **712** (mel-spectrogram features)

**Compara√ß√£o com F5-TTS padr√£o:**
- F5-TTS pip espera: `(1024, 300)` ‚ùå INCOMPAT√çVEL
- Modelo pt-BR usa: `(1024, 712)` 

**Conclus√£o:** Este modelo foi treinado com **mais features de entrada** (712 vs 300), provavelmente para melhor captura de caracter√≠sticas do √°udio.

---

#### 2. **Text Embeddings**

```python
transformer.text_embed.text_blocks.0.pwconv1.weight: (1024, 512)
transformer.text_embed.text_blocks.0.pwconv2.weight: (512, 1024)
```

**Estrutura:**
- 4 blocos de text embedding (text_blocks.0 at√© text_blocks.3)
- Cada bloco usa ConvNeXt-style blocks:
  - Depthwise convolution (dwconv)
  - Pointwise convolutions (pwconv1, pwconv2)
  - Global Response Normalization (grn)
  - Layer normalization

**Text embedding dimension:** **512**

**Compara√ß√£o com F5-TTS padr√£o:**
- F5-TTS pip espera: **100** ‚ùå INCOMPAT√çVEL
- Modelo pt-BR usa: **512**

**Conclus√£o:** Modelo pt-BR usa embeddings de texto **5x maiores**, permitindo representa√ß√µes mais ricas do texto em portugu√™s brasileiro.

---

#### 3. **Transformer Blocks**

```
Total: 22 transformer_blocks (0-21)
```

**Cada bloco cont√©m:**
```
transformer_blocks.{N}.attn.to_q.weight: (1024, 1024)
transformer_blocks.{N}.attn.to_k.weight: (1024, 1024)
transformer_blocks.{N}.attn.to_v.weight: (1024, 1024)
transformer_blocks.{N}.attn.to_out.0.weight: (1024, 1024)
transformer_blocks.{N}.attn_norm.linear.weight: (6144, 1024)
transformer_blocks.{N}.ff.ff.0.0.weight: (2048, 1024)
transformer_blocks.{N}.ff.ff.2.weight: (1024, 2048)
```

**Detalhes da Arquitetura:**
- **Attention heads:** Multi-head self-attention
- **Hidden dimension:** 1024
- **FFN expansion:** 2x (2048)
- **Attention normalization:** Adaptive layer norm (6144 = 1024 * 6 par√¢metros)

---

#### 4. **Output Projection**

```python
transformer.proj_out.weight: (100, 1024)
transformer.norm_out.linear.weight: (2048, 1024)
```

**Output channels:** **100** (mel-spectrogram bins)

---

## üîç AN√ÅLISE DE INCOMPATIBILIDADES

### Problemas Identificados

| Componente | F5-TTS pip | Modelo pt-BR | Status |
|------------|------------|--------------|--------|
| Input projection | `(1024, 300)` | `(1024, 712)` | ‚ùå INCOMPAT√çVEL |
| Text embed dim | `100` | `512` | ‚ùå INCOMPAT√çVEL |
| Estrutura | `layers.*` | `transformer_blocks.*` | ‚ùå INCOMPAT√çVEL |
| Num blocks | 24 (t√≠pico) | 22 | ‚ö†Ô∏è DIFERENTE |

### Causa Raiz

O modelo pt-BR foi treinado com uma vers√£o **customizada/modificada** do F5-TTS que usa:

1. **Mais features de entrada** (712 vs 300) - provavelmente mel-spectrogram de maior resolu√ß√£o
2. **Embeddings de texto maiores** (512 vs 100) - melhor representa√ß√£o lingu√≠stica
3. **Estrutura transformer_blocks** - vers√£o mais recente da arquitetura

---

## üéØ CONFIGURA√á√ÉO NECESS√ÅRIA

Para carregar este modelo, precisamos de um F5-TTS configurado com:

```python
model_config = {
    # Dimens√µes
    'dim': 1024,                    # Model dimension
    'input_channels': 712,          # Mel-spec features ‚ö†Ô∏è CUSTOMIZADO
    'text_dim': 512,                # Text embedding dimension ‚ö†Ô∏è CUSTOMIZADO
    'output_channels': 100,         # Output mel-spec bins
    
    # Transformer
    'depth': 22,                    # Number of transformer blocks
    'heads': 16,                    # Attention heads (inferido)
    'ff_mult': 2,                   # FFN expansion factor
    
    # Architecture
    'use_transformer_blocks': True, # Usa nova estrutura ‚ö†Ô∏è
    'text_num_blocks': 4,           # ConvNeXt text blocks
    
    # Positional encoding
    'use_rotary_emb': True,         # Rotary embeddings (detectado)
}
```

---

## üì¶ ESTRUTURA DE TENSORS (AMOSTRA)

### Primeiros 10 Tensors

```
1.  transformer.input_embed.conv_pos_embed.conv1d.0.bias      (1024,)
2.  transformer.input_embed.conv_pos_embed.conv1d.0.weight    (1024, 64, 31)
3.  transformer.input_embed.conv_pos_embed.conv1d.2.bias      (1024,)
4.  transformer.input_embed.conv_pos_embed.conv1d.2.weight    (1024, 64, 31)
5.  transformer.input_embed.proj.bias                         (1024,)
6.  transformer.input_embed.proj.weight                       (1024, 712) ‚ö†Ô∏è
7.  transformer.norm_out.linear.bias                          (2048,)
8.  transformer.norm_out.linear.weight                        (2048, 1024)
9.  transformer.proj_out.bias                                 (100,)
10. transformer.proj_out.weight                               (100, 1024)
```

### √öltimos 10 Tensors

```
355. transformer.transformer_blocks.9.attn.to_q.bias          (1024,)
356. transformer.transformer_blocks.9.attn.to_q.weight        (1024, 1024)
357. transformer.transformer_blocks.9.attn.to_v.bias          (1024,)
358. transformer.transformer_blocks.9.attn.to_v.weight        (1024, 1024)
359. transformer.transformer_blocks.9.attn_norm.linear.bias   (6144,)
360. transformer.transformer_blocks.9.attn_norm.linear.weight (6144, 1024)
361. transformer.transformer_blocks.9.ff.ff.0.0.bias          (2048,)
362. transformer.transformer_blocks.9.ff.ff.0.0.weight        (2048, 1024)
363. transformer.transformer_blocks.9.ff.ff.2.bias            (1024,)
364. transformer.transformer_blocks.9.ff.ff.2.weight          (1024, 2048)
```

**Observa√ß√£o:** Os √≠ndices dos transformer_blocks v√£o apenas at√© 9 nas √∫ltimas chaves, mas existem 22 blocos no total (0-21). Isso confirma que existem 22 blocos completos no modelo.

---

## üî¨ AN√ÅLISE DO REPOSIT√ìRIO F5-TTS OFICIAL

### Estado Atual do Reposit√≥rio

```bash
√öltimo commit: 3eecd94 (recente)
Vers√£o atual: v1.1.9
Branch: main
```

### Descobertas Importantes

1. **Arquivos de modelo encontrados:**
   - `src/f5_tts/model/cfm.py` - Conditional Flow Matching (modelo principal)
   - `src/f5_tts/model/backbones/dit.py` - DiT backbone
   - `src/f5_tts/model/backbones/mmdit.py` - MMDiT backbone
   - `src/f5_tts/model/backbones/unett.py` - UNet-T backbone

2. **Estrutura transformer_blocks confirmada:**
   - C√≥digo atual do F5-TTS usa `transformer_blocks`
   - Vers√£o pip deve estar desatualizada

3. **Compatibilidade:**
   - Instala√ß√£o do reposit√≥rio oficial deve suportar a estrutura do modelo pt-BR
   - Mas dimens√µes customizadas (712 input, 512 text) ainda requerem configura√ß√£o especial

---

## üöÄ PR√ìXIMOS PASSOS (Sprint 1.2)

### A√ß√µes Recomendadas

1. **Instalar F5-TTS do reposit√≥rio oficial** (n√£o do pip)
   ```bash
   cd /tmp/F5-TTS
   pip install -e .
   ```

2. **Criar loader customizado** que:
   - Infere configura√ß√£o das dimens√µes do checkpoint
   - Cria modelo com `input_channels=712` e `text_dim=512`
   - Carrega pesos do safetensors
   - Aplica otimiza√ß√µes GTX 1050 Ti (FP16, etc.)

3. **Testar carregamento** isoladamente antes de integrar

---

## üìù CONCLUS√ïES

### ‚úÖ Confirmado

1. Modelo √© v√°lido e bem estruturado
2. Usa arquitetura F5-TTS moderna (`transformer_blocks`)
3. Dimens√µes customizadas para melhor qualidade pt-BR
4. 22 transformer blocks (profundidade adequada)

### ‚ö†Ô∏è Desafios

1. Incompat√≠vel com F5-TTS pip (vers√£o desatualizada)
2. Requer configura√ß√£o customizada para dimens√µes
3. Sem arquivos auxiliares (vocab, config)
4. Precisaremos criar loader especializado

### üéØ Viabilidade

**ALTA** - Modelo √© totalmente vi√°vel com as seguintes condi√ß√µes:

- ‚úÖ Instalar F5-TTS do reposit√≥rio (n√£o pip)
- ‚úÖ Criar configura√ß√£o customizada com dimens√µes corretas
- ‚úÖ Implementar loader que infere config do checkpoint
- ‚úÖ Testar carregamento antes de integra√ß√£o completa

---

## üîó REFER√äNCIAS

- Reposit√≥rio F5-TTS: https://github.com/SWivid/F5-TTS
- Commit atual: `3eecd94`
- Vers√£o: v1.1.9
- Branch: main

---

**Status:** ‚úÖ SPRINT 1.1 CONCLU√çDA  
**Pr√≥ximo:** Sprint 1.2 - Pesquisa de Compatibilidade  
**Data:** 26/11/2025
